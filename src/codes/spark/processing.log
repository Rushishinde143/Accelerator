DEBUG:asyncio:Using proactor: IocpProactor
DEBUG:asyncio:Using proactor: IocpProactor
DEBUG:asyncio:Using proactor: IocpProactor
DEBUG:py4j.java_gateway:GatewayClient.address is deprecated and will be removed in version 1.0. Use GatewayParameters instead.
DEBUG:py4j.clientserver:Command to send: A
e5f201444d7b5788fbd7e60b6a416ebe7a4970add04f8ef270e123d28e0f036e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.SparkConf
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.api.java.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.api.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.ml.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.mllib.api.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.resource.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.sql.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.sql.api.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.sql.hive.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
scala.Tuple2
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkConf
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkConf
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.SparkConf
bTrue
e

DEBUG:py4j.clientserver:Answer received: !yro0
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yro1
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yro2
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.serializer.objectStreamReset
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.serializer.objectStreamReset
s100
e

DEBUG:py4j.clientserver:Answer received: !yro3
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.rdd.compress
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.rdd.compress
sTrue
e

DEBUG:py4j.clientserver:Answer received: !yro4
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.master
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.app.name
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.master
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
get
sspark.master
e

DEBUG:py4j.clientserver:Answer received: !yslocal[*]
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.app.name
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
get
sspark.app.name
e

DEBUG:py4j.clientserver:Answer received: !ysAvroProcessing
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.home
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o0
getAll
e

DEBUG:py4j.clientserver:Answer received: !yto5
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i0
e

DEBUG:py4j.clientserver:Answer received: !yro6
DEBUG:py4j.clientserver:Command to send: c
o6
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.app.submitTime
DEBUG:py4j.clientserver:Command to send: c
o6
_2
e

DEBUG:py4j.clientserver:Answer received: !ys1694536788005
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i1
e

DEBUG:py4j.clientserver:Answer received: !yro7
DEBUG:py4j.clientserver:Command to send: c
o7
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.rdd.compress
DEBUG:py4j.clientserver:Command to send: c
o7
_2
e

DEBUG:py4j.clientserver:Answer received: !ysTrue
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i2
e

DEBUG:py4j.clientserver:Answer received: !yro8
DEBUG:py4j.clientserver:Command to send: c
o8
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.app.name
DEBUG:py4j.clientserver:Command to send: c
o8
_2
e

DEBUG:py4j.clientserver:Answer received: !ysAvroProcessing
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i3
e

DEBUG:py4j.clientserver:Answer received: !yro9
DEBUG:py4j.clientserver:Command to send: c
o9
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.serializer.objectStreamReset
DEBUG:py4j.clientserver:Command to send: c
o9
_2
e

DEBUG:py4j.clientserver:Answer received: !ys100
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i4
e

DEBUG:py4j.clientserver:Answer received: !yro10
DEBUG:py4j.clientserver:Command to send: c
o10
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.submit.pyFiles
DEBUG:py4j.clientserver:Command to send: c
o10
_2
e

DEBUG:py4j.clientserver:Answer received: !ysC:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i5
e

DEBUG:py4j.clientserver:Answer received: !yro11
DEBUG:py4j.clientserver:Command to send: c
o11
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.master
DEBUG:py4j.clientserver:Command to send: c
o11
_2
e

DEBUG:py4j.clientserver:Answer received: !yslocal[*]
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i6
e

DEBUG:py4j.clientserver:Answer received: !yro12
DEBUG:py4j.clientserver:Command to send: c
o12
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.jars.packages
DEBUG:py4j.clientserver:Command to send: c
o12
_2
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark:spark-avro_2.12:3.4.1
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i7
e

DEBUG:py4j.clientserver:Answer received: !yro13
DEBUG:py4j.clientserver:Command to send: c
o13
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.submit.deployMode
DEBUG:py4j.clientserver:Command to send: c
o13
_2
e

DEBUG:py4j.clientserver:Answer received: !ysclient
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i8
e

DEBUG:py4j.clientserver:Answer received: !yro14
DEBUG:py4j.clientserver:Command to send: c
o14
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.files
DEBUG:py4j.clientserver:Command to send: c
o14
_2
e

DEBUG:py4j.clientserver:Answer received: !ysfile:///C:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,file:///C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i9
e

DEBUG:py4j.clientserver:Answer received: !yro15
DEBUG:py4j.clientserver:Command to send: c
o15
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.repl.local.jars
DEBUG:py4j.clientserver:Command to send: c
o15
_2
e

DEBUG:py4j.clientserver:Answer received: !ysfile:///C:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,file:///C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i10
e

DEBUG:py4j.clientserver:Answer received: !yro16
DEBUG:py4j.clientserver:Command to send: c
o16
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.jars
DEBUG:py4j.clientserver:Command to send: c
o16
_2
e

DEBUG:py4j.clientserver:Answer received: !ysfile:///C:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,file:///C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i11
e

DEBUG:py4j.clientserver:Answer received: !yro17
DEBUG:py4j.clientserver:Command to send: c
o17
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.ui.showConsoleProgress
DEBUG:py4j.clientserver:Command to send: c
o17
_2
e

DEBUG:py4j.clientserver:Answer received: !ystrue
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: r
u
JavaSparkContext
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.java.JavaSparkContext
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.api.java.JavaSparkContext
ro0
e

DEBUG:py4j.clientserver:Command to send: A
e5f201444d7b5788fbd7e60b6a416ebe7a4970add04f8ef270e123d28e0f036e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o1
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o2
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o3
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o4
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o5
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro18
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro19
DEBUG:py4j.clientserver:Command to send: c
o19
conf
e

DEBUG:py4j.clientserver:Answer received: !yro20
DEBUG:py4j.clientserver:Command to send: r
u
PythonAccumulatorV2
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonAccumulatorV2
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.api.python.PythonAccumulatorV2
s127.0.0.1
i54512
se5f201444d7b5788fbd7e60b6a416ebe7a4970add04f8ef270e123d28e0f036e
e

DEBUG:py4j.clientserver:Answer received: !yro21
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro22
DEBUG:py4j.clientserver:Command to send: c
o22
register
ro21
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
PythonUtils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.api.python.PythonUtils
isEncryptionEnabled
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.api.python.PythonUtils
isEncryptionEnabled
ro18
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
PythonUtils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils
DEBUG:py4j.clientserver:Command to send: m
d
o0
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.api.python.PythonUtils
getPythonAuthSocketTimeout
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o6
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.api.python.PythonUtils
getPythonAuthSocketTimeout
ro18
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o7
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yL15
DEBUG:py4j.clientserver:Command to send: m
d
o8
e

DEBUG:py4j.clientserver:Command to send: r
u
PythonUtils
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o9
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o10
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.api.python.PythonUtils
getSparkBufferSize
e

DEBUG:py4j.clientserver:Command to send: m
d
o11
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.api.python.PythonUtils
getSparkBufferSize
ro18
e

DEBUG:py4j.clientserver:Command to send: m
d
o12
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yi65536
DEBUG:py4j.clientserver:Command to send: m
d
o13
e

DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o14
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o15
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o16
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o17
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o19
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.SparkFiles
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-e6e1ce2f-d708-4cd7-a373-d23ada2e2d04\\userFiles-809a865a-083c-4119-a1ae-c16ce440d06c
DEBUG:py4j.clientserver:Command to send: c
o20
get
sspark.submit.pyFiles
s
e

DEBUG:py4j.clientserver:Answer received: !ysC:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.SparkFiles
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-e6e1ce2f-d708-4cd7-a373-d23ada2e2d04\\userFiles-809a865a-083c-4119-a1ae-c16ce440d06c
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.SparkFiles
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-e6e1ce2f-d708-4cd7-a373-d23ada2e2d04\\userFiles-809a865a-083c-4119-a1ae-c16ce440d06c
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
getLocalDir
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro23
DEBUG:py4j.clientserver:Command to send: c
o23
conf
e

DEBUG:py4j.clientserver:Answer received: !yro24
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
getLocalDir
ro24
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-e6e1ce2f-d708-4cd7-a373-d23ada2e2d04
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
createTempDir
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
createTempDir
sC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-e6e1ce2f-d708-4cd7-a373-d23ada2e2d04
spyspark
e

DEBUG:py4j.clientserver:Answer received: !yro25
DEBUG:py4j.clientserver:Command to send: c
o25
getAbsolutePath
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-e6e1ce2f-d708-4cd7-a373-d23ada2e2d04\\pyspark-a7b12770-a8f5-4343-89bf-612310dccf05
DEBUG:py4j.clientserver:Command to send: c
o20
get
sspark.python.profile
sfalse
e

DEBUG:py4j.clientserver:Answer received: !ysfalse
DEBUG:py4j.clientserver:Command to send: c
o20
get
sspark.python.profile.memory
sfalse
e

DEBUG:py4j.clientserver:Answer received: !ysfalse
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
getDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
getDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !yro26
DEBUG:py4j.clientserver:Command to send: c
o26
isDefined
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro27
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao28
DEBUG:py4j.clientserver:Command to send: c
o28
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o28
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.sql.SparkSession
ro27
ro28
e

DEBUG:py4j.clientserver:Answer received: !yro29
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
setDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
setDefaultSession
ro29
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
setActiveSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
setActiveSession
ro29
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
e5f201444d7b5788fbd7e60b6a416ebe7a4970add04f8ef270e123d28e0f036e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro30
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao31
DEBUG:py4j.clientserver:Command to send: c
o31
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o31
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o30
applyModifiableSettings
ro29
ro31
e

DEBUG:py4j.clientserver:Command to send: m
d
o28
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro32
DEBUG:py4j.clientserver:Command to send: c
o32
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro33
DEBUG:py4j.clientserver:Command to send: c
o33
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
e5f201444d7b5788fbd7e60b6a416ebe7a4970add04f8ef270e123d28e0f036e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro34
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Command to send: c
o34
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro35
DEBUG:py4j.clientserver:Command to send: c
o35
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro36
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao37
DEBUG:py4j.clientserver:Command to send: c
o37
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o37
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o36
applyModifiableSettings
ro29
ro37
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro38
DEBUG:py4j.clientserver:Command to send: c
o38
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro39
DEBUG:py4j.clientserver:Command to send: c
o39
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Command to send: m
d
o31
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o37
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro40
DEBUG:py4j.clientserver:Answer received: !yro41
DEBUG:py4j.clientserver:Answer received: !yro42
DEBUG:py4j.clientserver:Command to send: c
o40
schema
e

DEBUG:py4j.clientserver:Command to send: c
o41
schema
e

DEBUG:py4j.clientserver:Command to send: c
o42
schema
e

DEBUG:py4j.clientserver:Answer received: !yro45
DEBUG:py4j.clientserver:Answer received: !yro44
DEBUG:py4j.clientserver:Answer received: !yro43
DEBUG:py4j.clientserver:Command to send: c
o45
json
e

DEBUG:py4j.clientserver:Command to send: c
o44
json
e

DEBUG:py4j.clientserver:Command to send: c
o43
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s.no","type":"long","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"string","nullable":true,"metadata":{}},{"name":"longitude","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s.no","type":"long","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"string","nullable":true,"metadata":{}},{"name":"longitude","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s.no","type":"long","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"string","nullable":true,"metadata":{}},{"name":"longitude","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o40
apply
ssurveydate
e

DEBUG:py4j.clientserver:Command to send: c
o41
apply
ssurveydate
e

DEBUG:py4j.clientserver:Command to send: c
o42
apply
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !yro46
DEBUG:py4j.clientserver:Answer received: !yro48
DEBUG:py4j.clientserver:Answer received: !yro47
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro47
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro48
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro46
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro50
DEBUG:py4j.clientserver:Answer received: !yro49
DEBUG:py4j.clientserver:Answer received: !yro51
DEBUG:py4j.clientserver:Command to send: c
o42
withColumn
ssurveydate
ro50
e

DEBUG:py4j.clientserver:Command to send: c
o41
withColumn
ssurveydate
ro49
e

DEBUG:py4j.clientserver:Command to send: c
o40
withColumn
ssurveydate
ro51
e

DEBUG:py4j.clientserver:Answer received: !yro52
DEBUG:py4j.clientserver:Answer received: !yro53
DEBUG:py4j.clientserver:Command to send: c
o52
apply
ssurveydate
e

DEBUG:py4j.clientserver:Command to send: c
o53
apply
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !yro54
DEBUG:py4j.clientserver:Command to send: c
o54
apply
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !yro55
DEBUG:py4j.clientserver:Answer received: !yro56
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro57
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro58
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro59
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro60
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro55
ro58
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !yro61
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro56
ro59
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro57
ro60
e

DEBUG:py4j.clientserver:Answer received: !yro62
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro63
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro61
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro62
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro63
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro64
DEBUG:py4j.clientserver:Answer received: !yro65
DEBUG:py4j.clientserver:Answer received: !yro66
DEBUG:py4j.clientserver:Command to send: c
o52
withColumn
ssurveydate_add_months
ro64
e

DEBUG:py4j.clientserver:Command to send: c
o53
withColumn
ssurveydate_add_months
ro65
e

DEBUG:py4j.clientserver:Command to send: c
o54
withColumn
ssurveydate_add_months
ro66
e

DEBUG:py4j.clientserver:Answer received: !yro67
DEBUG:py4j.clientserver:Command to send: c
o67
apply
scity_name
e

DEBUG:py4j.clientserver:Answer received: !yro68
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro69
DEBUG:py4j.clientserver:Answer received: !yro70
DEBUG:py4j.clientserver:Command to send: c
o69
apply
scity_name
e

DEBUG:py4j.clientserver:Command to send: c
o70
apply
scity_name
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Answer received: !yro71
DEBUG:py4j.clientserver:Answer received: !yro72
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro68
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro73
DEBUG:py4j.clientserver:Command to send: c
o67
withColumn
scity_name
ro73
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro71
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro74
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro72
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: c
o70
withColumn
scity_name
ro74
e

DEBUG:py4j.clientserver:Answer received: !yro75
DEBUG:py4j.clientserver:Command to send: c
o69
withColumn
scity_name
ro75
e

DEBUG:py4j.clientserver:Answer received: !yro76
DEBUG:py4j.clientserver:Command to send: c
o76
apply
scity_name
e

DEBUG:py4j.clientserver:Answer received: !yro77
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro78
DEBUG:py4j.clientserver:Command to send: c
o78
apply
scity_name
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !yro79
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro80
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro81
DEBUG:py4j.clientserver:Command to send: c
o81
apply
scity_name
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro82
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro83
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro77
ro80
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro84
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro85
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro79
ro83
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro86
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro84
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !yro87
DEBUG:py4j.clientserver:Command to send: c
o76
withColumn
scity_name_add_months
ro87
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro82
ro85
e

DEBUG:py4j.clientserver:Answer received: !yro88
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro86
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro89
DEBUG:py4j.clientserver:Command to send: c
o78
withColumn
scity_name_add_months
ro89
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro88
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro90
DEBUG:py4j.clientserver:Command to send: c
o81
withColumn
scity_name_add_months
ro90
e

DEBUG:py4j.clientserver:Answer received: !yro91
DEBUG:py4j.clientserver:Command to send: c
o91
apply
sstate
e

DEBUG:py4j.clientserver:Answer received: !yro92
DEBUG:py4j.clientserver:Command to send: c
o92
apply
sstate
e

DEBUG:py4j.clientserver:Answer received: !yro93
DEBUG:py4j.clientserver:Answer received: !yro94
DEBUG:py4j.clientserver:Command to send: c
o93
apply
sstate
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro95
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro96
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro94
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro95
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro97
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro96
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro98
DEBUG:py4j.clientserver:Command to send: c
o91
withColumn
sstate
ro97
e

DEBUG:py4j.clientserver:Command to send: c
o92
withColumn
sstate
ro98
e

DEBUG:py4j.clientserver:Answer received: !yro99
DEBUG:py4j.clientserver:Command to send: c
o93
withColumn
sstate
ro99
e

DEBUG:py4j.clientserver:Answer received: !yro100
DEBUG:py4j.clientserver:Answer received: !yro101
DEBUG:py4j.clientserver:Command to send: c
o100
apply
sstate
e

DEBUG:py4j.clientserver:Answer received: !yro102
DEBUG:py4j.clientserver:Command to send: c
o101
apply
sstate
e

DEBUG:py4j.clientserver:Command to send: c
o102
apply
sstate
e

DEBUG:py4j.clientserver:Answer received: !yro103
DEBUG:py4j.clientserver:Answer received: !yro104
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro105
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro106
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro107
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro108
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro103
ro106
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro104
ro107
e

DEBUG:py4j.clientserver:Answer received: !yro109
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro110
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro105
ro108
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro111
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro109
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !yro112
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: c
o100
withColumn
sstate_add_months
ro112
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro110
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro111
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro113
DEBUG:py4j.clientserver:Command to send: c
o101
withColumn
sstate_add_months
ro113
e

DEBUG:py4j.clientserver:Answer received: !yro114
DEBUG:py4j.clientserver:Command to send: c
o102
withColumn
sstate_add_months
ro114
e

DEBUG:py4j.clientserver:Answer received: !yro116
DEBUG:py4j.clientserver:Answer received: !yro115
DEBUG:py4j.clientserver:Command to send: c
o116
apply
slatitude
e

DEBUG:py4j.clientserver:Command to send: c
o115
apply
slatitude
e

DEBUG:py4j.clientserver:Answer received: !yro117
DEBUG:py4j.clientserver:Answer received: !yro119
DEBUG:py4j.clientserver:Answer received: !yro118
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
o119
apply
slatitude
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro120
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro117
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro118
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro121
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro120
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro122
DEBUG:py4j.clientserver:Command to send: c
o116
withColumn
slatitude
ro121
e

DEBUG:py4j.clientserver:Command to send: c
o115
withColumn
slatitude
ro122
e

DEBUG:py4j.clientserver:Answer received: !yro123
DEBUG:py4j.clientserver:Command to send: c
o119
withColumn
slatitude
ro123
e

DEBUG:py4j.clientserver:Answer received: !yro124
DEBUG:py4j.clientserver:Command to send: c
o124
apply
slatitude
e

DEBUG:py4j.clientserver:Answer received: !yro125
DEBUG:py4j.clientserver:Command to send: c
o125
apply
slatitude
e

DEBUG:py4j.clientserver:Answer received: !yro126
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro127
DEBUG:py4j.clientserver:Command to send: c
o127
apply
slatitude
e

DEBUG:py4j.clientserver:Answer received: !yro128
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro129
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !yro130
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro131
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro132
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro126
ro130
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !yro133
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro128
ro131
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro129
ro132
e

DEBUG:py4j.clientserver:Answer received: !yro134
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro135
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro133
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !yro136
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: c
o124
withColumn
slatitude_add_months
ro136
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro134
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro135
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro137
DEBUG:py4j.clientserver:Command to send: c
o125
withColumn
slatitude_add_months
ro137
e

DEBUG:py4j.clientserver:Answer received: !yro138
DEBUG:py4j.clientserver:Command to send: c
o127
withColumn
slatitude_add_months
ro138
e

DEBUG:py4j.clientserver:Answer received: !yro139
DEBUG:py4j.clientserver:Command to send: c
o139
apply
slongitude
e

DEBUG:py4j.clientserver:Answer received: !yro140
DEBUG:py4j.clientserver:Command to send: c
o140
apply
slongitude
e

DEBUG:py4j.clientserver:Answer received: !yro141
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro142
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro143
DEBUG:py4j.clientserver:Command to send: c
o143
apply
slongitude
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Answer received: !yro144
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro141
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro145
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro142
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: c
o139
withColumn
slongitude
ro145
e

DEBUG:py4j.clientserver:Answer received: !yro146
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
o140
withColumn
slongitude
ro146
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro144
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro147
DEBUG:py4j.clientserver:Command to send: c
o143
withColumn
slongitude
ro147
e

DEBUG:py4j.clientserver:Answer received: !yro148
DEBUG:py4j.clientserver:Answer received: !yro149
DEBUG:py4j.clientserver:Command to send: c
o148
apply
slongitude
e

DEBUG:py4j.clientserver:Command to send: c
o149
apply
slongitude
e

DEBUG:py4j.clientserver:Answer received: !yro150
DEBUG:py4j.clientserver:Command to send: c
o150
apply
slongitude
e

DEBUG:py4j.clientserver:Answer received: !yro151
DEBUG:py4j.clientserver:Answer received: !yro152
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro153
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro154
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro155
DEBUG:py4j.clientserver:Answer received: !yro156
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro151
ro154
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !yro157
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro153
ro156
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro152
ro155
e

DEBUG:py4j.clientserver:Answer received: !yro158
DEBUG:py4j.clientserver:Answer received: !yro159
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro157
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro160
DEBUG:py4j.clientserver:Command to send: c
o148
withColumn
slongitude_add_months
ro160
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro158
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro159
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro161
DEBUG:py4j.clientserver:Answer received: !yro162
DEBUG:py4j.clientserver:Command to send: c
o150
withColumn
slongitude_add_months
ro161
e

DEBUG:py4j.clientserver:Command to send: c
o149
withColumn
slongitude_add_months
ro162
e

DEBUG:py4j.clientserver:Answer received: !yro163
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro164
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro165
DEBUG:py4j.clientserver:Command to send: c
o163
withColumn
scurrent_timestamp
ro165
e

DEBUG:py4j.clientserver:Answer received: !yro166
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro167
DEBUG:py4j.clientserver:Command to send: c
o164
withColumn
scurrent_timestamp
ro167
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro168
DEBUG:py4j.clientserver:Command to send: c
o166
withColumn
scurrent_timestamp
ro168
e

DEBUG:py4j.clientserver:Answer received: !yro169
DEBUG:py4j.clientserver:Command to send: c
o169
write
e

DEBUG:py4j.clientserver:Answer received: !yro170
DEBUG:py4j.clientserver:Command to send: c
o170
write
e

DEBUG:py4j.clientserver:Answer received: !yro171
DEBUG:py4j.clientserver:Command to send: c
o171
write
e

DEBUG:py4j.clientserver:Answer received: !yro172
DEBUG:py4j.clientserver:Command to send: c
o172
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro173
DEBUG:py4j.clientserver:Answer received: !yro174
DEBUG:py4j.clientserver:Command to send: c
o173
mode
soverwrite
e

DEBUG:py4j.clientserver:Command to send: c
o174
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro177
DEBUG:py4j.clientserver:Answer received: !yro176
DEBUG:py4j.clientserver:Answer received: !yro175
DEBUG:py4j.clientserver:Command to send: c
o177
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o176
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o175
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro178
DEBUG:py4j.clientserver:Answer received: !yro179
DEBUG:py4j.clientserver:Answer received: !yro180
DEBUG:py4j.clientserver:Command to send: c
o178
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-12-22-10-09.avro
e

DEBUG:py4j.clientserver:Command to send: c
o179
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-12-22-10-09.avro
e

DEBUG:py4j.clientserver:Command to send: c
o180
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-12-22-10-09.avro
e

DEBUG:py4j.clientserver:Command to send: m
d
o22
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o23
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o24
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o25
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o26
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o27
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o30
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o32
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o33
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o34
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o35
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o36
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o38
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o39
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o40
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o41
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o42
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o45
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o44
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o43
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o46
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o48
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o47
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o50
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o49
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o51
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o52
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o53
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o54
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o55
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o56
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o57
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o58
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o59
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o60
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o61
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o62
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o63
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o64
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o65
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o66
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o67
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o68
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o69
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o70
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o71
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o72
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o73
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o74
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o75
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o76
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o77
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o78
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o79
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o80
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o81
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o82
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o83
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o84
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o85
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o86
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o87
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o88
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o89
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o90
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o91
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o92
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o93
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o94
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o95
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o96
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o97
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o98
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o99
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !xro182
DEBUG:py4j.clientserver:Answer received: !xro181
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !xro183
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro182
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro181
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro183
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro182
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro181
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
o182
getMessage
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
o181
getMessage
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro183
e

DEBUG:py4j.clientserver:Answer received: !ys[INVALID_COLUMN_NAME_AS_PATH] The datasource AvroFileFormat cannot save the column `s`.`no` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !ys[INVALID_COLUMN_NAME_AS_PATH] The datasource AvroFileFormat cannot save the column `s`.`no` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Command to send: c
o183
getMessage
e

DEBUG:py4j.clientserver:Answer received: !ys[INVALID_COLUMN_NAME_AS_PATH] The datasource AvroFileFormat cannot save the column `s`.`no` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro182
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro181
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro183
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark.sql.AnalysisException: [INVALID_COLUMN_NAME_AS_PATH] The datasource AvroFileFormat cannot save the column `s`.`no` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.\r\n	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidColumnNameAsPathError(QueryCompilationErrors.scala:2760)\r\n	at org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$checkFieldNames$1(DataSourceUtils.scala:77)\r\n	at org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$checkFieldNames$1$adapted(DataSourceUtils.scala:74)\r\n	at scala.collection.Iterator.foreach(Iterator.scala:943)\r\n	at scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n	at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n	at org.apache.spark.sql.types.StructType.foreach(StructType.scala:105)\r\n	at org.apache.spark.sql.execution.datasources.DataSourceUtils$.checkFieldNames(DataSourceUtils.scala:74)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:120)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o182
getCause
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark.sql.AnalysisException: [INVALID_COLUMN_NAME_AS_PATH] The datasource AvroFileFormat cannot save the column `s`.`no` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.\r\n	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidColumnNameAsPathError(QueryCompilationErrors.scala:2760)\r\n	at org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$checkFieldNames$1(DataSourceUtils.scala:77)\r\n	at org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$checkFieldNames$1$adapted(DataSourceUtils.scala:74)\r\n	at scala.collection.Iterator.foreach(Iterator.scala:943)\r\n	at scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n	at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n	at org.apache.spark.sql.types.StructType.foreach(StructType.scala:105)\r\n	at org.apache.spark.sql.execution.datasources.DataSourceUtils$.checkFieldNames(DataSourceUtils.scala:74)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:120)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark.sql.AnalysisException: [INVALID_COLUMN_NAME_AS_PATH] The datasource AvroFileFormat cannot save the column `s`.`no` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.\r\n	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidColumnNameAsPathError(QueryCompilationErrors.scala:2760)\r\n	at org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$checkFieldNames$1(DataSourceUtils.scala:77)\r\n	at org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$checkFieldNames$1$adapted(DataSourceUtils.scala:74)\r\n	at scala.collection.Iterator.foreach(Iterator.scala:943)\r\n	at scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n	at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n	at org.apache.spark.sql.types.StructType.foreach(StructType.scala:105)\r\n	at org.apache.spark.sql.execution.datasources.DataSourceUtils$.checkFieldNames(DataSourceUtils.scala:74)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:120)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o181
getCause
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o183
getCause
e

DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.sql
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.sql
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.sql.internal
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.sql
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.sql.internal
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.sql.internal
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.internal.SQLConf
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.internal.SQLConf
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.internal.SQLConf
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

DEBUG:py4j.clientserver:Answer received: !yro184
DEBUG:py4j.clientserver:Answer received: !yro185
DEBUG:py4j.clientserver:Answer received: !yro186
DEBUG:py4j.clientserver:Command to send: c
o184
pysparkJVMStacktraceEnabled
e

DEBUG:py4j.clientserver:Command to send: c
o185
pysparkJVMStacktraceEnabled
e

DEBUG:py4j.clientserver:Command to send: c
o186
pysparkJVMStacktraceEnabled
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
ERROR:root:Error processing old_citydata.avro: [INVALID_COLUMN_NAME_AS_PATH] The datasource AvroFileFormat cannot save the column `s`.`no` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
DEBUG:py4j.clientserver:Answer received: !ybfalse
ERROR:root:Error processing old_citydata.avro: [INVALID_COLUMN_NAME_AS_PATH] The datasource AvroFileFormat cannot save the column `s`.`no` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
INFO:py4j.clientserver:Closing down clientserver connection
ERROR:root:Error processing old_citydata.avro: [INVALID_COLUMN_NAME_AS_PATH] The datasource AvroFileFormat cannot save the column `s`.`no` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
INFO:py4j.clientserver:Closing down clientserver connection
INFO:py4j.clientserver:Closing down clientserver connection
INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:asyncio:Using proactor: IocpProactor
DEBUG:py4j.java_gateway:GatewayClient.address is deprecated and will be removed in version 1.0. Use GatewayParameters instead.
DEBUG:py4j.clientserver:Command to send: A
afda6a6ae25d050ad196e393fc47a9f587a8e931c52766e4e23cee5443b63d56

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.SparkConf
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.api.java.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.api.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.ml.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.mllib.api.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.resource.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.sql.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.sql.api.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.sql.hive.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
scala.Tuple2
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkConf
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkConf
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.SparkConf
bTrue
e

DEBUG:py4j.clientserver:Answer received: !yro0
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yro1
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yro2
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.serializer.objectStreamReset
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.serializer.objectStreamReset
s100
e

DEBUG:py4j.clientserver:Answer received: !yro3
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.rdd.compress
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.rdd.compress
sTrue
e

DEBUG:py4j.clientserver:Answer received: !yro4
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.master
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.app.name
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.master
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
get
sspark.master
e

DEBUG:py4j.clientserver:Answer received: !yslocal[*]
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.app.name
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
get
sspark.app.name
e

DEBUG:py4j.clientserver:Answer received: !ysAvroProcessing
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.home
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o0
getAll
e

DEBUG:py4j.clientserver:Answer received: !yto5
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i0
e

DEBUG:py4j.clientserver:Answer received: !yro6
DEBUG:py4j.clientserver:Command to send: c
o6
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.app.submitTime
DEBUG:py4j.clientserver:Command to send: c
o6
_2
e

DEBUG:py4j.clientserver:Answer received: !ys1694536911586
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i1
e

DEBUG:py4j.clientserver:Answer received: !yro7
DEBUG:py4j.clientserver:Command to send: c
o7
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.rdd.compress
DEBUG:py4j.clientserver:Command to send: c
o7
_2
e

DEBUG:py4j.clientserver:Answer received: !ysTrue
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i2
e

DEBUG:py4j.clientserver:Answer received: !yro8
DEBUG:py4j.clientserver:Command to send: c
o8
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.app.name
DEBUG:py4j.clientserver:Command to send: c
o8
_2
e

DEBUG:py4j.clientserver:Answer received: !ysAvroProcessing
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i3
e

DEBUG:py4j.clientserver:Answer received: !yro9
DEBUG:py4j.clientserver:Command to send: c
o9
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.serializer.objectStreamReset
DEBUG:py4j.clientserver:Command to send: c
o9
_2
e

DEBUG:py4j.clientserver:Answer received: !ys100
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i4
e

DEBUG:py4j.clientserver:Answer received: !yro10
DEBUG:py4j.clientserver:Command to send: c
o10
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.submit.pyFiles
DEBUG:py4j.clientserver:Command to send: c
o10
_2
e

DEBUG:py4j.clientserver:Answer received: !ysC:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i5
e

DEBUG:py4j.clientserver:Answer received: !yro11
DEBUG:py4j.clientserver:Command to send: c
o11
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.master
DEBUG:py4j.clientserver:Command to send: c
o11
_2
e

DEBUG:py4j.clientserver:Answer received: !yslocal[*]
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i6
e

DEBUG:py4j.clientserver:Answer received: !yro12
DEBUG:py4j.clientserver:Command to send: c
o12
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.jars.packages
DEBUG:py4j.clientserver:Command to send: c
o12
_2
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark:spark-avro_2.12:3.4.1
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i7
e

DEBUG:py4j.clientserver:Answer received: !yro13
DEBUG:py4j.clientserver:Command to send: c
o13
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.submit.deployMode
DEBUG:py4j.clientserver:Command to send: c
o13
_2
e

DEBUG:py4j.clientserver:Answer received: !ysclient
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i8
e

DEBUG:py4j.clientserver:Answer received: !yro14
DEBUG:py4j.clientserver:Command to send: c
o14
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.files
DEBUG:py4j.clientserver:Command to send: c
o14
_2
e

DEBUG:py4j.clientserver:Answer received: !ysfile:///C:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,file:///C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i9
e

DEBUG:py4j.clientserver:Answer received: !yro15
DEBUG:py4j.clientserver:Command to send: c
o15
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.repl.local.jars
DEBUG:py4j.clientserver:Command to send: c
o15
_2
e

DEBUG:py4j.clientserver:Answer received: !ysfile:///C:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,file:///C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i10
e

DEBUG:py4j.clientserver:Answer received: !yro16
DEBUG:py4j.clientserver:Command to send: c
o16
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.jars
DEBUG:py4j.clientserver:Command to send: c
o16
_2
e

DEBUG:py4j.clientserver:Answer received: !ysfile:///C:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,file:///C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i11
e

DEBUG:py4j.clientserver:Answer received: !yro17
DEBUG:py4j.clientserver:Command to send: c
o17
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.ui.showConsoleProgress
DEBUG:py4j.clientserver:Command to send: c
o17
_2
e

DEBUG:py4j.clientserver:Answer received: !ystrue
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: r
u
JavaSparkContext
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.java.JavaSparkContext
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.api.java.JavaSparkContext
ro0
e

DEBUG:py4j.clientserver:Command to send: A
afda6a6ae25d050ad196e393fc47a9f587a8e931c52766e4e23cee5443b63d56

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o1
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o2
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o3
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o4
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o5
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro18
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro19
DEBUG:py4j.clientserver:Command to send: c
o19
conf
e

DEBUG:py4j.clientserver:Answer received: !yro20
DEBUG:py4j.clientserver:Command to send: r
u
PythonAccumulatorV2
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonAccumulatorV2
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.api.python.PythonAccumulatorV2
s127.0.0.1
i54637
safda6a6ae25d050ad196e393fc47a9f587a8e931c52766e4e23cee5443b63d56
e

DEBUG:py4j.clientserver:Answer received: !yro21
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro22
DEBUG:py4j.clientserver:Command to send: c
o22
register
ro21
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
PythonUtils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.api.python.PythonUtils
isEncryptionEnabled
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.api.python.PythonUtils
isEncryptionEnabled
ro18
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
PythonUtils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.api.python.PythonUtils
getPythonAuthSocketTimeout
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.api.python.PythonUtils
getPythonAuthSocketTimeout
ro18
e

DEBUG:py4j.clientserver:Answer received: !yL15
DEBUG:py4j.clientserver:Command to send: r
u
PythonUtils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.api.python.PythonUtils
getSparkBufferSize
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.api.python.PythonUtils
getSparkBufferSize
ro18
e

DEBUG:py4j.clientserver:Answer received: !yi65536
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.SparkFiles
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-eebe98c2-1ef6-4ce8-888a-42c38454a7d3\\userFiles-791ca69f-4f89-4685-b483-198e8ad947bf
DEBUG:py4j.clientserver:Command to send: c
o20
get
sspark.submit.pyFiles
s
e

DEBUG:py4j.clientserver:Answer received: !ysC:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.SparkFiles
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-eebe98c2-1ef6-4ce8-888a-42c38454a7d3\\userFiles-791ca69f-4f89-4685-b483-198e8ad947bf
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.SparkFiles
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-eebe98c2-1ef6-4ce8-888a-42c38454a7d3\\userFiles-791ca69f-4f89-4685-b483-198e8ad947bf
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
getLocalDir
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro23
DEBUG:py4j.clientserver:Command to send: c
o23
conf
e

DEBUG:py4j.clientserver:Answer received: !yro24
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
getLocalDir
ro24
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-eebe98c2-1ef6-4ce8-888a-42c38454a7d3
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
createTempDir
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
createTempDir
sC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-eebe98c2-1ef6-4ce8-888a-42c38454a7d3
spyspark
e

DEBUG:py4j.clientserver:Answer received: !yro25
DEBUG:py4j.clientserver:Command to send: c
o25
getAbsolutePath
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-eebe98c2-1ef6-4ce8-888a-42c38454a7d3\\pyspark-b40d9e52-2cab-426c-b7b6-dbc472d220a4
DEBUG:py4j.clientserver:Command to send: c
o20
get
sspark.python.profile
sfalse
e

DEBUG:py4j.clientserver:Answer received: !ysfalse
DEBUG:py4j.clientserver:Command to send: c
o20
get
sspark.python.profile.memory
sfalse
e

DEBUG:py4j.clientserver:Answer received: !ysfalse
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
getDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
getDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !yro26
DEBUG:py4j.clientserver:Command to send: c
o26
isDefined
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro27
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao28
DEBUG:py4j.clientserver:Command to send: c
o28
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o28
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.sql.SparkSession
ro27
ro28
e

DEBUG:py4j.clientserver:Answer received: !yro29
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
setDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
setDefaultSession
ro29
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
setActiveSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
setActiveSession
ro29
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: m
d
o0
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o6
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o7
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o8
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o9
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o10
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o11
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o12
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o13
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o14
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o15
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o16
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o17
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o19
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o28
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro30
DEBUG:py4j.clientserver:Command to send: c
o30
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro31
DEBUG:py4j.clientserver:Command to send: c
o31
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro32
DEBUG:py4j.clientserver:Command to send: c
o32
schema
e

DEBUG:py4j.clientserver:Answer received: !yro33
DEBUG:py4j.clientserver:Command to send: c
o33
treeString
e

DEBUG:py4j.clientserver:Answer received: !ysroot\n |-- s.no: long (nullable = true)\n |-- surveydate: string (nullable = true)\n |-- city_name: string (nullable = true)\n |-- state: string (nullable = true)\n |-- latitude: string (nullable = true)\n |-- longitude: string (nullable = true)\n
DEBUG:py4j.clientserver:Command to send: c
o32
schema
e

DEBUG:py4j.clientserver:Answer received: !yro34
DEBUG:py4j.clientserver:Command to send: c
o34
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s.no","type":"long","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"string","nullable":true,"metadata":{}},{"name":"longitude","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o32
apply
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !yro35
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro35
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro36
DEBUG:py4j.clientserver:Command to send: c
o32
withColumn
ssurveydate
ro36
e

DEBUG:py4j.clientserver:Answer received: !yro37
DEBUG:py4j.clientserver:Command to send: c
o37
apply
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !yro38
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro39
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro38
ro39
e

DEBUG:py4j.clientserver:Answer received: !yro40
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro40
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro41
DEBUG:py4j.clientserver:Command to send: c
o37
withColumn
ssurveydate_add_months
ro41
e

DEBUG:py4j.clientserver:Answer received: !yro42
DEBUG:py4j.clientserver:Command to send: c
o42
apply
scity_name
e

DEBUG:py4j.clientserver:Answer received: !yro43
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro43
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro44
DEBUG:py4j.clientserver:Command to send: c
o42
withColumn
scity_name
ro44
e

DEBUG:py4j.clientserver:Answer received: !yro45
DEBUG:py4j.clientserver:Command to send: c
o45
apply
scity_name
e

DEBUG:py4j.clientserver:Answer received: !yro46
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro47
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro46
ro47
e

DEBUG:py4j.clientserver:Answer received: !yro48
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro48
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro49
DEBUG:py4j.clientserver:Command to send: c
o45
withColumn
scity_name_add_months
ro49
e

DEBUG:py4j.clientserver:Answer received: !yro50
DEBUG:py4j.clientserver:Command to send: c
o50
apply
sstate
e

DEBUG:py4j.clientserver:Answer received: !yro51
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro51
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro52
DEBUG:py4j.clientserver:Command to send: c
o50
withColumn
sstate
ro52
e

DEBUG:py4j.clientserver:Answer received: !yro53
DEBUG:py4j.clientserver:Command to send: c
o53
apply
sstate
e

DEBUG:py4j.clientserver:Answer received: !yro54
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro55
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro54
ro55
e

DEBUG:py4j.clientserver:Answer received: !yro56
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro56
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro57
DEBUG:py4j.clientserver:Command to send: c
o53
withColumn
sstate_add_months
ro57
e

DEBUG:py4j.clientserver:Answer received: !yro58
DEBUG:py4j.clientserver:Command to send: c
o58
apply
slatitude
e

DEBUG:py4j.clientserver:Answer received: !yro59
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro59
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro60
DEBUG:py4j.clientserver:Command to send: c
o58
withColumn
slatitude
ro60
e

DEBUG:py4j.clientserver:Answer received: !yro61
DEBUG:py4j.clientserver:Command to send: c
o61
apply
slatitude
e

DEBUG:py4j.clientserver:Answer received: !yro62
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro63
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro62
ro63
e

DEBUG:py4j.clientserver:Answer received: !yro64
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro64
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro65
DEBUG:py4j.clientserver:Command to send: c
o61
withColumn
slatitude_add_months
ro65
e

DEBUG:py4j.clientserver:Answer received: !yro66
DEBUG:py4j.clientserver:Command to send: c
o66
apply
slongitude
e

DEBUG:py4j.clientserver:Answer received: !yro67
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
to_date
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
to_date
ro67
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro68
DEBUG:py4j.clientserver:Command to send: c
o66
withColumn
slongitude
ro68
e

DEBUG:py4j.clientserver:Answer received: !yro69
DEBUG:py4j.clientserver:Command to send: c
o69
apply
slongitude
e

DEBUG:py4j.clientserver:Answer received: !yro70
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro71
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro70
ro71
e

DEBUG:py4j.clientserver:Answer received: !yro72
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro72
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro73
DEBUG:py4j.clientserver:Command to send: c
o69
withColumn
slongitude_add_months
ro73
e

DEBUG:py4j.clientserver:Answer received: !yro74
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro75
DEBUG:py4j.clientserver:Command to send: c
o74
withColumn
scurrent_timestamp
ro75
e

DEBUG:py4j.clientserver:Answer received: !yro76
DEBUG:py4j.clientserver:Command to send: c
o76
write
e

DEBUG:py4j.clientserver:Answer received: !yro77
DEBUG:py4j.clientserver:Command to send: c
o77
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro78
DEBUG:py4j.clientserver:Command to send: c
o78
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro79
DEBUG:py4j.clientserver:Command to send: c
o79
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-12-22-12-11.avro
e

DEBUG:py4j.clientserver:Answer received: !xro80
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro80
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro80
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o80
getMessage
e

DEBUG:py4j.clientserver:Answer received: !ys[INVALID_COLUMN_NAME_AS_PATH] The datasource AvroFileFormat cannot save the column `s`.`no` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro80
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark.sql.AnalysisException: [INVALID_COLUMN_NAME_AS_PATH] The datasource AvroFileFormat cannot save the column `s`.`no` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.\r\n	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidColumnNameAsPathError(QueryCompilationErrors.scala:2760)\r\n	at org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$checkFieldNames$1(DataSourceUtils.scala:77)\r\n	at org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$checkFieldNames$1$adapted(DataSourceUtils.scala:74)\r\n	at scala.collection.Iterator.foreach(Iterator.scala:943)\r\n	at scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n	at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n	at org.apache.spark.sql.types.StructType.foreach(StructType.scala:105)\r\n	at org.apache.spark.sql.execution.datasources.DataSourceUtils$.checkFieldNames(DataSourceUtils.scala:74)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:120)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o80
getCause
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.sql
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.sql.internal
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.internal.SQLConf
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

DEBUG:py4j.clientserver:Answer received: !yro81
DEBUG:py4j.clientserver:Command to send: c
o81
pysparkJVMStacktraceEnabled
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
ERROR:root:Error processing old_citydata.avro: [INVALID_COLUMN_NAME_AS_PATH] The datasource AvroFileFormat cannot save the column `s`.`no` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
INFO:py4j.clientserver:Closing down clientserver connection
INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:asyncio:Using proactor: IocpProactor
DEBUG:py4j.java_gateway:GatewayClient.address is deprecated and will be removed in version 1.0. Use GatewayParameters instead.
DEBUG:py4j.clientserver:Command to send: A
110b04894a22af6acf664491659d2f83e51e8d9c732ba049a52dda0de25ef41e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.SparkConf
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.api.java.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.api.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.ml.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.mllib.api.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.resource.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.sql.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.sql.api.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.sql.hive.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
scala.Tuple2
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkConf
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkConf
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.SparkConf
bTrue
e

DEBUG:py4j.clientserver:Answer received: !yro0
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yro1
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yro2
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.serializer.objectStreamReset
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.serializer.objectStreamReset
s100
e

DEBUG:py4j.clientserver:Answer received: !yro3
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.rdd.compress
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.rdd.compress
sTrue
e

DEBUG:py4j.clientserver:Answer received: !yro4
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.master
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.app.name
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.master
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
get
sspark.master
e

DEBUG:py4j.clientserver:Answer received: !yslocal[*]
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.app.name
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
get
sspark.app.name
e

DEBUG:py4j.clientserver:Answer received: !ysAvroProcessing
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.home
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o0
getAll
e

DEBUG:py4j.clientserver:Answer received: !yto5
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i0
e

DEBUG:py4j.clientserver:Answer received: !yro6
DEBUG:py4j.clientserver:Command to send: c
o6
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.app.submitTime
DEBUG:py4j.clientserver:Command to send: c
o6
_2
e

DEBUG:py4j.clientserver:Answer received: !ys1694538810718
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i1
e

DEBUG:py4j.clientserver:Answer received: !yro7
DEBUG:py4j.clientserver:Command to send: c
o7
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.rdd.compress
DEBUG:py4j.clientserver:Command to send: c
o7
_2
e

DEBUG:py4j.clientserver:Answer received: !ysTrue
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i2
e

DEBUG:py4j.clientserver:Answer received: !yro8
DEBUG:py4j.clientserver:Command to send: c
o8
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.app.name
DEBUG:py4j.clientserver:Command to send: c
o8
_2
e

DEBUG:py4j.clientserver:Answer received: !ysAvroProcessing
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i3
e

DEBUG:py4j.clientserver:Answer received: !yro9
DEBUG:py4j.clientserver:Command to send: c
o9
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.serializer.objectStreamReset
DEBUG:py4j.clientserver:Command to send: c
o9
_2
e

DEBUG:py4j.clientserver:Answer received: !ys100
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i4
e

DEBUG:py4j.clientserver:Answer received: !yro10
DEBUG:py4j.clientserver:Command to send: c
o10
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.submit.pyFiles
DEBUG:py4j.clientserver:Command to send: c
o10
_2
e

DEBUG:py4j.clientserver:Answer received: !ysC:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i5
e

DEBUG:py4j.clientserver:Answer received: !yro11
DEBUG:py4j.clientserver:Command to send: c
o11
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.master
DEBUG:py4j.clientserver:Command to send: c
o11
_2
e

DEBUG:py4j.clientserver:Answer received: !yslocal[*]
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i6
e

DEBUG:py4j.clientserver:Answer received: !yro12
DEBUG:py4j.clientserver:Command to send: c
o12
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.jars.packages
DEBUG:py4j.clientserver:Command to send: c
o12
_2
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark:spark-avro_2.12:3.4.1
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i7
e

DEBUG:py4j.clientserver:Answer received: !yro13
DEBUG:py4j.clientserver:Command to send: c
o13
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.submit.deployMode
DEBUG:py4j.clientserver:Command to send: c
o13
_2
e

DEBUG:py4j.clientserver:Answer received: !ysclient
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i8
e

DEBUG:py4j.clientserver:Answer received: !yro14
DEBUG:py4j.clientserver:Command to send: c
o14
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.files
DEBUG:py4j.clientserver:Command to send: c
o14
_2
e

DEBUG:py4j.clientserver:Answer received: !ysfile:///C:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,file:///C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i9
e

DEBUG:py4j.clientserver:Answer received: !yro15
DEBUG:py4j.clientserver:Command to send: c
o15
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.repl.local.jars
DEBUG:py4j.clientserver:Command to send: c
o15
_2
e

DEBUG:py4j.clientserver:Answer received: !ysfile:///C:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,file:///C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i10
e

DEBUG:py4j.clientserver:Answer received: !yro16
DEBUG:py4j.clientserver:Command to send: c
o16
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.jars
DEBUG:py4j.clientserver:Command to send: c
o16
_2
e

DEBUG:py4j.clientserver:Answer received: !ysfile:///C:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,file:///C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i11
e

DEBUG:py4j.clientserver:Answer received: !yro17
DEBUG:py4j.clientserver:Command to send: c
o17
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.ui.showConsoleProgress
DEBUG:py4j.clientserver:Command to send: c
o17
_2
e

DEBUG:py4j.clientserver:Answer received: !ystrue
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: r
u
JavaSparkContext
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.java.JavaSparkContext
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.api.java.JavaSparkContext
ro0
e

DEBUG:py4j.clientserver:Command to send: A
110b04894a22af6acf664491659d2f83e51e8d9c732ba049a52dda0de25ef41e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o1
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o2
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o3
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o4
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o5
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro18
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro19
DEBUG:py4j.clientserver:Command to send: c
o19
conf
e

DEBUG:py4j.clientserver:Answer received: !yro20
DEBUG:py4j.clientserver:Command to send: r
u
PythonAccumulatorV2
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonAccumulatorV2
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.api.python.PythonAccumulatorV2
s127.0.0.1
i55893
s110b04894a22af6acf664491659d2f83e51e8d9c732ba049a52dda0de25ef41e
e

DEBUG:py4j.clientserver:Answer received: !yro21
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro22
DEBUG:py4j.clientserver:Command to send: c
o22
register
ro21
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
PythonUtils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.api.python.PythonUtils
isEncryptionEnabled
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.api.python.PythonUtils
isEncryptionEnabled
ro18
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
PythonUtils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.api.python.PythonUtils
getPythonAuthSocketTimeout
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.api.python.PythonUtils
getPythonAuthSocketTimeout
ro18
e

DEBUG:py4j.clientserver:Answer received: !yL15
DEBUG:py4j.clientserver:Command to send: r
u
PythonUtils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.api.python.PythonUtils
getSparkBufferSize
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.api.python.PythonUtils
getSparkBufferSize
ro18
e

DEBUG:py4j.clientserver:Answer received: !yi65536
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.SparkFiles
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-7be9c0cf-0bf0-4795-a078-79698012547f\\userFiles-41088f63-0ff7-4515-819f-374ac23beab0
DEBUG:py4j.clientserver:Command to send: c
o20
get
sspark.submit.pyFiles
s
e

DEBUG:py4j.clientserver:Answer received: !ysC:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.SparkFiles
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-7be9c0cf-0bf0-4795-a078-79698012547f\\userFiles-41088f63-0ff7-4515-819f-374ac23beab0
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.SparkFiles
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-7be9c0cf-0bf0-4795-a078-79698012547f\\userFiles-41088f63-0ff7-4515-819f-374ac23beab0
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
getLocalDir
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro23
DEBUG:py4j.clientserver:Command to send: c
o23
conf
e

DEBUG:py4j.clientserver:Answer received: !yro24
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
getLocalDir
ro24
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-7be9c0cf-0bf0-4795-a078-79698012547f
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
createTempDir
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
createTempDir
sC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-7be9c0cf-0bf0-4795-a078-79698012547f
spyspark
e

DEBUG:py4j.clientserver:Answer received: !yro25
DEBUG:py4j.clientserver:Command to send: c
o25
getAbsolutePath
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-7be9c0cf-0bf0-4795-a078-79698012547f\\pyspark-d401c100-cdb6-4705-823c-f6dc2abf8c9e
DEBUG:py4j.clientserver:Command to send: c
o20
get
sspark.python.profile
sfalse
e

DEBUG:py4j.clientserver:Answer received: !ysfalse
DEBUG:py4j.clientserver:Command to send: c
o20
get
sspark.python.profile.memory
sfalse
e

DEBUG:py4j.clientserver:Answer received: !ysfalse
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
getDefaultSession
e

DEBUG:py4j.clientserver:Command to send: m
d
o0
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o6
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o7
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o8
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o9
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o10
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o11
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o12
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o13
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o14
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o15
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o16
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o17
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o19
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
getDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !yro26
DEBUG:py4j.clientserver:Command to send: c
o26
isDefined
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro27
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao28
DEBUG:py4j.clientserver:Command to send: c
o28
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o28
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.sql.SparkSession
ro27
ro28
e

DEBUG:py4j.clientserver:Answer received: !yro29
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
setDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
setDefaultSession
ro29
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
setActiveSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
setActiveSession
ro29
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
110b04894a22af6acf664491659d2f83e51e8d9c732ba049a52dda0de25ef41e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro30
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao31
DEBUG:py4j.clientserver:Command to send: c
o31
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o31
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o30
applyModifiableSettings
ro29
ro31
e

DEBUG:py4j.clientserver:Command to send: m
d
o28
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro32
DEBUG:py4j.clientserver:Command to send: c
o32
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro33
DEBUG:py4j.clientserver:Command to send: c
o33
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro34
DEBUG:py4j.clientserver:Command to send: c
o34
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro35
DEBUG:py4j.clientserver:Command to send: c
o35
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Command to send: m
d
o31
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro37
DEBUG:py4j.clientserver:Answer received: !yro36
DEBUG:py4j.clientserver:Command to send: c
o37
schema
e

DEBUG:py4j.clientserver:Command to send: c
o36
schema
e

DEBUG:py4j.clientserver:Answer received: !yro39
DEBUG:py4j.clientserver:Answer received: !yro38
DEBUG:py4j.clientserver:Command to send: c
o39
json
e

DEBUG:py4j.clientserver:Command to send: c
o38
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"long","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"long","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"long","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"long","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
ERROR:root:Error processing old_empdata.avro: 'function' object is not subscriptable
ERROR:root:Error processing old_empdata.avro: 'function' object is not subscriptable
INFO:py4j.clientserver:Closing down clientserver connection
INFO:py4j.clientserver:Closing down clientserver connection
INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:asyncio:Using proactor: IocpProactor
DEBUG:py4j.java_gateway:GatewayClient.address is deprecated and will be removed in version 1.0. Use GatewayParameters instead.
DEBUG:py4j.clientserver:Command to send: A
1f5beb19c478264f83830e3e36a006627a65ca194745e831ba17f50b32a6ab13

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.SparkConf
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.api.java.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.api.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.ml.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.mllib.api.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.resource.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.sql.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.sql.api.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.sql.hive.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
scala.Tuple2
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkConf
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkConf
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.SparkConf
bTrue
e

DEBUG:py4j.clientserver:Answer received: !yro0
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yro1
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yro2
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.serializer.objectStreamReset
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.serializer.objectStreamReset
s100
e

DEBUG:py4j.clientserver:Answer received: !yro3
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.rdd.compress
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.rdd.compress
sTrue
e

DEBUG:py4j.clientserver:Answer received: !yro4
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.master
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.app.name
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.master
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
get
sspark.master
e

DEBUG:py4j.clientserver:Answer received: !yslocal[*]
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.app.name
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
get
sspark.app.name
e

DEBUG:py4j.clientserver:Answer received: !ysAvroProcessing
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.home
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o0
getAll
e

DEBUG:py4j.clientserver:Answer received: !yto5
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i0
e

DEBUG:py4j.clientserver:Answer received: !yro6
DEBUG:py4j.clientserver:Command to send: c
o6
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.rdd.compress
DEBUG:py4j.clientserver:Command to send: c
o6
_2
e

DEBUG:py4j.clientserver:Answer received: !ysTrue
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i1
e

DEBUG:py4j.clientserver:Answer received: !yro7
DEBUG:py4j.clientserver:Command to send: c
o7
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.app.name
DEBUG:py4j.clientserver:Command to send: c
o7
_2
e

DEBUG:py4j.clientserver:Answer received: !ysAvroProcessing
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i2
e

DEBUG:py4j.clientserver:Answer received: !yro8
DEBUG:py4j.clientserver:Command to send: c
o8
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.serializer.objectStreamReset
DEBUG:py4j.clientserver:Command to send: c
o8
_2
e

DEBUG:py4j.clientserver:Answer received: !ys100
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i3
e

DEBUG:py4j.clientserver:Answer received: !yro9
DEBUG:py4j.clientserver:Command to send: c
o9
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.submit.pyFiles
DEBUG:py4j.clientserver:Command to send: c
o9
_2
e

DEBUG:py4j.clientserver:Answer received: !ysC:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i4
e

DEBUG:py4j.clientserver:Answer received: !yro10
DEBUG:py4j.clientserver:Command to send: c
o10
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.master
DEBUG:py4j.clientserver:Command to send: c
o10
_2
e

DEBUG:py4j.clientserver:Answer received: !yslocal[*]
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i5
e

DEBUG:py4j.clientserver:Answer received: !yro11
DEBUG:py4j.clientserver:Command to send: c
o11
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.jars.packages
DEBUG:py4j.clientserver:Command to send: c
o11
_2
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark:spark-avro_2.12:3.4.1
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i6
e

DEBUG:py4j.clientserver:Answer received: !yro12
DEBUG:py4j.clientserver:Command to send: c
o12
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.submit.deployMode
DEBUG:py4j.clientserver:Command to send: c
o12
_2
e

DEBUG:py4j.clientserver:Answer received: !ysclient
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i7
e

DEBUG:py4j.clientserver:Answer received: !yro13
DEBUG:py4j.clientserver:Command to send: c
o13
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.files
DEBUG:py4j.clientserver:Command to send: c
o13
_2
e

DEBUG:py4j.clientserver:Answer received: !ysfile:///C:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,file:///C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i8
e

DEBUG:py4j.clientserver:Answer received: !yro14
DEBUG:py4j.clientserver:Command to send: c
o14
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.repl.local.jars
DEBUG:py4j.clientserver:Command to send: c
o14
_2
e

DEBUG:py4j.clientserver:Answer received: !ysfile:///C:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,file:///C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i9
e

DEBUG:py4j.clientserver:Answer received: !yro15
DEBUG:py4j.clientserver:Command to send: c
o15
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.app.submitTime
DEBUG:py4j.clientserver:Command to send: c
o15
_2
e

DEBUG:py4j.clientserver:Answer received: !ys1694581785225
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i10
e

DEBUG:py4j.clientserver:Answer received: !yro16
DEBUG:py4j.clientserver:Command to send: c
o16
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.jars
DEBUG:py4j.clientserver:Command to send: c
o16
_2
e

DEBUG:py4j.clientserver:Answer received: !ysfile:///C:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,file:///C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i11
e

DEBUG:py4j.clientserver:Answer received: !yro17
DEBUG:py4j.clientserver:Command to send: c
o17
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.ui.showConsoleProgress
DEBUG:py4j.clientserver:Command to send: c
o17
_2
e

DEBUG:py4j.clientserver:Answer received: !ystrue
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: r
u
JavaSparkContext
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.java.JavaSparkContext
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.api.java.JavaSparkContext
ro0
e

DEBUG:py4j.clientserver:Command to send: A
1f5beb19c478264f83830e3e36a006627a65ca194745e831ba17f50b32a6ab13

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o1
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o2
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o3
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o4
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o5
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro18
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro19
DEBUG:py4j.clientserver:Command to send: c
o19
conf
e

DEBUG:py4j.clientserver:Answer received: !yro20
DEBUG:py4j.clientserver:Command to send: r
u
PythonAccumulatorV2
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonAccumulatorV2
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.api.python.PythonAccumulatorV2
s127.0.0.1
i56897
s1f5beb19c478264f83830e3e36a006627a65ca194745e831ba17f50b32a6ab13
e

DEBUG:py4j.clientserver:Answer received: !yro21
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro22
DEBUG:py4j.clientserver:Command to send: c
o22
register
ro21
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
PythonUtils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.api.python.PythonUtils
isEncryptionEnabled
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.api.python.PythonUtils
isEncryptionEnabled
ro18
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
PythonUtils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.api.python.PythonUtils
getPythonAuthSocketTimeout
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.api.python.PythonUtils
getPythonAuthSocketTimeout
ro18
e

DEBUG:py4j.clientserver:Answer received: !yL15
DEBUG:py4j.clientserver:Command to send: r
u
PythonUtils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.api.python.PythonUtils
getSparkBufferSize
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.api.python.PythonUtils
getSparkBufferSize
ro18
e

DEBUG:py4j.clientserver:Answer received: !yi65536
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.SparkFiles
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-971a7830-3eb1-4bc7-ba06-2ff249277b55\\userFiles-819fa425-9ecc-40d4-a4c7-a5e209d91bc8
DEBUG:py4j.clientserver:Command to send: c
o20
get
sspark.submit.pyFiles
s
e

DEBUG:py4j.clientserver:Answer received: !ysC:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.SparkFiles
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-971a7830-3eb1-4bc7-ba06-2ff249277b55\\userFiles-819fa425-9ecc-40d4-a4c7-a5e209d91bc8
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.SparkFiles
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-971a7830-3eb1-4bc7-ba06-2ff249277b55\\userFiles-819fa425-9ecc-40d4-a4c7-a5e209d91bc8
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
getLocalDir
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro23
DEBUG:py4j.clientserver:Command to send: c
o23
conf
e

DEBUG:py4j.clientserver:Answer received: !yro24
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
getLocalDir
ro24
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-971a7830-3eb1-4bc7-ba06-2ff249277b55
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
createTempDir
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
createTempDir
sC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-971a7830-3eb1-4bc7-ba06-2ff249277b55
spyspark
e

DEBUG:py4j.clientserver:Answer received: !yro25
DEBUG:py4j.clientserver:Command to send: c
o25
getAbsolutePath
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-971a7830-3eb1-4bc7-ba06-2ff249277b55\\pyspark-52b5ad47-7e8f-4a85-b169-9ec6229acdcb
DEBUG:py4j.clientserver:Command to send: c
o20
get
sspark.python.profile
sfalse
e

DEBUG:py4j.clientserver:Answer received: !ysfalse
DEBUG:py4j.clientserver:Command to send: c
o20
get
sspark.python.profile.memory
sfalse
e

DEBUG:py4j.clientserver:Answer received: !ysfalse
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
getDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
getDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !yro26
DEBUG:py4j.clientserver:Command to send: c
o26
isDefined
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro27
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao28
DEBUG:py4j.clientserver:Command to send: c
o28
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o28
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.sql.SparkSession
ro27
ro28
e

DEBUG:py4j.clientserver:Answer received: !yro29
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
setDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
setDefaultSession
ro29
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
setActiveSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
setActiveSession
ro29
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
1f5beb19c478264f83830e3e36a006627a65ca194745e831ba17f50b32a6ab13

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro30
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao31
DEBUG:py4j.clientserver:Command to send: c
o31
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o31
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o30
applyModifiableSettings
ro29
ro31
e

DEBUG:py4j.clientserver:Command to send: m
d
o28
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro32
DEBUG:py4j.clientserver:Command to send: c
o32
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro33
DEBUG:py4j.clientserver:Command to send: c
o33
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
1f5beb19c478264f83830e3e36a006627a65ca194745e831ba17f50b32a6ab13

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro34
DEBUG:py4j.clientserver:Command to send: c
o34
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro35
DEBUG:py4j.clientserver:Command to send: c
o35
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro36
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao37
DEBUG:py4j.clientserver:Command to send: c
o37
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o37
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o36
applyModifiableSettings
ro29
ro37
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
1f5beb19c478264f83830e3e36a006627a65ca194745e831ba17f50b32a6ab13

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro38
DEBUG:py4j.clientserver:Command to send: c
o38
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro39
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o39
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro40
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao41
DEBUG:py4j.clientserver:Command to send: c
o41
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o41
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o40
applyModifiableSettings
ro29
ro41
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
1f5beb19c478264f83830e3e36a006627a65ca194745e831ba17f50b32a6ab13

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro42
DEBUG:py4j.clientserver:Command to send: c
o42
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro43
DEBUG:py4j.clientserver:Command to send: c
o43
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro44
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao45
DEBUG:py4j.clientserver:Command to send: c
o45
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o45
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o44
applyModifiableSettings
ro29
ro45
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
1f5beb19c478264f83830e3e36a006627a65ca194745e831ba17f50b32a6ab13

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o31
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: m
d
o37
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o41
e

DEBUG:py4j.clientserver:Answer received: !yro46
DEBUG:py4j.clientserver:Command to send: c
o46
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o45
e

DEBUG:py4j.clientserver:Answer received: !yro47
DEBUG:py4j.clientserver:Command to send: c
o47
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro48
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao49
DEBUG:py4j.clientserver:Command to send: c
o49
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o49
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o48
applyModifiableSettings
ro29
ro49
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
1f5beb19c478264f83830e3e36a006627a65ca194745e831ba17f50b32a6ab13

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro50
DEBUG:py4j.clientserver:Command to send: c
o50
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro51
DEBUG:py4j.clientserver:Command to send: c
o51
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro52
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao53
DEBUG:py4j.clientserver:Command to send: c
o53
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o53
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o52
applyModifiableSettings
ro29
ro53
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: A
1f5beb19c478264f83830e3e36a006627a65ca194745e831ba17f50b32a6ab13

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro54
DEBUG:py4j.clientserver:Command to send: c
o54
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro55
DEBUG:py4j.clientserver:Command to send: c
o55
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro56
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao57
DEBUG:py4j.clientserver:Command to send: c
o57
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o57
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o56
applyModifiableSettings
ro29
ro57
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
1f5beb19c478264f83830e3e36a006627a65ca194745e831ba17f50b32a6ab13

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro58
DEBUG:py4j.clientserver:Command to send: c
o58
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro59
DEBUG:py4j.clientserver:Command to send: c
o59
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro60
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao61
DEBUG:py4j.clientserver:Command to send: c
o61
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o61
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o60
applyModifiableSettings
ro29
ro61
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro62
DEBUG:py4j.clientserver:Command to send: c
o62
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro63
DEBUG:py4j.clientserver:Command to send: c
o63
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Command to send: A
1f5beb19c478264f83830e3e36a006627a65ca194745e831ba17f50b32a6ab13

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro64
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao65
DEBUG:py4j.clientserver:Command to send: c
o65
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o65
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o64
applyModifiableSettings
ro29
ro65
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro66
DEBUG:py4j.clientserver:Command to send: c
o66
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro67
DEBUG:py4j.clientserver:Command to send: c
o67
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Command to send: A
1f5beb19c478264f83830e3e36a006627a65ca194745e831ba17f50b32a6ab13

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro68
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao69
DEBUG:py4j.clientserver:Command to send: c
o69
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o69
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o68
applyModifiableSettings
ro29
ro69
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro70
DEBUG:py4j.clientserver:Command to send: c
o70
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro71
DEBUG:py4j.clientserver:Command to send: c
o71
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Command to send: m
d
o49
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o53
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o57
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o0
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o6
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o7
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o8
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o9
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o10
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o11
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o12
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o13
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o14
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o15
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o16
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o17
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o19
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o22
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o23
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o24
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o25
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o26
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o27
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o30
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o32
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o34
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o36
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o38
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o40
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o42
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o44
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o46
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o48
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o50
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o52
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o54
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o56
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o58
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o61
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o65
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o69
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro72
DEBUG:py4j.clientserver:Answer received: !yro74
DEBUG:py4j.clientserver:Answer received: !yro73
DEBUG:py4j.clientserver:Answer received: !yro76
DEBUG:py4j.clientserver:Answer received: !yro75
DEBUG:py4j.clientserver:Answer received: !yro77
DEBUG:py4j.clientserver:Command to send: c
o72
schema
e

DEBUG:py4j.clientserver:Answer received: !yro80
DEBUG:py4j.clientserver:Answer received: !yro79
DEBUG:py4j.clientserver:Answer received: !yro78
DEBUG:py4j.clientserver:Answer received: !yro81
DEBUG:py4j.clientserver:Answer received: !yro82
DEBUG:py4j.clientserver:Command to send: c
o74
schema
e

DEBUG:py4j.clientserver:Command to send: c
o73
schema
e

DEBUG:py4j.clientserver:Command to send: c
o76
schema
e

DEBUG:py4j.clientserver:Command to send: c
o75
schema
e

DEBUG:py4j.clientserver:Command to send: c
o77
schema
e

DEBUG:py4j.clientserver:Command to send: c
o80
schema
e

DEBUG:py4j.clientserver:Command to send: c
o79
schema
e

DEBUG:py4j.clientserver:Command to send: c
o78
schema
e

DEBUG:py4j.clientserver:Command to send: c
o81
schema
e

DEBUG:py4j.clientserver:Command to send: c
o82
schema
e

DEBUG:py4j.clientserver:Answer received: !yro86
DEBUG:py4j.clientserver:Answer received: !yro88
DEBUG:py4j.clientserver:Answer received: !yro84
DEBUG:py4j.clientserver:Answer received: !yro87
DEBUG:py4j.clientserver:Answer received: !yro85
DEBUG:py4j.clientserver:Answer received: !yro89
DEBUG:py4j.clientserver:Answer received: !yro83
DEBUG:py4j.clientserver:Answer received: !yro90
DEBUG:py4j.clientserver:Answer received: !yro92
DEBUG:py4j.clientserver:Answer received: !yro91
DEBUG:py4j.clientserver:Answer received: !yro93
DEBUG:py4j.clientserver:Command to send: c
o86
json
e

DEBUG:py4j.clientserver:Command to send: c
o88
json
e

DEBUG:py4j.clientserver:Command to send: c
o84
json
e

DEBUG:py4j.clientserver:Command to send: c
o87
json
e

DEBUG:py4j.clientserver:Command to send: c
o85
json
e

DEBUG:py4j.clientserver:Command to send: c
o89
json
e

DEBUG:py4j.clientserver:Command to send: c
o83
json
e

DEBUG:py4j.clientserver:Command to send: c
o90
json
e

DEBUG:py4j.clientserver:Command to send: c
o92
json
e

DEBUG:py4j.clientserver:Command to send: c
o91
json
e

DEBUG:py4j.clientserver:Command to send: c
o93
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !yro94
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !yro95
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro96
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o94
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o95
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o96
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro97
DEBUG:py4j.clientserver:Answer received: !yro99
DEBUG:py4j.clientserver:Answer received: !yro100
DEBUG:py4j.clientserver:Answer received: !yro98
DEBUG:py4j.clientserver:Answer received: !yro101
DEBUG:py4j.clientserver:Answer received: !yro102
DEBUG:py4j.clientserver:Answer received: !yro103
DEBUG:py4j.clientserver:Answer received: !yro104
DEBUG:py4j.clientserver:Command to send: c
o97
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro107
DEBUG:py4j.clientserver:Answer received: !yro105
DEBUG:py4j.clientserver:Answer received: !yro106
DEBUG:py4j.clientserver:Command to send: c
o99
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o100
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o98
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o101
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o102
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Command to send: c
o103
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o104
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o107
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Answer received: !yro108
DEBUG:py4j.clientserver:Command to send: c
o105
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Command to send: c
o106
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro109
DEBUG:py4j.clientserver:Command to send: c
o109
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Answer received: !yro110
DEBUG:py4j.clientserver:Answer received: !yro113
DEBUG:py4j.clientserver:Command to send: c
o113
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Answer received: !yro115
DEBUG:py4j.clientserver:Command to send: c
o115
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Answer received: !yro112
DEBUG:py4j.clientserver:Command to send: c
o112
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Command to send: c
o110
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Answer received: !yro111
DEBUG:py4j.clientserver:Command to send: c
o111
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Answer received: !yro114
DEBUG:py4j.clientserver:Command to send: c
o114
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Command to send: c
o108
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Answer received: !yro116
DEBUG:py4j.clientserver:Command to send: c
o116
schema
e

DEBUG:py4j.clientserver:Answer received: !yro117
DEBUG:py4j.clientserver:Command to send: c
o117
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro118
DEBUG:py4j.clientserver:Command to send: c
o118
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro119
DEBUG:py4j.clientserver:Command to send: c
o119
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Answer received: !yro120
DEBUG:py4j.clientserver:Command to send: c
o120
schema
e

DEBUG:py4j.clientserver:Answer received: !yro121
DEBUG:py4j.clientserver:Command to send: c
o121
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro122
DEBUG:py4j.clientserver:Command to send: c
o122
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro123
DEBUG:py4j.clientserver:Command to send: c
o123
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Answer received: !yro124
DEBUG:py4j.clientserver:Command to send: c
o124
schema
e

DEBUG:py4j.clientserver:Answer received: !yro125
DEBUG:py4j.clientserver:Command to send: c
o125
schema
e

DEBUG:py4j.clientserver:Answer received: !yro126
DEBUG:py4j.clientserver:Command to send: c
o126
json
e

DEBUG:py4j.clientserver:Answer received: !yro127
DEBUG:py4j.clientserver:Command to send: c
o127
schema
e

DEBUG:py4j.clientserver:Answer received: !yro128
DEBUG:py4j.clientserver:Command to send: c
o128
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro129
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !yro130
DEBUG:py4j.clientserver:Command to send: c
o129
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o130
schema
e

DEBUG:py4j.clientserver:Answer received: !yro131
DEBUG:py4j.clientserver:Answer received: !yro133
DEBUG:py4j.clientserver:Answer received: !yro132
DEBUG:py4j.clientserver:Command to send: c
o131
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Answer received: !yro134
DEBUG:py4j.clientserver:Command to send: c
o133
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o132
json
e

DEBUG:py4j.clientserver:Command to send: c
o134
json
e

DEBUG:py4j.clientserver:Answer received: !yro135
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o135
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro136
DEBUG:py4j.clientserver:Command to send: c
o136
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro137
DEBUG:py4j.clientserver:Command to send: c
o137
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Answer received: !yro138
DEBUG:py4j.clientserver:Command to send: c
o138
schema
e

DEBUG:py4j.clientserver:Answer received: !yro139
DEBUG:py4j.clientserver:Command to send: c
o139
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: m
d
o62
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o63
e

DEBUG:py4j.clientserver:Answer received: !yro140
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o140
schema
e

DEBUG:py4j.clientserver:Answer received: !yro141
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: m
d
o64
e

DEBUG:py4j.clientserver:Command to send: c
o141
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro142
DEBUG:py4j.clientserver:Command to send: c
o142
json
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o66
e

DEBUG:py4j.clientserver:Answer received: !yro143
DEBUG:py4j.clientserver:Command to send: c
o143
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o67
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: m
d
o68
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro144
DEBUG:py4j.clientserver:Command to send: m
d
o70
e

DEBUG:py4j.clientserver:Command to send: c
o144
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro145
DEBUG:py4j.clientserver:Command to send: c
o145
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Answer received: !yro146
DEBUG:py4j.clientserver:Command to send: m
d
o71
e

DEBUG:py4j.clientserver:Command to send: c
o146
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o72
e

DEBUG:py4j.clientserver:Answer received: !yro147
DEBUG:py4j.clientserver:Command to send: c
o147
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o74
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o77
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o80
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o81
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o82
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o86
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o88
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o84
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o87
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o85
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o89
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o83
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o90
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o92
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o91
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o93
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o94
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o95
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o96
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o97
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o99
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o100
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o98
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o101
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o103
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o104
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro148
DEBUG:py4j.clientserver:Command to send: m
d
o107
e

DEBUG:py4j.clientserver:Command to send: c
o148
schema
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro149
DEBUG:py4j.clientserver:Command to send: m
d
o106
e

DEBUG:py4j.clientserver:Command to send: c
o149
json
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o108
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: m
d
o110
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o113
e

DEBUG:py4j.clientserver:Answer received: !yro150
DEBUG:py4j.clientserver:Command to send: c
o150
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o111
e

DEBUG:py4j.clientserver:Answer received: !yro151
DEBUG:py4j.clientserver:Command to send: c
o151
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o114
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o117
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o118
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o121
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o122
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o126
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o128
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o129
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o133
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o132
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro152
DEBUG:py4j.clientserver:Answer received: !yro153
DEBUG:py4j.clientserver:Command to send: c
o152
schema
e

DEBUG:py4j.clientserver:Command to send: c
o153
schema
e

DEBUG:py4j.clientserver:Answer received: !yro154
DEBUG:py4j.clientserver:Answer received: !yro155
DEBUG:py4j.clientserver:Command to send: c
o154
json
e

DEBUG:py4j.clientserver:Command to send: c
o155
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro156
DEBUG:py4j.clientserver:Answer received: !yro157
DEBUG:py4j.clientserver:Command to send: c
o156
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o157
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro158
DEBUG:py4j.clientserver:Answer received: !yro159
DEBUG:py4j.clientserver:Command to send: c
o158
schema
e

DEBUG:py4j.clientserver:Answer received: !yro160
DEBUG:py4j.clientserver:Command to send: c
o159
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Command to send: c
o160
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Answer received: !yro161
DEBUG:py4j.clientserver:Command to send: c
o161
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro162
DEBUG:py4j.clientserver:Command to send: c
o162
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro163
DEBUG:py4j.clientserver:Command to send: c
o163
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro164
DEBUG:py4j.clientserver:Command to send: c
o164
schema
e

DEBUG:py4j.clientserver:Answer received: !yro165
DEBUG:py4j.clientserver:Command to send: c
o165
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro166
DEBUG:py4j.clientserver:Command to send: c
o166
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro167
DEBUG:py4j.clientserver:Command to send: c
o167
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro168
DEBUG:py4j.clientserver:Command to send: c
o168
schema
e

DEBUG:py4j.clientserver:Answer received: !yro170
DEBUG:py4j.clientserver:Answer received: !yro169
DEBUG:py4j.clientserver:Command to send: c
o170
schema
e

DEBUG:py4j.clientserver:Command to send: c
o169
schema
e

DEBUG:py4j.clientserver:Answer received: !yro171
DEBUG:py4j.clientserver:Command to send: c
o171
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro172
DEBUG:py4j.clientserver:Command to send: c
o172
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro173
DEBUG:py4j.clientserver:Command to send: c
o173
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro174
DEBUG:py4j.clientserver:Command to send: c
o174
schema
e

DEBUG:py4j.clientserver:Answer received: !yro176
DEBUG:py4j.clientserver:Answer received: !yro175
DEBUG:py4j.clientserver:Command to send: c
o175
json
e

DEBUG:py4j.clientserver:Command to send: c
o176
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro177
DEBUG:py4j.clientserver:Command to send: c
o177
format
savro
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !yro178
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o178
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro179
DEBUG:py4j.clientserver:Command to send: c
o179
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro180
DEBUG:py4j.clientserver:Command to send: c
o180
schema
e

DEBUG:py4j.clientserver:Answer received: !yro181
DEBUG:py4j.clientserver:Command to send: c
o181
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro182
DEBUG:py4j.clientserver:Command to send: c
o182
schema
e

DEBUG:py4j.clientserver:Answer received: !yro183
DEBUG:py4j.clientserver:Command to send: c
o183
json
e

DEBUG:py4j.clientserver:Answer received: !yro184
DEBUG:py4j.clientserver:Command to send: c
o184
schema
e

DEBUG:py4j.clientserver:Answer received: !yro185
DEBUG:py4j.clientserver:Command to send: c
o185
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !yro186
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o186
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro187
DEBUG:py4j.clientserver:Answer received: !yro188
DEBUG:py4j.clientserver:Command to send: c
o187
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro189
DEBUG:py4j.clientserver:Command to send: c
o188
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Command to send: c
o189
json
e

DEBUG:py4j.clientserver:Answer received: !yro190
DEBUG:py4j.clientserver:Command to send: c
o190
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro191
DEBUG:py4j.clientserver:Command to send: c
o191
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro192
DEBUG:py4j.clientserver:Command to send: c
o192
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro193
DEBUG:py4j.clientserver:Command to send: c
o193
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro194
DEBUG:py4j.clientserver:Answer received: !yro196
DEBUG:py4j.clientserver:Command to send: c
o196
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o194
schema
e

DEBUG:py4j.clientserver:Answer received: !yro195
DEBUG:py4j.clientserver:Answer received: !yro197
DEBUG:py4j.clientserver:Answer received: !yro198
DEBUG:py4j.clientserver:Command to send: c
o195
schema
e

DEBUG:py4j.clientserver:Command to send: c
o197
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Command to send: c
o198
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro199
DEBUG:py4j.clientserver:Command to send: c
o199
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro200
DEBUG:py4j.clientserver:Answer received: !yro201
DEBUG:py4j.clientserver:Command to send: c
o200
schema
e

DEBUG:py4j.clientserver:Answer received: !yro202
DEBUG:py4j.clientserver:Command to send: c
o201
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Command to send: c
o202
json
e

DEBUG:py4j.clientserver:Answer received: !yro203
DEBUG:py4j.clientserver:Command to send: c
o203
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro204
DEBUG:py4j.clientserver:Command to send: c
o204
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro205
DEBUG:py4j.clientserver:Command to send: c
o205
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro206
DEBUG:py4j.clientserver:Command to send: c
o206
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro207
DEBUG:py4j.clientserver:Command to send: c
o207
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro208
DEBUG:py4j.clientserver:Command to send: c
o208
schema
e

DEBUG:py4j.clientserver:Answer received: !yro209
DEBUG:py4j.clientserver:Command to send: c
o209
json
e

DEBUG:py4j.clientserver:Answer received: !yro210
DEBUG:py4j.clientserver:Command to send: c
o210
schema
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !yro211
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o211
json
e

DEBUG:py4j.clientserver:Answer received: !yro212
DEBUG:py4j.clientserver:Command to send: c
o212
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro213
DEBUG:py4j.clientserver:Command to send: c
o213
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro214
DEBUG:py4j.clientserver:Command to send: c
o214
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro215
DEBUG:py4j.clientserver:Command to send: c
o215
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro216
DEBUG:py4j.clientserver:Command to send: c
o216
schema
e

DEBUG:py4j.clientserver:Answer received: !yro217
DEBUG:py4j.clientserver:Command to send: c
o217
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro218
DEBUG:py4j.clientserver:Command to send: c
o218
schema
e

DEBUG:py4j.clientserver:Answer received: !yro219
DEBUG:py4j.clientserver:Command to send: c
o219
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro220
DEBUG:py4j.clientserver:Command to send: c
o220
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro221
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
o221
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !yro222
DEBUG:py4j.clientserver:Command to send: c
o222
schema
e

DEBUG:py4j.clientserver:Answer received: !yro223
DEBUG:py4j.clientserver:Command to send: c
o223
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro224
DEBUG:py4j.clientserver:Command to send: c
o224
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro225
DEBUG:py4j.clientserver:Command to send: c
o225
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro226
DEBUG:py4j.clientserver:Command to send: c
o226
schema
e

DEBUG:py4j.clientserver:Answer received: !yro227
DEBUG:py4j.clientserver:Command to send: c
o227
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro228
DEBUG:py4j.clientserver:Command to send: c
o228
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro229
DEBUG:py4j.clientserver:Command to send: c
o229
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro230
DEBUG:py4j.clientserver:Command to send: c
o230
schema
e

DEBUG:py4j.clientserver:Answer received: !yro231
DEBUG:py4j.clientserver:Command to send: c
o231
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro232
DEBUG:py4j.clientserver:Command to send: c
o232
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro233
DEBUG:py4j.clientserver:Command to send: c
o233
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro234
DEBUG:py4j.clientserver:Command to send: c
o234
schema
e

DEBUG:py4j.clientserver:Answer received: !yro235
DEBUG:py4j.clientserver:Command to send: c
o235
json
e

DEBUG:py4j.clientserver:Answer received: !yro236
DEBUG:py4j.clientserver:Command to send: c
o236
schema
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !yro237
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o237
json
e

DEBUG:py4j.clientserver:Answer received: !yro238
DEBUG:py4j.clientserver:Command to send: c
o238
format
savro
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro239
DEBUG:py4j.clientserver:Command to send: c
o239
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro240
DEBUG:py4j.clientserver:Command to send: c
o240
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro241
DEBUG:py4j.clientserver:Answer received: !yro242
DEBUG:py4j.clientserver:Command to send: c
o241
schema
e

DEBUG:py4j.clientserver:Answer received: !yro243
DEBUG:py4j.clientserver:Command to send: c
o242
schema
e

DEBUG:py4j.clientserver:Command to send: c
o243
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro244
DEBUG:py4j.clientserver:Answer received: !yro245
DEBUG:py4j.clientserver:Command to send: c
o244
json
e

DEBUG:py4j.clientserver:Command to send: c
o245
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro246
DEBUG:py4j.clientserver:Answer received: !yro247
DEBUG:py4j.clientserver:Command to send: c
o246
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o247
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro248
DEBUG:py4j.clientserver:Answer received: !yro249
DEBUG:py4j.clientserver:Command to send: c
o248
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Command to send: c
o249
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro250
DEBUG:py4j.clientserver:Command to send: c
o250
schema
e

DEBUG:py4j.clientserver:Answer received: !yro251
DEBUG:py4j.clientserver:Command to send: c
o251
schema
e

DEBUG:py4j.clientserver:Answer received: !yro252
DEBUG:py4j.clientserver:Answer received: !yro253
DEBUG:py4j.clientserver:Command to send: c
o252
json
e

DEBUG:py4j.clientserver:Command to send: c
o253
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !yro254
DEBUG:py4j.clientserver:Command to send: c
o254
schema
e

DEBUG:py4j.clientserver:Answer received: !yro255
DEBUG:py4j.clientserver:Command to send: c
o255
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro256
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro257
DEBUG:py4j.clientserver:Answer received: !yro258
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro259
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !yro260
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !yro261
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro262
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro263
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro256
ro260
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !yro264
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro257
ro261
e

DEBUG:py4j.clientserver:Answer received: !yro265
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro258
ro262
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro259
ro263
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro265
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro266
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro264
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro267
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro269
DEBUG:py4j.clientserver:Answer received: !yro268
DEBUG:py4j.clientserver:Command to send: c
o250
withColumn
sstart_date_add_months
ro269
e

DEBUG:py4j.clientserver:Command to send: c
o216
withColumn
sstart_date_add_months
ro268
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro267
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !yro270
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
o254
withColumn
sstart_date_add_months
ro270
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro266
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro271
DEBUG:py4j.clientserver:Command to send: c
o251
withColumn
sstart_date_add_months
ro271
e

DEBUG:py4j.clientserver:Answer received: !yro272
DEBUG:py4j.clientserver:Command to send: c
o272
schema
e

DEBUG:py4j.clientserver:Answer received: !yro273
DEBUG:py4j.clientserver:Command to send: c
o273
schema
e

DEBUG:py4j.clientserver:Answer received: !yro274
DEBUG:py4j.clientserver:Command to send: c
o274
json
e

DEBUG:py4j.clientserver:Answer received: !yro275
DEBUG:py4j.clientserver:Command to send: c
o275
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro276
DEBUG:py4j.clientserver:Command to send: c
o276
schema
e

DEBUG:py4j.clientserver:Answer received: !yro277
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
o277
json
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !yro278
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro279
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro280
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro281
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro282
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro278
ro281
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro279
ro282
e

DEBUG:py4j.clientserver:Answer received: !yro284
DEBUG:py4j.clientserver:Answer received: !yro283
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !yro285
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro284
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro283
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro287
DEBUG:py4j.clientserver:Answer received: !yro286
DEBUG:py4j.clientserver:Command to send: c
o272
withColumn
sstart_date_add_months
ro287
e

DEBUG:py4j.clientserver:Command to send: c
o273
withColumn
sstart_date_add_months
ro286
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro288
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Command to send: c
o288
schema
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro289
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro280
ro285
e

DEBUG:py4j.clientserver:Command to send: c
o289
json
e

DEBUG:py4j.clientserver:Answer received: !yro290
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro291
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
o291
schema
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !yro292
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
o292
json
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro290
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro293
DEBUG:py4j.clientserver:Command to send: c
o276
withColumn
sstart_date_add_months
ro293
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !yro294
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro295
DEBUG:py4j.clientserver:Command to send: c
o295
schema
e

DEBUG:py4j.clientserver:Answer received: !yro296
DEBUG:py4j.clientserver:Command to send: c
o296
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !yro297
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro298
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !yro299
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !yro300
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro294
ro298
e

DEBUG:py4j.clientserver:Answer received: !yro301
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro301
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro297
ro300
e

DEBUG:py4j.clientserver:Answer received: !yro302
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro303
DEBUG:py4j.clientserver:Command to send: c
o288
withColumn
sstart_date_add_months
ro303
e

DEBUG:py4j.clientserver:Answer received: !yro304
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro299
ro302
e

DEBUG:py4j.clientserver:Answer received: !yro305
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
o305
schema
e

DEBUG:py4j.clientserver:Answer received: !yro306
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro304
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro307
DEBUG:py4j.clientserver:Command to send: c
o307
json
e

DEBUG:py4j.clientserver:Answer received: !yro308
DEBUG:py4j.clientserver:Command to send: c
o291
withColumn
sstart_date_add_months
ro308
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro306
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !yro309
DEBUG:py4j.clientserver:Command to send: c
o295
withColumn
sstart_date_add_months
ro309
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !yro310
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro311
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro310
ro311
e

DEBUG:py4j.clientserver:Answer received: !yro312
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro312
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro313
DEBUG:py4j.clientserver:Command to send: c
o305
withColumn
sstart_date_add_months
ro313
e

DEBUG:py4j.clientserver:Answer received: !yro314
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro315
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro316
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro317
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro318
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Command to send: c
o314
withColumn
scurrent_timestamp
ro318
e

DEBUG:py4j.clientserver:Answer received: !yro319
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro320
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Command to send: c
o315
withColumn
scurrent_timestamp
ro320
e

DEBUG:py4j.clientserver:Answer received: !yro321
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro322
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro324
DEBUG:py4j.clientserver:Answer received: !yro323
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Command to send: c
o317
withColumn
scurrent_timestamp
ro324
e

DEBUG:py4j.clientserver:Command to send: c
o316
withColumn
scurrent_timestamp
ro323
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro325
DEBUG:py4j.clientserver:Command to send: c
o319
withColumn
scurrent_timestamp
ro325
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro326
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro327
DEBUG:py4j.clientserver:Command to send: c
o322
withColumn
scurrent_timestamp
ro327
e

DEBUG:py4j.clientserver:Answer received: !yro328
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro329
DEBUG:py4j.clientserver:Command to send: c
o329
write
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro331
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro330
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Command to send: c
o330
write
e

DEBUG:py4j.clientserver:Answer received: !yro333
DEBUG:py4j.clientserver:Answer received: !yro332
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
o333
write
e

DEBUG:py4j.clientserver:Command to send: c
o332
write
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro335
DEBUG:py4j.clientserver:Command to send: c
o321
withColumn
scurrent_timestamp
ro335
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro337
DEBUG:py4j.clientserver:Answer received: !yro336
DEBUG:py4j.clientserver:Command to send: c
o336
mode
soverwrite
e

DEBUG:py4j.clientserver:Command to send: c
o326
withColumn
scurrent_timestamp
ro337
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro334
DEBUG:py4j.clientserver:Answer received: !yro338
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
o338
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro339
DEBUG:py4j.clientserver:Command to send: c
o334
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro340
DEBUG:py4j.clientserver:Command to send: c
o331
withColumn
scurrent_timestamp
ro340
e

DEBUG:py4j.clientserver:Command to send: c
o339
mode
soverwrite
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro341
DEBUG:py4j.clientserver:Command to send: c
o328
withColumn
scurrent_timestamp
ro341
e

DEBUG:py4j.clientserver:Answer received: !yro343
DEBUG:py4j.clientserver:Command to send: c
o343
write
e

DEBUG:py4j.clientserver:Answer received: !yro342
DEBUG:py4j.clientserver:Command to send: c
o342
write
e

DEBUG:py4j.clientserver:Answer received: !yro344
DEBUG:py4j.clientserver:Command to send: c
o344
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro349
DEBUG:py4j.clientserver:Answer received: !yro345
DEBUG:py4j.clientserver:Command to send: c
o345
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro351
DEBUG:py4j.clientserver:Answer received: !yro346
DEBUG:py4j.clientserver:Command to send: c
o346
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro348
DEBUG:py4j.clientserver:Answer received: !yro350
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro352
DEBUG:py4j.clientserver:Command to send: c
o351
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro353
DEBUG:py4j.clientserver:Command to send: c
o348
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro347
DEBUG:py4j.clientserver:Answer received: !yro354
DEBUG:py4j.clientserver:Command to send: c
o350
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro
e

DEBUG:py4j.clientserver:Answer received: !yro355
DEBUG:py4j.clientserver:Command to send: c
o355
write
e

DEBUG:py4j.clientserver:Command to send: c
o352
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro356
DEBUG:py4j.clientserver:Command to send: c
o347
write
e

DEBUG:py4j.clientserver:Answer received: !yro357
DEBUG:py4j.clientserver:Command to send: c
o357
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro
e

DEBUG:py4j.clientserver:Command to send: c
o354
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro360
DEBUG:py4j.clientserver:Command to send: c
o360
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro358
DEBUG:py4j.clientserver:Command to send: c
o358
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro359
DEBUG:py4j.clientserver:Command to send: c
o359
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro362
DEBUG:py4j.clientserver:Command to send: c
o362
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o353
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro363
DEBUG:py4j.clientserver:Answer received: !yro364
DEBUG:py4j.clientserver:Answer received: !yro361
DEBUG:py4j.clientserver:Answer received: !yro365
DEBUG:py4j.clientserver:Command to send: c
o365
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro
e

DEBUG:py4j.clientserver:Command to send: c
o363
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o356
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o364
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro
e

DEBUG:py4j.clientserver:Answer received: !yro367
DEBUG:py4j.clientserver:Command to send: c
o367
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro
e

DEBUG:py4j.clientserver:Answer received: !yro366
DEBUG:py4j.clientserver:Command to send: c
o349
withColumn
scurrent_timestamp
ro366
e

DEBUG:py4j.clientserver:Command to send: c
o361
write
e

DEBUG:py4j.clientserver:Answer received: !yro368
DEBUG:py4j.clientserver:Command to send: c
o368
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro
e

DEBUG:py4j.clientserver:Answer received: !yro369
DEBUG:py4j.clientserver:Command to send: c
o369
mode
soverwrite
e

DEBUG:py4j.clientserver:Command to send: m
d
o33
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o35
e

DEBUG:py4j.clientserver:Answer received: !yro370
DEBUG:py4j.clientserver:Command to send: c
o370
write
e

DEBUG:py4j.clientserver:Answer received: !yro371
DEBUG:py4j.clientserver:Command to send: c
o371
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o39
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o43
e

DEBUG:py4j.clientserver:Answer received: !yro372
DEBUG:py4j.clientserver:Command to send: c
o372
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro
e

DEBUG:py4j.clientserver:Answer received: !yro373
DEBUG:py4j.clientserver:Command to send: c
o373
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro374
DEBUG:py4j.clientserver:Command to send: c
o374
write
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o47
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o51
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o55
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o59
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o60
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o102
e

DEBUG:py4j.clientserver:Answer received: !yro375
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o105
e

DEBUG:py4j.clientserver:Command to send: c
o375
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o109
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o115
e

DEBUG:py4j.clientserver:Answer received: !yro376
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o376
format
savro
e

DEBUG:py4j.clientserver:Command to send: m
d
o112
e

DEBUG:py4j.clientserver:Answer received: !yro377
DEBUG:py4j.clientserver:Command to send: c
o377
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro378
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o378
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro
e

DEBUG:py4j.clientserver:Answer received: !yro379
DEBUG:py4j.clientserver:Command to send: m
d
o119
e

DEBUG:py4j.clientserver:Command to send: c
o379
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o123
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o131
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o135
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o73
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o76
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o75
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o79
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o78
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o116
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o120
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o124
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o125
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o127
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o130
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o134
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o136
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o137
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o138
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o139
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o140
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o141
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o142
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o143
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o144
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o145
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o146
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o147
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o148
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o149
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o150
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o151
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o152
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o153
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o154
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o155
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o156
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o157
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o158
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o159
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o160
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o161
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o162
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o163
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o165
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o166
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o168
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o169
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o171
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o172
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o174
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o176
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o175
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o177
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o178
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o179
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o180
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o181
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o182
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o183
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o184
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o185
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o186
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o187
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o188
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o189
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o190
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o191
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o192
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o193
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o194
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o196
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o195
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o197
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o198
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o199
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o200
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o201
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o202
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o203
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o204
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o205
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o206
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o207
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o209
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o211
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o212
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o214
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o217
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o219
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o220
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o223
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o224
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o227
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o228
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o231
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o232
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o235
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o237
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o238
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o241
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o242
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o243
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o244
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o245
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o246
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o247
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o248
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o249
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o250
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o251
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o252
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o253
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o254
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o255
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o256
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o257
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o258
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o259
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o260
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o261
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o262
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o263
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o264
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o265
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o266
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o267
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o269
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o268
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o270
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o271
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o272
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o273
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o274
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o275
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o276
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o277
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o278
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o279
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o280
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o281
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o282
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o284
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o283
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o285
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o287
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o286
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o288
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o289
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o290
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o291
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o292
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o293
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o294
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o295
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o296
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o297
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o298
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o299
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o300
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o301
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o302
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o303
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o304
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o305
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o306
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o307
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o308
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o309
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o310
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o311
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o312
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o313
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o314
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o315
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o316
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o317
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o319
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o321
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o322
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o328
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o331
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o336
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o334
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o338
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o339
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o344
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o345
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o351
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o346
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o348
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o352
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o356
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o360
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o358
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o359
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o362
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o363
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !xro380
DEBUG:py4j.clientserver:Answer received: !xro381
DEBUG:py4j.clientserver:Answer received: !xro382
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !xro383
DEBUG:py4j.clientserver:Answer received: !xro384
DEBUG:py4j.clientserver:Answer received: !xro385
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !xro387
DEBUG:py4j.clientserver:Answer received: !xro386
DEBUG:py4j.clientserver:Answer received: !xro388
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !xro389
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !xro390
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro381
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro389
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro380
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro382
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro386
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro390
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro383
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro385
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro387
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro384
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro388
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro382
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro381
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro389
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro380
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro383
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro386
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro385
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro384
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro388
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro387
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro390
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro381
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro382
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro389
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro383
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro384
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro380
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro386
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro385
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro387
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro390
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro389
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro388
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro381
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro386
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro382
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro385
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro384
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro383
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro380
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro389
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro387
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro390
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro388
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro382
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro381
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro386
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro390
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro380
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro383
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro388
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro385
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro389
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro381
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro384
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro390
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro382
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro383
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro380
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro388
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro386
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro385
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro387
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro384
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro381
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro389
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro382
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro390
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro388
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro384
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro381
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro387
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro383
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro382
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro380
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro385
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro389
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro386
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro390
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro388
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro384
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro381
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro382
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro383
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro387
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro389
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro380
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro385
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro390
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro386
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro384
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro388
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro381
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro382
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro383
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro384
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro385
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro386
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro380
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro389
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro390
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro381
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro388
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro387
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
o381
getCause
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro383
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro385
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro382
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro390
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
o382
getCause
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o390
getCause
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro384
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro380
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro389
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
o384
getCause
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
o389
getCause
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro386
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro388
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro387
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro385
e

DEBUG:py4j.clientserver:Command to send: c
o388
getCause
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro383
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro380
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro381
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
o385
getCause
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o380
getCause
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_2023091310402136742492246785423_0009_m_000000/part-00000-49425078-83dd-4255-811b-fe24fb153dee-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o381
toString
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Command to send: c
o383
getCause
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_2023091310402136742492246785423_0009_m_000000/part-00000-49425078-83dd-4255-811b-fe24fb153dee-c000.avro does not exist
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: p
ro381
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro390
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro382
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040212314414462231009951_0006_m_000000/.part-00000-bda9587c-eebb-45d3-b562-a9ab15d83e4f-c000.avro.crc does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:707)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_2023091310402136742492246785423_0009_m_000000/part-00000-49425078-83dd-4255-811b-fe24fb153dee-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro389
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
o390
toString
e

ERROR:root:Error processing old_examdata.avro: An error occurred while calling o357.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_2023091310402136742492246785423_0009_m_000000/part-00000-49425078-83dd-4255-811b-fe24fb153dee-c000.avro does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)
	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

DEBUG:py4j.clientserver:Answer received: !ysjava.nio.file.AccessDeniedException: C:\\Users\\lyekollu\\PycharmProject_Updated\\Accelerator\\Media\\destination_folder\\new__examdata_2023-09-13-10-40-17.avro\\_temporary\\0\\task_202309131040211889797379848263539_0003_m_000000: Permission denied\r\n	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1455)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o382
toString
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !ysjava.nio.file.AccessDeniedException: C:\\Users\\lyekollu\\PycharmProject_Updated\\Accelerator\\Media\\destination_folder\\new__examdata_2023-09-13-10-40-17.avro\\_temporary\\0\\task_202309131040211889797379848263539_0003_m_000000: Permission denied
DEBUG:py4j.clientserver:Command to send: p
ro382
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040213840147432455209652_0000_m_000000 does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Command to send: c
o389
toString
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ysjava.nio.file.AccessDeniedException: C:\\Users\\lyekollu\\PycharmProject_Updated\\Accelerator\\Media\\destination_folder\\new__examdata_2023-09-13-10-40-17.avro\\_temporary\\0\\task_202309131040211889797379848263539_0003_m_000000: Permission denied\r\n	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1455)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
ERROR:root:Error processing old_examdata.avro: An error occurred while calling o364.save.
: java.nio.file.AccessDeniedException: C:\Users\lyekollu\PycharmProject_Updated\Accelerator\Media\destination_folder\new__examdata_2023-09-13-10-40-17.avro\_temporary\0\task_202309131040211889797379848263539_0003_m_000000: Permission denied
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1455)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro384
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040212314414462231009951_0006_m_000000/.part-00000-bda9587c-eebb-45d3-b562-a9ab15d83e4f-c000.avro.crc does not exist
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040213840147432455209652_0000_m_000000 does not exist
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro387
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Command to send: p
ro390
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro386
e

DEBUG:py4j.clientserver:Answer received: !ysjava.nio.file.AccessDeniedException: C:\\Users\\lyekollu\\PycharmProject_Updated\\Accelerator\\Media\\destination_folder\\new__examdata_2023-09-13-10-40-17.avro\\_temporary\\0\\task_202309131040212314414462231009951_0006_m_000000: Permission denied\r\n	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1455)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: p
ro389
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Command to send: c
o384
toString
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040213840147432455209652_0000_m_000000 does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

ERROR:root:Error processing old_examdata.avro: An error occurred while calling o367.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040213840147432455209652_0000_m_000000 does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040212314414462231009951_0006_m_000000/.part-00000-bda9587c-eebb-45d3-b562-a9ab15d83e4f-c000.avro.crc does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:707)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o386
getCause
e

DEBUG:py4j.clientserver:Answer received: !yp
INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro380
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

ERROR:root:Error processing old_examdata.avro: An error occurred while calling o378.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040212314414462231009951_0006_m_000000/.part-00000-bda9587c-eebb-45d3-b562-a9ab15d83e4f-c000.avro.crc does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:707)
	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Answer received: !ysjava.nio.file.AccessDeniedException: C:\\Users\\lyekollu\\PycharmProject_Updated\\Accelerator\\Media\\destination_folder\\new__examdata_2023-09-13-10-40-17.avro\\_temporary\\0\\task_202309131040212314414462231009951_0006_m_000000: Permission denied
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Command to send: p
ro384
e

DEBUG:py4j.clientserver:Answer received: !ysjava.nio.file.AccessDeniedException: C:\\Users\\lyekollu\\PycharmProject_Updated\\Accelerator\\Media\\destination_folder\\new__examdata_2023-09-13-10-40-17.avro\\_temporary\\0\\task_202309131040212314414462231009951_0006_m_000000: Permission denied\r\n	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1455)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
ERROR:root:Error processing old_examdata.avro: An error occurred while calling o354.save.
: java.nio.file.AccessDeniedException: C:\Users\lyekollu\PycharmProject_Updated\Accelerator\Media\destination_folder\new__examdata_2023-09-13-10-40-17.avro\_temporary\0\task_202309131040212314414462231009951_0006_m_000000: Permission denied
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1455)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040211889797379848263539_0003_m_000000/part-00000-0de202d5-c0d4-41dd-8e0c-a91bebf39348-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro388
e

INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro383
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro385
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040217826125948758206483_0001_m_000000/part-00000-bfa9f9a7-6510-4c16-a5ad-463842d44940-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o380
toString
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040211889797379848263539_0003_m_000000/part-00000-0de202d5-c0d4-41dd-8e0c-a91bebf39348-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o383
toString
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Command to send: c
o388
toString
e

DEBUG:py4j.clientserver:Answer received: !ysjava.nio.file.AccessDeniedException: C:\\Users\\lyekollu\\PycharmProject_Updated\\Accelerator\\Media\\destination_folder\\new__examdata_2023-09-13-10-40-17.avro\\_temporary\\0\\task_2023091310402136742492246785423_0009_m_000000\\.part-00000-49425078-83dd-4255-811b-fe24fb153dee-c000.avro.crc\r\n	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)\r\n	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)\r\n	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:108)\r\n	at java.base/sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(WindowsFileAttributeViews.java:53)\r\n	at java.base/sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(WindowsFileAttributeViews.java:38)\r\n	at java.base/sun.nio.fs.WindowsFileSystemProvider.readAttributes(WindowsFileSystemProvider.java:194)\r\n	at java.base/java.nio.file.Files.readAttributes(Files.java:1763)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getLastAccessTime(RawLocalFileSystem.java:796)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.<init>(RawLocalFileSystem.java:807)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:776)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:608)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o385
toString
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040217826125948758206483_0001_m_000000/part-00000-bfa9f9a7-6510-4c16-a5ad-463842d44940-c000.avro does not exist
DEBUG:py4j.clientserver:Command to send: p
ro383
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040211889797379848263539_0003_m_000000/part-00000-0de202d5-c0d4-41dd-8e0c-a91bebf39348-c000.avro does not exist
DEBUG:py4j.clientserver:Command to send: p
ro388
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040211889797379848263539_0003_m_000000/part-00000-0de202d5-c0d4-41dd-8e0c-a91bebf39348-c000.avro does not exist
DEBUG:py4j.clientserver:Command to send: p
ro380
e

DEBUG:py4j.clientserver:Answer received: !ysjava.nio.file.AccessDeniedException: C:\\Users\\lyekollu\\PycharmProject_Updated\\Accelerator\\Media\\destination_folder\\new__examdata_2023-09-13-10-40-17.avro\\_temporary\\0\\task_2023091310402136742492246785423_0009_m_000000\\.part-00000-49425078-83dd-4255-811b-fe24fb153dee-c000.avro.crc
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040211889797379848263539_0003_m_000000/part-00000-0de202d5-c0d4-41dd-8e0c-a91bebf39348-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
ERROR:root:Error processing old_examdata.avro: An error occurred while calling o365.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040211889797379848263539_0003_m_000000/part-00000-0de202d5-c0d4-41dd-8e0c-a91bebf39348-c000.avro does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)
	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040211889797379848263539_0003_m_000000/part-00000-0de202d5-c0d4-41dd-8e0c-a91bebf39348-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: p
ro385
e

INFO:py4j.clientserver:Closing down clientserver connection
ERROR:root:Error processing old_examdata.avro: An error occurred while calling o353.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040211889797379848263539_0003_m_000000/part-00000-0de202d5-c0d4-41dd-8e0c-a91bebf39348-c000.avro does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)
	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040217826125948758206483_0001_m_000000/part-00000-bfa9f9a7-6510-4c16-a5ad-463842d44940-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
ERROR:root:Error processing old_examdata.avro: An error occurred while calling o379.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040217826125948758206483_0001_m_000000/part-00000-bfa9f9a7-6510-4c16-a5ad-463842d44940-c000.avro does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)
	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

INFO:py4j.clientserver:Closing down clientserver connection
INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Answer received: !ysjava.nio.file.AccessDeniedException: C:\\Users\\lyekollu\\PycharmProject_Updated\\Accelerator\\Media\\destination_folder\\new__examdata_2023-09-13-10-40-17.avro\\_temporary\\0\\task_2023091310402136742492246785423_0009_m_000000\\.part-00000-49425078-83dd-4255-811b-fe24fb153dee-c000.avro.crc\r\n	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)\r\n	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)\r\n	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:108)\r\n	at java.base/sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(WindowsFileAttributeViews.java:53)\r\n	at java.base/sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(WindowsFileAttributeViews.java:38)\r\n	at java.base/sun.nio.fs.WindowsFileSystemProvider.readAttributes(WindowsFileSystemProvider.java:194)\r\n	at java.base/java.nio.file.Files.readAttributes(Files.java:1763)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getLastAccessTime(RawLocalFileSystem.java:796)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.<init>(RawLocalFileSystem.java:807)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:776)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:608)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
ERROR:root:Error processing old_examdata.avro: An error occurred while calling o372.save.
: java.nio.file.AccessDeniedException: C:\Users\lyekollu\PycharmProject_Updated\Accelerator\Media\destination_folder\new__examdata_2023-09-13-10-40-17.avro\_temporary\0\task_2023091310402136742492246785423_0009_m_000000\.part-00000-49425078-83dd-4255-811b-fe24fb153dee-c000.avro.crc
	at java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
	at java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:108)
	at java.base/sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(WindowsFileAttributeViews.java:53)
	at java.base/sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(WindowsFileAttributeViews.java:38)
	at java.base/sun.nio.fs.WindowsFileSystemProvider.readAttributes(WindowsFileSystemProvider.java:194)
	at java.base/java.nio.file.Files.readAttributes(Files.java:1763)
	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getLastAccessTime(RawLocalFileSystem.java:796)
	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.<init>(RawLocalFileSystem.java:807)
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:776)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:608)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro387
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
o387
getCause
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro386
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040211889797379848263539_0003_m_000000/part-00000-0de202d5-c0d4-41dd-8e0c-a91bebf39348-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o386
toString
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040211889797379848263539_0003_m_000000/part-00000-0de202d5-c0d4-41dd-8e0c-a91bebf39348-c000.avro does not exist
DEBUG:py4j.clientserver:Command to send: p
ro386
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040211889797379848263539_0003_m_000000/part-00000-0de202d5-c0d4-41dd-8e0c-a91bebf39348-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
ERROR:root:Error processing old_examdata.avro: An error occurred while calling o350.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040211889797379848263539_0003_m_000000/part-00000-0de202d5-c0d4-41dd-8e0c-a91bebf39348-c000.avro does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)
	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro387
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040217826125948758206483_0001_m_000000/part-00000-bfa9f9a7-6510-4c16-a5ad-463842d44940-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o387
toString
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040217826125948758206483_0001_m_000000/part-00000-bfa9f9a7-6510-4c16-a5ad-463842d44940-c000.avro does not exist
DEBUG:py4j.clientserver:Command to send: p
ro387
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040217826125948758206483_0001_m_000000/part-00000-bfa9f9a7-6510-4c16-a5ad-463842d44940-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
ERROR:root:Error processing old_examdata.avro: An error occurred while calling o368.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-40-17.avro/_temporary/0/task_202309131040217826125948758206483_0001_m_000000/part-00000-bfa9f9a7-6510-4c16-a5ad-463842d44940-c000.avro does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)
	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

INFO:py4j.clientserver:Closing down clientserver connection
INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:asyncio:Using proactor: IocpProactor
DEBUG:py4j.java_gateway:GatewayClient.address is deprecated and will be removed in version 1.0. Use GatewayParameters instead.
DEBUG:py4j.clientserver:Command to send: A
fed07bf5672ee84da3108ec5d547c857076a50c99a43c6d248f9bc5e91766074

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.SparkConf
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.api.java.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.api.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.ml.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.mllib.api.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.resource.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.sql.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.sql.api.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.sql.hive.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
scala.Tuple2
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkConf
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkConf
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.SparkConf
bTrue
e

DEBUG:py4j.clientserver:Answer received: !yro0
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yro1
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yro2
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.serializer.objectStreamReset
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.serializer.objectStreamReset
s100
e

DEBUG:py4j.clientserver:Answer received: !yro3
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.rdd.compress
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.rdd.compress
sTrue
e

DEBUG:py4j.clientserver:Answer received: !yro4
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.master
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.app.name
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.master
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
get
sspark.master
e

DEBUG:py4j.clientserver:Answer received: !yslocal[*]
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.app.name
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
get
sspark.app.name
e

DEBUG:py4j.clientserver:Answer received: !ysAvroProcessing
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.home
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o0
getAll
e

DEBUG:py4j.clientserver:Answer received: !yto5
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i0
e

DEBUG:py4j.clientserver:Answer received: !yro6
DEBUG:py4j.clientserver:Command to send: c
o6
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.rdd.compress
DEBUG:py4j.clientserver:Command to send: c
o6
_2
e

DEBUG:py4j.clientserver:Answer received: !ysTrue
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i1
e

DEBUG:py4j.clientserver:Answer received: !yro7
DEBUG:py4j.clientserver:Command to send: c
o7
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.app.name
DEBUG:py4j.clientserver:Command to send: c
o7
_2
e

DEBUG:py4j.clientserver:Answer received: !ysAvroProcessing
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i2
e

DEBUG:py4j.clientserver:Answer received: !yro8
DEBUG:py4j.clientserver:Command to send: c
o8
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.serializer.objectStreamReset
DEBUG:py4j.clientserver:Command to send: c
o8
_2
e

DEBUG:py4j.clientserver:Answer received: !ys100
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i3
e

DEBUG:py4j.clientserver:Answer received: !yro9
DEBUG:py4j.clientserver:Command to send: c
o9
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.submit.pyFiles
DEBUG:py4j.clientserver:Command to send: c
o9
_2
e

DEBUG:py4j.clientserver:Answer received: !ysC:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i4
e

DEBUG:py4j.clientserver:Answer received: !yro10
DEBUG:py4j.clientserver:Command to send: c
o10
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.master
DEBUG:py4j.clientserver:Command to send: c
o10
_2
e

DEBUG:py4j.clientserver:Answer received: !yslocal[*]
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i5
e

DEBUG:py4j.clientserver:Answer received: !yro11
DEBUG:py4j.clientserver:Command to send: c
o11
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.jars.packages
DEBUG:py4j.clientserver:Command to send: c
o11
_2
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark:spark-avro_2.12:3.4.1
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i6
e

DEBUG:py4j.clientserver:Answer received: !yro12
DEBUG:py4j.clientserver:Command to send: c
o12
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.submit.deployMode
DEBUG:py4j.clientserver:Command to send: c
o12
_2
e

DEBUG:py4j.clientserver:Answer received: !ysclient
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i7
e

DEBUG:py4j.clientserver:Answer received: !yro13
DEBUG:py4j.clientserver:Command to send: c
o13
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.files
DEBUG:py4j.clientserver:Command to send: c
o13
_2
e

DEBUG:py4j.clientserver:Answer received: !ysfile:///C:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,file:///C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i8
e

DEBUG:py4j.clientserver:Answer received: !yro14
DEBUG:py4j.clientserver:Command to send: c
o14
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.repl.local.jars
DEBUG:py4j.clientserver:Command to send: c
o14
_2
e

DEBUG:py4j.clientserver:Answer received: !ysfile:///C:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,file:///C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i9
e

DEBUG:py4j.clientserver:Answer received: !yro15
DEBUG:py4j.clientserver:Command to send: c
o15
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.jars
DEBUG:py4j.clientserver:Command to send: c
o15
_2
e

DEBUG:py4j.clientserver:Answer received: !ysfile:///C:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,file:///C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i10
e

DEBUG:py4j.clientserver:Answer received: !yro16
DEBUG:py4j.clientserver:Command to send: c
o16
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.ui.showConsoleProgress
DEBUG:py4j.clientserver:Command to send: c
o16
_2
e

DEBUG:py4j.clientserver:Answer received: !ystrue
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i11
e

DEBUG:py4j.clientserver:Answer received: !yro17
DEBUG:py4j.clientserver:Command to send: c
o17
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.app.submitTime
DEBUG:py4j.clientserver:Command to send: c
o17
_2
e

DEBUG:py4j.clientserver:Answer received: !ys1694582123022
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: r
u
JavaSparkContext
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.java.JavaSparkContext
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.api.java.JavaSparkContext
ro0
e

DEBUG:py4j.clientserver:Command to send: A
fed07bf5672ee84da3108ec5d547c857076a50c99a43c6d248f9bc5e91766074

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o1
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o2
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o3
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o4
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o5
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro18
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro19
DEBUG:py4j.clientserver:Command to send: c
o19
conf
e

DEBUG:py4j.clientserver:Answer received: !yro20
DEBUG:py4j.clientserver:Command to send: r
u
PythonAccumulatorV2
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonAccumulatorV2
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.api.python.PythonAccumulatorV2
s127.0.0.1
i57037
sfed07bf5672ee84da3108ec5d547c857076a50c99a43c6d248f9bc5e91766074
e

DEBUG:py4j.clientserver:Answer received: !yro21
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro22
DEBUG:py4j.clientserver:Command to send: c
o22
register
ro21
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
PythonUtils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.api.python.PythonUtils
isEncryptionEnabled
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.api.python.PythonUtils
isEncryptionEnabled
ro18
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
PythonUtils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.api.python.PythonUtils
getPythonAuthSocketTimeout
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.api.python.PythonUtils
getPythonAuthSocketTimeout
ro18
e

DEBUG:py4j.clientserver:Answer received: !yL15
DEBUG:py4j.clientserver:Command to send: r
u
PythonUtils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.api.python.PythonUtils
getSparkBufferSize
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.api.python.PythonUtils
getSparkBufferSize
ro18
e

DEBUG:py4j.clientserver:Answer received: !yi65536
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.SparkFiles
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-08fe722e-535c-4e44-b9ba-e32f150c6b81\\userFiles-8fe8bfab-7b03-42b4-91cb-60e3539b4047
DEBUG:py4j.clientserver:Command to send: c
o20
get
sspark.submit.pyFiles
s
e

DEBUG:py4j.clientserver:Answer received: !ysC:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.SparkFiles
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-08fe722e-535c-4e44-b9ba-e32f150c6b81\\userFiles-8fe8bfab-7b03-42b4-91cb-60e3539b4047
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.SparkFiles
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-08fe722e-535c-4e44-b9ba-e32f150c6b81\\userFiles-8fe8bfab-7b03-42b4-91cb-60e3539b4047
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
getLocalDir
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro23
DEBUG:py4j.clientserver:Command to send: c
o23
conf
e

DEBUG:py4j.clientserver:Answer received: !yro24
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
getLocalDir
ro24
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-08fe722e-535c-4e44-b9ba-e32f150c6b81
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
createTempDir
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
createTempDir
sC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-08fe722e-535c-4e44-b9ba-e32f150c6b81
spyspark
e

DEBUG:py4j.clientserver:Answer received: !yro25
DEBUG:py4j.clientserver:Command to send: c
o25
getAbsolutePath
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-08fe722e-535c-4e44-b9ba-e32f150c6b81\\pyspark-0e7de969-35ec-4d7d-aeb2-9a2c7129240a
DEBUG:py4j.clientserver:Command to send: c
o20
get
sspark.python.profile
sfalse
e

DEBUG:py4j.clientserver:Answer received: !ysfalse
DEBUG:py4j.clientserver:Command to send: c
o20
get
sspark.python.profile.memory
sfalse
e

DEBUG:py4j.clientserver:Answer received: !ysfalse
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
getDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
getDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !yro26
DEBUG:py4j.clientserver:Command to send: c
o26
isDefined
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro27
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao28
DEBUG:py4j.clientserver:Command to send: c
o28
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o28
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.sql.SparkSession
ro27
ro28
e

DEBUG:py4j.clientserver:Answer received: !yro29
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
setDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
setDefaultSession
ro29
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
setActiveSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
setActiveSession
ro29
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
fed07bf5672ee84da3108ec5d547c857076a50c99a43c6d248f9bc5e91766074

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro30
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao31
DEBUG:py4j.clientserver:Command to send: c
o31
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o31
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o30
applyModifiableSettings
ro29
ro31
e

DEBUG:py4j.clientserver:Command to send: m
d
o28
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro32
DEBUG:py4j.clientserver:Command to send: c
o32
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro33
DEBUG:py4j.clientserver:Command to send: c
o33
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
fed07bf5672ee84da3108ec5d547c857076a50c99a43c6d248f9bc5e91766074

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro34
DEBUG:py4j.clientserver:Command to send: c
o34
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro35
DEBUG:py4j.clientserver:Command to send: c
o35
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro36
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao37
DEBUG:py4j.clientserver:Command to send: c
o37
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o37
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o36
applyModifiableSettings
ro29
ro37
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
fed07bf5672ee84da3108ec5d547c857076a50c99a43c6d248f9bc5e91766074

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro38
DEBUG:py4j.clientserver:Command to send: c
o38
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro39
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Command to send: c
o39
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro40
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao41
DEBUG:py4j.clientserver:Command to send: c
o41
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o41
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o40
applyModifiableSettings
ro29
ro41
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
fed07bf5672ee84da3108ec5d547c857076a50c99a43c6d248f9bc5e91766074

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro42
DEBUG:py4j.clientserver:Command to send: c
o42
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro43
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Command to send: c
o43
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro44
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao45
DEBUG:py4j.clientserver:Command to send: c
o45
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o45
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o44
applyModifiableSettings
ro29
ro45
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
fed07bf5672ee84da3108ec5d547c857076a50c99a43c6d248f9bc5e91766074

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro46
DEBUG:py4j.clientserver:Command to send: c
o46
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro47
DEBUG:py4j.clientserver:Command to send: c
o47
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro48
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao49
DEBUG:py4j.clientserver:Command to send: c
o49
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o49
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o48
applyModifiableSettings
ro29
ro49
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
fed07bf5672ee84da3108ec5d547c857076a50c99a43c6d248f9bc5e91766074

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro50
DEBUG:py4j.clientserver:Command to send: c
o50
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro51
DEBUG:py4j.clientserver:Command to send: c
o51
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro52
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao53
DEBUG:py4j.clientserver:Command to send: c
o53
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o53
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o52
applyModifiableSettings
ro29
ro53
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: A
fed07bf5672ee84da3108ec5d547c857076a50c99a43c6d248f9bc5e91766074

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro54
DEBUG:py4j.clientserver:Command to send: c
o54
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro55
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Command to send: c
o55
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro56
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao57
DEBUG:py4j.clientserver:Command to send: c
o57
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o57
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o56
applyModifiableSettings
ro29
ro57
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
fed07bf5672ee84da3108ec5d547c857076a50c99a43c6d248f9bc5e91766074

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro58
DEBUG:py4j.clientserver:Command to send: c
o58
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro59
DEBUG:py4j.clientserver:Command to send: c
o59
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro60
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao61
DEBUG:py4j.clientserver:Command to send: c
o61
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o61
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o60
applyModifiableSettings
ro29
ro61
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
fed07bf5672ee84da3108ec5d547c857076a50c99a43c6d248f9bc5e91766074

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro62
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o62
format
savro
e

DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !yro63
DEBUG:py4j.clientserver:Command to send: c
o63
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro64
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao65
DEBUG:py4j.clientserver:Command to send: c
o65
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o65
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o64
applyModifiableSettings
ro29
ro65
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: A
fed07bf5672ee84da3108ec5d547c857076a50c99a43c6d248f9bc5e91766074

DEBUG:py4j.clientserver:Answer received: !yro66
DEBUG:py4j.clientserver:Command to send: c
o66
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro67
DEBUG:py4j.clientserver:Command to send: c
o67
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro68
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao69
DEBUG:py4j.clientserver:Command to send: c
o69
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o69
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o68
applyModifiableSettings
ro29
ro69
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: m
d
o31
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o37
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro70
DEBUG:py4j.clientserver:Command to send: c
o70
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro71
DEBUG:py4j.clientserver:Command to send: c
o71
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o41
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o45
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o49
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o53
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o57
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o0
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o6
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o7
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o8
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o9
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o10
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o11
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o12
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o13
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o14
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o15
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o16
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o17
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o19
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o22
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o23
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o24
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o25
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o26
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o27
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o30
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o32
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o34
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o36
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o38
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o40
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o42
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o44
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o46
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o48
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o50
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o52
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o54
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o56
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o58
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o61
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o65
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o69
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro72
DEBUG:py4j.clientserver:Answer received: !yro73
DEBUG:py4j.clientserver:Command to send: c
o72
schema
e

DEBUG:py4j.clientserver:Answer received: !yro75
DEBUG:py4j.clientserver:Answer received: !yro77
DEBUG:py4j.clientserver:Answer received: !yro76
DEBUG:py4j.clientserver:Command to send: c
o73
schema
e

DEBUG:py4j.clientserver:Answer received: !yro74
DEBUG:py4j.clientserver:Answer received: !yro78
DEBUG:py4j.clientserver:Answer received: !yro79
DEBUG:py4j.clientserver:Command to send: c
o75
schema
e

DEBUG:py4j.clientserver:Answer received: !yro80
DEBUG:py4j.clientserver:Command to send: c
o77
schema
e

DEBUG:py4j.clientserver:Command to send: c
o76
schema
e

DEBUG:py4j.clientserver:Command to send: c
o74
schema
e

DEBUG:py4j.clientserver:Command to send: c
o78
schema
e

DEBUG:py4j.clientserver:Command to send: c
o79
schema
e

DEBUG:py4j.clientserver:Command to send: c
o80
schema
e

DEBUG:py4j.clientserver:Answer received: !yro81
DEBUG:py4j.clientserver:Command to send: c
o81
schema
e

DEBUG:py4j.clientserver:Answer received: !yro82
DEBUG:py4j.clientserver:Command to send: c
o82
schema
e

DEBUG:py4j.clientserver:Answer received: !yro83
DEBUG:py4j.clientserver:Answer received: !yro88
DEBUG:py4j.clientserver:Answer received: !yro87
DEBUG:py4j.clientserver:Answer received: !yro84
DEBUG:py4j.clientserver:Answer received: !yro85
DEBUG:py4j.clientserver:Answer received: !yro86
DEBUG:py4j.clientserver:Answer received: !yro89
DEBUG:py4j.clientserver:Answer received: !yro90
DEBUG:py4j.clientserver:Answer received: !yro91
DEBUG:py4j.clientserver:Command to send: c
o83
json
e

DEBUG:py4j.clientserver:Answer received: !yro92
DEBUG:py4j.clientserver:Answer received: !yro93
DEBUG:py4j.clientserver:Command to send: c
o88
json
e

DEBUG:py4j.clientserver:Command to send: c
o87
json
e

DEBUG:py4j.clientserver:Command to send: c
o84
json
e

DEBUG:py4j.clientserver:Command to send: c
o85
json
e

DEBUG:py4j.clientserver:Command to send: c
o86
json
e

DEBUG:py4j.clientserver:Command to send: c
o89
json
e

DEBUG:py4j.clientserver:Command to send: c
o90
json
e

DEBUG:py4j.clientserver:Command to send: c
o91
json
e

DEBUG:py4j.clientserver:Command to send: c
o92
json
e

DEBUG:py4j.clientserver:Command to send: c
o93
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"string","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro94
DEBUG:py4j.clientserver:Answer received: !yro95
DEBUG:py4j.clientserver:Answer received: !yro97
DEBUG:py4j.clientserver:Answer received: !yro96
DEBUG:py4j.clientserver:Answer received: !yro98
DEBUG:py4j.clientserver:Answer received: !yro99
DEBUG:py4j.clientserver:Answer received: !yro100
DEBUG:py4j.clientserver:Answer received: !yro102
DEBUG:py4j.clientserver:Answer received: !yro101
DEBUG:py4j.clientserver:Answer received: !yro103
DEBUG:py4j.clientserver:Command to send: c
o94
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro104
DEBUG:py4j.clientserver:Command to send: c
o95
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o97
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o96
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o98
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o99
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o100
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o102
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o101
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o103
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o104
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro105
DEBUG:py4j.clientserver:Answer received: !yro106
DEBUG:py4j.clientserver:Answer received: !yro108
DEBUG:py4j.clientserver:Answer received: !yro107
DEBUG:py4j.clientserver:Answer received: !yro109
DEBUG:py4j.clientserver:Answer received: !yro110
DEBUG:py4j.clientserver:Answer received: !yro111
DEBUG:py4j.clientserver:Answer received: !yro113
DEBUG:py4j.clientserver:Answer received: !yro112
DEBUG:py4j.clientserver:Answer received: !yro114
DEBUG:py4j.clientserver:Answer received: !yro115
DEBUG:py4j.clientserver:Command to send: c
o105
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Command to send: c
o106
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Command to send: c
o108
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Command to send: c
o107
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Command to send: c
o109
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Command to send: c
o110
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Command to send: c
o111
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Command to send: c
o113
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Command to send: c
o112
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Command to send: c
o114
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Command to send: c
o115
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_creditcard.avro
e

DEBUG:py4j.clientserver:Answer received: !yro116
DEBUG:py4j.clientserver:Command to send: c
o116
schema
e

DEBUG:py4j.clientserver:Answer received: !yro118
DEBUG:py4j.clientserver:Command to send: c
o118
schema
e

DEBUG:py4j.clientserver:Answer received: !yro119
DEBUG:py4j.clientserver:Command to send: c
o119
json
e

DEBUG:py4j.clientserver:Answer received: !yro117
DEBUG:py4j.clientserver:Command to send: c
o117
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !yro120
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o120
schema
e

DEBUG:py4j.clientserver:Answer received: !yro121
DEBUG:py4j.clientserver:Answer received: !yro122
DEBUG:py4j.clientserver:Command to send: c
o122
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o121
schema
e

DEBUG:py4j.clientserver:Answer received: !yro123
DEBUG:py4j.clientserver:Command to send: c
o123
json
e

DEBUG:py4j.clientserver:Answer received: !yro124
DEBUG:py4j.clientserver:Command to send: c
o124
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro127
DEBUG:py4j.clientserver:Answer received: !yro125
DEBUG:py4j.clientserver:Command to send: c
o125
json
e

DEBUG:py4j.clientserver:Command to send: c
o127
schema
e

DEBUG:py4j.clientserver:Answer received: !yro129
DEBUG:py4j.clientserver:Command to send: c
o129
json
e

DEBUG:py4j.clientserver:Answer received: !yro128
DEBUG:py4j.clientserver:Command to send: c
o128
format
savro
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !yro126
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o126
schema
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro130
DEBUG:py4j.clientserver:Answer received: !yro131
DEBUG:py4j.clientserver:Command to send: c
o130
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o131
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro133
DEBUG:py4j.clientserver:Answer received: !yro134
DEBUG:py4j.clientserver:Answer received: !yro132
DEBUG:py4j.clientserver:Answer received: !yro135
DEBUG:py4j.clientserver:Command to send: c
o135
json
e

DEBUG:py4j.clientserver:Command to send: c
o133
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Answer received: !yro136
DEBUG:py4j.clientserver:Command to send: c
o132
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Command to send: c
o134
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o136
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro137
DEBUG:py4j.clientserver:Command to send: c
o137
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Answer received: !yro138
DEBUG:py4j.clientserver:Command to send: c
o138
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro139
DEBUG:py4j.clientserver:Command to send: c
o139
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Answer received: !yro140
DEBUG:py4j.clientserver:Command to send: c
o140
schema
e

DEBUG:py4j.clientserver:Answer received: !yro141
DEBUG:py4j.clientserver:Command to send: c
o141
json
e

DEBUG:py4j.clientserver:Answer received: !yro142
DEBUG:py4j.clientserver:Command to send: c
o142
schema
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro143
DEBUG:py4j.clientserver:Command to send: c
o143
json
e

DEBUG:py4j.clientserver:Answer received: !yro144
DEBUG:py4j.clientserver:Command to send: c
o144
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro145
DEBUG:py4j.clientserver:Command to send: c
o145
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro146
DEBUG:py4j.clientserver:Command to send: c
o146
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro147
DEBUG:py4j.clientserver:Command to send: c
o147
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Answer received: !yro148
DEBUG:py4j.clientserver:Command to send: c
o148
schema
e

DEBUG:py4j.clientserver:Answer received: !yro149
DEBUG:py4j.clientserver:Command to send: c
o149
json
e

DEBUG:py4j.clientserver:Answer received: !yro150
DEBUG:py4j.clientserver:Command to send: c
o150
schema
e

DEBUG:py4j.clientserver:Answer received: !yro152
DEBUG:py4j.clientserver:Answer received: !yro151
DEBUG:py4j.clientserver:Command to send: c
o152
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o151
schema
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro153
DEBUG:py4j.clientserver:Command to send: c
o153
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !yro154
DEBUG:py4j.clientserver:Command to send: c
o154
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro155
DEBUG:py4j.clientserver:Answer received: !yro156
DEBUG:py4j.clientserver:Command to send: c
o155
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro157
DEBUG:py4j.clientserver:Command to send: c
o157
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o156
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Answer received: !yro158
DEBUG:py4j.clientserver:Answer received: !yro159
DEBUG:py4j.clientserver:Command to send: c
o159
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Command to send: c
o158
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Answer received: !yro160
DEBUG:py4j.clientserver:Command to send: c
o160
schema
e

DEBUG:py4j.clientserver:Answer received: !yro161
DEBUG:py4j.clientserver:Command to send: c
o161
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro162
DEBUG:py4j.clientserver:Command to send: c
o162
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro163
DEBUG:py4j.clientserver:Command to send: c
o163
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro164
DEBUG:py4j.clientserver:Command to send: c
o164
schema
e

DEBUG:py4j.clientserver:Answer received: !yro165
DEBUG:py4j.clientserver:Command to send: c
o165
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro166
DEBUG:py4j.clientserver:Command to send: c
o166
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro167
DEBUG:py4j.clientserver:Command to send: c
o167
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro168
DEBUG:py4j.clientserver:Command to send: c
o168
schema
e

DEBUG:py4j.clientserver:Answer received: !yro169
DEBUG:py4j.clientserver:Command to send: c
o169
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro170
DEBUG:py4j.clientserver:Command to send: c
o170
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro171
DEBUG:py4j.clientserver:Answer received: !yro172
DEBUG:py4j.clientserver:Command to send: c
o172
schema
e

DEBUG:py4j.clientserver:Command to send: c
o171
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro173
DEBUG:py4j.clientserver:Command to send: c
o173
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro174
DEBUG:py4j.clientserver:Command to send: c
o174
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro176
DEBUG:py4j.clientserver:Command to send: c
o176
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro175
DEBUG:py4j.clientserver:Command to send: c
o175
schema
e

DEBUG:py4j.clientserver:Answer received: !yro177
DEBUG:py4j.clientserver:Command to send: c
o177
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro178
DEBUG:py4j.clientserver:Command to send: c
o178
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro179
DEBUG:py4j.clientserver:Command to send: c
o179
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro180
DEBUG:py4j.clientserver:Command to send: c
o180
schema
e

DEBUG:py4j.clientserver:Answer received: !yro181
DEBUG:py4j.clientserver:Command to send: c
o181
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro182
DEBUG:py4j.clientserver:Command to send: c
o182
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro183
DEBUG:py4j.clientserver:Command to send: c
o183
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro184
DEBUG:py4j.clientserver:Command to send: c
o184
schema
e

DEBUG:py4j.clientserver:Answer received: !yro185
DEBUG:py4j.clientserver:Command to send: c
o185
json
e

DEBUG:py4j.clientserver:Answer received: !yro186
DEBUG:py4j.clientserver:Command to send: c
o186
schema
e

DEBUG:py4j.clientserver:Answer received: !yro187
DEBUG:py4j.clientserver:Command to send: c
o187
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro188
DEBUG:py4j.clientserver:Command to send: c
o188
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro189
DEBUG:py4j.clientserver:Command to send: c
o189
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro190
DEBUG:py4j.clientserver:Command to send: c
o190
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro191
DEBUG:py4j.clientserver:Command to send: c
o191
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro192
DEBUG:py4j.clientserver:Command to send: c
o192
schema
e

DEBUG:py4j.clientserver:Answer received: !yro193
DEBUG:py4j.clientserver:Command to send: c
o193
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro194
DEBUG:py4j.clientserver:Command to send: c
o194
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro195
DEBUG:py4j.clientserver:Command to send: c
o195
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro196
DEBUG:py4j.clientserver:Command to send: c
o196
schema
e

DEBUG:py4j.clientserver:Answer received: !yro197
DEBUG:py4j.clientserver:Command to send: c
o197
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro198
DEBUG:py4j.clientserver:Command to send: c
o198
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro199
DEBUG:py4j.clientserver:Command to send: c
o199
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro200
DEBUG:py4j.clientserver:Command to send: c
o200
schema
e

DEBUG:py4j.clientserver:Answer received: !yro201
DEBUG:py4j.clientserver:Command to send: c
o201
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro202
DEBUG:py4j.clientserver:Command to send: c
o202
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro203
DEBUG:py4j.clientserver:Command to send: c
o203
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro204
DEBUG:py4j.clientserver:Command to send: c
o204
schema
e

DEBUG:py4j.clientserver:Answer received: !yro205
DEBUG:py4j.clientserver:Command to send: c
o205
schema
e

DEBUG:py4j.clientserver:Answer received: !yro206
DEBUG:py4j.clientserver:Command to send: c
o206
schema
e

DEBUG:py4j.clientserver:Answer received: !yro207
DEBUG:py4j.clientserver:Command to send: c
o207
json
e

DEBUG:py4j.clientserver:Answer received: !yro208
DEBUG:py4j.clientserver:Command to send: c
o208
json
e

DEBUG:py4j.clientserver:Answer received: !yro209
DEBUG:py4j.clientserver:Command to send: c
o209
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro210
DEBUG:py4j.clientserver:Command to send: c
o210
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro211
DEBUG:py4j.clientserver:Command to send: c
o211
schema
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !yro212
DEBUG:py4j.clientserver:Command to send: c
o212
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro213
DEBUG:py4j.clientserver:Command to send: c
o213
json
e

DEBUG:py4j.clientserver:Answer received: !yro214
DEBUG:py4j.clientserver:Command to send: c
o214
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro215
DEBUG:py4j.clientserver:Command to send: c
o215
format
savro
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !yro218
DEBUG:py4j.clientserver:Answer received: !yro216
DEBUG:py4j.clientserver:Command to send: c
o216
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro217
DEBUG:py4j.clientserver:Command to send: c
o218
schema
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o217
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro220
DEBUG:py4j.clientserver:Command to send: c
o220
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro219
DEBUG:py4j.clientserver:Command to send: c
o219
json
e

DEBUG:py4j.clientserver:Answer received: !yro221
DEBUG:py4j.clientserver:Command to send: c
o221
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro222
DEBUG:py4j.clientserver:Command to send: c
o222
schema
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !yro223
DEBUG:py4j.clientserver:Command to send: c
o223
json
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro224
DEBUG:py4j.clientserver:Command to send: c
o224
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro225
DEBUG:py4j.clientserver:Command to send: c
o225
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro226
DEBUG:py4j.clientserver:Command to send: c
o226
format
savro
e

DEBUG:py4j.clientserver:Command to send: m
d
o62
e

DEBUG:py4j.clientserver:Answer received: !yro227
DEBUG:py4j.clientserver:Command to send: c
o227
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o63
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o64
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o66
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o67
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o68
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o70
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o71
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o72
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o73
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o75
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o76
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o78
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o79
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o83
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o88
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o87
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o84
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o85
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o86
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o89
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o90
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o91
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o92
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o93
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o94
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o95
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o97
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o96
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o98
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o99
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o100
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o102
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o101
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o103
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o104
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o105
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o106
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o110
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o111
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o114
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o115
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o119
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o117
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o122
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o123
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o125
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o129
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o128
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o130
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o131
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro228
DEBUG:py4j.clientserver:Command to send: c
o228
schema
e

DEBUG:py4j.clientserver:Answer received: !yro229
DEBUG:py4j.clientserver:Command to send: c
o229
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro231
DEBUG:py4j.clientserver:Command to send: c
o231
schema
e

DEBUG:py4j.clientserver:Answer received: !yro230
DEBUG:py4j.clientserver:Command to send: c
o230
schema
e

DEBUG:py4j.clientserver:Answer received: !yro233
DEBUG:py4j.clientserver:Command to send: c
o233
schema
e

DEBUG:py4j.clientserver:Answer received: !yro232
DEBUG:py4j.clientserver:Answer received: !yro236
DEBUG:py4j.clientserver:Answer received: !yro237
DEBUG:py4j.clientserver:Command to send: c
o237
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o233
apply
sstart_date
e

DEBUG:py4j.clientserver:Command to send: c
o236
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro239
DEBUG:py4j.clientserver:Command to send: c
o239
schema
e

DEBUG:py4j.clientserver:Answer received: !yro240
DEBUG:py4j.clientserver:Command to send: c
o240
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !yro235
DEBUG:py4j.clientserver:Answer received: !yro234
DEBUG:py4j.clientserver:Command to send: c
o232
json
e

DEBUG:py4j.clientserver:Command to send: c
o239
apply
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !yro238
DEBUG:py4j.clientserver:Answer received: !yro241
DEBUG:py4j.clientserver:Command to send: c
o235
schema
e

DEBUG:py4j.clientserver:Command to send: c
o234
json
e

DEBUG:py4j.clientserver:Answer received: !yro242
DEBUG:py4j.clientserver:Command to send: c
o238
schema
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o241
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro243
DEBUG:py4j.clientserver:Command to send: c
o242
schema
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o231
apply
sstart_date
e

DEBUG:py4j.clientserver:Command to send: c
o230
apply
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !yro244
DEBUG:py4j.clientserver:Command to send: c
o244
json
e

DEBUG:py4j.clientserver:Command to send: c
o243
json
e

DEBUG:py4j.clientserver:Answer received: !yro245
DEBUG:py4j.clientserver:Command to send: c
o245
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o235
apply
sstart_date
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o238
apply
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !yro246
DEBUG:py4j.clientserver:Command to send: c
o246
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro247
DEBUG:py4j.clientserver:Command to send: c
o247
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro248
DEBUG:py4j.clientserver:Answer received: !yro249
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro250
DEBUG:py4j.clientserver:Command to send: c
o250
schema
e

DEBUG:py4j.clientserver:Answer received: !yro251
DEBUG:py4j.clientserver:Command to send: c
o251
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro252
DEBUG:py4j.clientserver:Command to send: c
o252
schema
e

DEBUG:py4j.clientserver:Answer received: !yro254
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro253
DEBUG:py4j.clientserver:Command to send: c
o253
format
savro
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro255
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !yro256
DEBUG:py4j.clientserver:Command to send: c
o255
json
e

DEBUG:py4j.clientserver:Answer received: !yro257
DEBUG:py4j.clientserver:Command to send: c
o256
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro258
DEBUG:py4j.clientserver:Command to send: c
o258
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro259
DEBUG:py4j.clientserver:Command to send: c
o259
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro260
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro257
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro248
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro261
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro249
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro254
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro260
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro261
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro262
DEBUG:py4j.clientserver:Command to send: c
o262
schema
e

DEBUG:py4j.clientserver:Answer received: !yro263
DEBUG:py4j.clientserver:Command to send: c
o263
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro264
DEBUG:py4j.clientserver:Command to send: c
o264
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro265
DEBUG:py4j.clientserver:Command to send: c
o265
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro266
DEBUG:py4j.clientserver:Command to send: c
o266
schema
e

DEBUG:py4j.clientserver:Answer received: !yro267
DEBUG:py4j.clientserver:Command to send: c
o267
json
e

DEBUG:py4j.clientserver:Answer received: !yro268
DEBUG:py4j.clientserver:Command to send: c
o268
schema
e

DEBUG:py4j.clientserver:Answer received: !yro269
DEBUG:py4j.clientserver:Command to send: c
o269
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o266
apply
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o268
apply
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !yro270
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro272
DEBUG:py4j.clientserver:Answer received: !yro271
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
o233
withColumn
sstart_date
ro272
e

DEBUG:py4j.clientserver:Answer received: !yro277
DEBUG:py4j.clientserver:Command to send: c
o239
withColumn
sstart_date
ro277
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !yro273
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro275
DEBUG:py4j.clientserver:Answer received: !yro274
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro270
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro276
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: c
o238
withColumn
sstart_date
ro275
e

DEBUG:py4j.clientserver:Command to send: c
o230
withColumn
sstart_date
ro274
e

DEBUG:py4j.clientserver:Command to send: c
o231
withColumn
sstart_date
ro273
e

DEBUG:py4j.clientserver:Command to send: c
o235
withColumn
sstart_date
ro276
e

DEBUG:py4j.clientserver:Answer received: !yro278
DEBUG:py4j.clientserver:Command to send: c
o266
withColumn
sstart_date
ro278
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro271
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro279
DEBUG:py4j.clientserver:Command to send: c
o268
withColumn
sstart_date
ro279
e

DEBUG:py4j.clientserver:Answer received: !yro280
DEBUG:py4j.clientserver:Command to send: c
o280
schema
e

DEBUG:py4j.clientserver:Answer received: !yro281
DEBUG:py4j.clientserver:Command to send: c
o281
schema
e

DEBUG:py4j.clientserver:Answer received: !yro282
DEBUG:py4j.clientserver:Command to send: c
o282
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o280
apply
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !yro283
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro284
DEBUG:py4j.clientserver:Command to send: c
o284
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o281
apply
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro283
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro285
DEBUG:py4j.clientserver:Command to send: c
o280
withColumn
sstart_date
ro285
e

DEBUG:py4j.clientserver:Answer received: !yro286
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro286
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro287
DEBUG:py4j.clientserver:Command to send: c
o281
withColumn
sstart_date
ro287
e

DEBUG:py4j.clientserver:Answer received: !yro288
DEBUG:py4j.clientserver:Command to send: c
o288
schema
e

DEBUG:py4j.clientserver:Answer received: !yro289
DEBUG:py4j.clientserver:Command to send: c
o289
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o288
apply
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !yro290
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro290
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro291
DEBUG:py4j.clientserver:Command to send: c
o288
withColumn
sstart_date
ro291
e

DEBUG:py4j.clientserver:Answer received: !yro292
DEBUG:py4j.clientserver:Answer received: !yro293
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro294
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro295
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !yro296
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro298
DEBUG:py4j.clientserver:Answer received: !yro297
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro299
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro300
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro301
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro302
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !yro304
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro303
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro305
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !yro307
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro306
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !yro308
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro309
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !yro310
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro311
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro298
ro306
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !yro312
DEBUG:py4j.clientserver:Answer received: !yro313
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro314
DEBUG:py4j.clientserver:Answer received: !yro315
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro316
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro317
DEBUG:py4j.clientserver:Answer received: !yro318
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro300
ro308
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro319
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !yro320
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro299
ro311
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !yro321
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !yro322
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro323
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro314
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro297
ro315
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro307
ro316
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro324
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro305
ro318
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro325
DEBUG:py4j.clientserver:Answer received: !yro326
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Command to send: c
o293
withColumn
sstart_date_add_months
ro324
e

DEBUG:py4j.clientserver:Answer received: !yro327
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro319
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !yro328
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro329
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro330
DEBUG:py4j.clientserver:Command to send: c
o294
withColumn
sstart_date_add_months
ro330
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro310
ro322
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro332
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro331
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro333
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro325
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro313
ro327
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro326
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !yro335
DEBUG:py4j.clientserver:Command to send: c
o302
withColumn
sstart_date_add_months
ro335
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro323
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro336
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro332
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !yro337
DEBUG:py4j.clientserver:Command to send: c
o295
withColumn
sstart_date_add_months
ro337
e

DEBUG:py4j.clientserver:Answer received: !yro338
DEBUG:py4j.clientserver:Command to send: c
o304
withColumn
sstart_date_add_months
ro338
e

DEBUG:py4j.clientserver:Answer received: !yro334
DEBUG:py4j.clientserver:Command to send: c
o292
withColumn
sstart_date_add_months
ro334
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro321
ro331
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro329
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro317
ro328
e

DEBUG:py4j.clientserver:Answer received: !yro341
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro339
DEBUG:py4j.clientserver:Command to send: c
o296
withColumn
sstart_date_add_months
ro339
e

DEBUG:py4j.clientserver:Answer received: !yro340
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro320
ro333
e

DEBUG:py4j.clientserver:Answer received: !yro342
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro336
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro343
DEBUG:py4j.clientserver:Command to send: c
o301
withColumn
sstart_date_add_months
ro343
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro342
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro341
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro344
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
o309
withColumn
sstart_date_add_months
ro344
e

DEBUG:py4j.clientserver:Answer received: !yro345
DEBUG:py4j.clientserver:Command to send: c
o312
withColumn
sstart_date_add_months
ro345
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !yro346
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro348
DEBUG:py4j.clientserver:Command to send: c
o346
withColumn
scurrent_timestamp
ro348
e

DEBUG:py4j.clientserver:Command to send: m
d
o33
e

DEBUG:py4j.clientserver:Answer received: !yro347
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro340
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro349
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro350
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o35
e

DEBUG:py4j.clientserver:Answer received: !yro351
DEBUG:py4j.clientserver:Command to send: c
o303
withColumn
sstart_date_add_months
ro351
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o39
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o43
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro352
DEBUG:py4j.clientserver:Command to send: m
d
o47
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro353
DEBUG:py4j.clientserver:Command to send: c
o347
withColumn
scurrent_timestamp
ro353
e

DEBUG:py4j.clientserver:Command to send: c
o352
write
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o51
e

DEBUG:py4j.clientserver:Answer received: !yro355
DEBUG:py4j.clientserver:Command to send: c
o350
withColumn
scurrent_timestamp
ro355
e

DEBUG:py4j.clientserver:Answer received: !yro354
DEBUG:py4j.clientserver:Command to send: c
o349
withColumn
scurrent_timestamp
ro354
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o55
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o59
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o60
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o108
e

DEBUG:py4j.clientserver:Answer received: !yro356
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o107
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o109
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o113
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o112
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Command to send: m
d
o124
e

DEBUG:py4j.clientserver:Answer received: !yro358
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o133
e

DEBUG:py4j.clientserver:Answer received: !yro359
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro357
DEBUG:py4j.clientserver:Command to send: c
o357
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro361
DEBUG:py4j.clientserver:Command to send: c
o356
withColumn
scurrent_timestamp
ro361
e

DEBUG:py4j.clientserver:Answer received: !yro360
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro362
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro363
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: m
d
o134
e

DEBUG:py4j.clientserver:Answer received: !yro364
DEBUG:py4j.clientserver:Command to send: c
o363
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Command to send: c
o364
write
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o132
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro365
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Command to send: c
o365
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro366
DEBUG:py4j.clientserver:Command to send: c
o359
withColumn
scurrent_timestamp
ro366
e

DEBUG:py4j.clientserver:Command to send: c
o362
write
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o136
e

DEBUG:py4j.clientserver:Answer received: !yro367
DEBUG:py4j.clientserver:Command to send: c
o358
withColumn
scurrent_timestamp
ro367
e

DEBUG:py4j.clientserver:Answer received: !yro368
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
o368
mode
soverwrite
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro369
DEBUG:py4j.clientserver:Command to send: c
o360
withColumn
scurrent_timestamp
ro369
e

DEBUG:py4j.clientserver:Answer received: !yro370
DEBUG:py4j.clientserver:Command to send: c
o370
write
e

DEBUG:py4j.clientserver:Answer received: !yro371
DEBUG:py4j.clientserver:Command to send: c
o371
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro372
DEBUG:py4j.clientserver:Command to send: c
o372
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro
e

DEBUG:py4j.clientserver:Answer received: !yro373
DEBUG:py4j.clientserver:Command to send: c
o373
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro375
DEBUG:py4j.clientserver:Answer received: !yro374
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro376
DEBUG:py4j.clientserver:Command to send: c
o376
format
savro
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro377
DEBUG:py4j.clientserver:Command to send: c
o377
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro
e

DEBUG:py4j.clientserver:Answer received: !yro378
DEBUG:py4j.clientserver:Command to send: c
o378
write
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro379
DEBUG:py4j.clientserver:Command to send: c
o375
withColumn
scurrent_timestamp
ro379
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro380
DEBUG:py4j.clientserver:Command to send: c
o374
withColumn
scurrent_timestamp
ro380
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o77
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o74
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o80
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o81
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o82
e

DEBUG:py4j.clientserver:Answer received: !yro382
DEBUG:py4j.clientserver:Command to send: c
o382
write
e

DEBUG:py4j.clientserver:Answer received: !yro381
DEBUG:py4j.clientserver:Command to send: c
o381
write
e

DEBUG:py4j.clientserver:Answer received: !yro384
DEBUG:py4j.clientserver:Command to send: c
o384
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro385
DEBUG:py4j.clientserver:Command to send: c
o385
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro386
DEBUG:py4j.clientserver:Command to send: c
o386
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o116
e

DEBUG:py4j.clientserver:Answer received: !yro383
DEBUG:py4j.clientserver:Command to send: c
o383
write
e

DEBUG:py4j.clientserver:Answer received: !yro388
DEBUG:py4j.clientserver:Command to send: c
o388
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o118
e

DEBUG:py4j.clientserver:Answer received: !yro387
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o120
e

DEBUG:py4j.clientserver:Answer received: !yro390
DEBUG:py4j.clientserver:Command to send: c
o390
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o387
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o121
e

DEBUG:py4j.clientserver:Answer received: !yro389
DEBUG:py4j.clientserver:Answer received: !yro391
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro392
DEBUG:py4j.clientserver:Command to send: c
o389
mode
soverwrite
e

DEBUG:py4j.clientserver:Command to send: c
o391
format
savro
e

DEBUG:py4j.clientserver:Command to send: m
d
o127
e

DEBUG:py4j.clientserver:Command to send: c
o392
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro
e

DEBUG:py4j.clientserver:Answer received: !yro394
DEBUG:py4j.clientserver:Command to send: c
o394
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o126
e

DEBUG:py4j.clientserver:Answer received: !yro395
DEBUG:py4j.clientserver:Command to send: c
o395
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro
e

DEBUG:py4j.clientserver:Answer received: !yro393
DEBUG:py4j.clientserver:Command to send: c
o393
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro396
DEBUG:py4j.clientserver:Command to send: c
o396
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o135
e

DEBUG:py4j.clientserver:Answer received: !yro397
DEBUG:py4j.clientserver:Command to send: c
o397
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o137
e

DEBUG:py4j.clientserver:Answer received: !yro398
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o398
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro
e

DEBUG:py4j.clientserver:Command to send: m
d
o138
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o139
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o140
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o141
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o142
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o143
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o144
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o145
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o146
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o147
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o148
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o149
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o150
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o152
e

DEBUG:py4j.clientserver:Answer received: !yro399
DEBUG:py4j.clientserver:Command to send: c
o399
write
e

DEBUG:py4j.clientserver:Answer received: !yro400
DEBUG:py4j.clientserver:Command to send: c
o400
write
e

DEBUG:py4j.clientserver:Answer received: !yro401
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro402
DEBUG:py4j.clientserver:Command to send: c
o402
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o151
e

DEBUG:py4j.clientserver:Answer received: !yro403
DEBUG:py4j.clientserver:Command to send: c
o403
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o153
e

DEBUG:py4j.clientserver:Answer received: !yro404
DEBUG:py4j.clientserver:Command to send: c
o404
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro405
DEBUG:py4j.clientserver:Command to send: m
d
o154
e

DEBUG:py4j.clientserver:Command to send: c
o405
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o155
e

DEBUG:py4j.clientserver:Answer received: !yro406
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o156
e

DEBUG:py4j.clientserver:Command to send: c
o406
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o157
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o158
e

DEBUG:py4j.clientserver:Answer received: !yro407
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o407
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro
e

DEBUG:py4j.clientserver:Command to send: m
d
o159
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o160
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o161
e

DEBUG:py4j.clientserver:Answer received: !yro408
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o401
withColumn
scurrent_timestamp
ro408
e

DEBUG:py4j.clientserver:Command to send: m
d
o162
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o163
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o164
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o165
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o166
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o167
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o168
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o169
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o170
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o171
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro409
DEBUG:py4j.clientserver:Command to send: m
d
o172
e

DEBUG:py4j.clientserver:Command to send: c
o409
write
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o173
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro410
DEBUG:py4j.clientserver:Command to send: m
d
o174
e

DEBUG:py4j.clientserver:Command to send: c
o410
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o176
e

DEBUG:py4j.clientserver:Answer received: !yro411
DEBUG:py4j.clientserver:Command to send: c
o411
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro412
DEBUG:py4j.clientserver:Command to send: c
o412
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro
e

DEBUG:py4j.clientserver:Command to send: m
d
o175
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o177
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o178
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o179
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o180
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o181
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o182
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o183
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o184
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o185
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o187
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o188
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o189
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o190
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o193
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o194
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o197
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o198
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o200
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o201
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o202
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o203
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o205
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o206
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o207
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o208
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o209
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o210
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o213
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o214
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o215
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o218
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o216
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o217
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o220
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o219
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o223
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o224
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o225
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o226
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o229
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o237
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o240
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o234
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o241
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o242
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o243
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o244
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o245
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o246
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o247
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o248
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o249
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o250
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o251
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o252
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o254
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o253
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o255
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o256
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o257
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o258
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o259
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o260
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o261
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o262
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o263
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o264
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o265
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o267
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o269
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o270
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o272
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o271
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o277
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o273
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o275
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o274
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o276
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o278
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o279
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o282
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o283
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o284
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o285
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o286
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o287
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o289
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o290
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o291
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o298
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o297
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o299
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o300
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o305
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o307
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o306
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o308
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o310
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o311
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o313
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o314
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o315
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o316
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o317
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o318
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o319
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o320
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o321
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o322
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o323
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o325
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o326
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o327
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o328
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o329
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o332
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o331
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o333
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o336
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o341
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o342
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro413
DEBUG:py4j.clientserver:Command to send: c
o413
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro414
DEBUG:py4j.clientserver:Command to send: c
o414
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_first.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro415
DEBUG:py4j.clientserver:Command to send: c
o415
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro416
DEBUG:py4j.clientserver:Command to send: c
o416
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_first.avro
e

DEBUG:py4j.clientserver:Answer received: !yro417
DEBUG:py4j.clientserver:Command to send: c
o417
schema
e

DEBUG:py4j.clientserver:Answer received: !yro418
DEBUG:py4j.clientserver:Command to send: c
o418
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"first","type":"string","nullable":true,"metadata":{}},{"name":"last","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"date","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro419
DEBUG:py4j.clientserver:Command to send: c
o419
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro420
DEBUG:py4j.clientserver:Command to send: c
o420
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_fullname.avro
e

DEBUG:py4j.clientserver:Answer received: !yro421
DEBUG:py4j.clientserver:Command to send: c
o421
schema
e

DEBUG:py4j.clientserver:Answer received: !yro422
DEBUG:py4j.clientserver:Command to send: c
o422
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"first","type":"string","nullable":true,"metadata":{}},{"name":"last","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"date","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro423
DEBUG:py4j.clientserver:Command to send: c
o423
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro424
DEBUG:py4j.clientserver:Command to send: c
o424
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_fullname.avro
e

DEBUG:py4j.clientserver:Answer received: !yro425
DEBUG:py4j.clientserver:Command to send: c
o425
schema
e

DEBUG:py4j.clientserver:Answer received: !yro426
DEBUG:py4j.clientserver:Command to send: c
o426
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Fullname","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"LastDate","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro427
DEBUG:py4j.clientserver:Command to send: c
o427
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro428
DEBUG:py4j.clientserver:Command to send: c
o428
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_name.avro
e

DEBUG:py4j.clientserver:Answer received: !yro430
DEBUG:py4j.clientserver:Answer received: !yro429
DEBUG:py4j.clientserver:Command to send: c
o430
schema
e

DEBUG:py4j.clientserver:Command to send: c
o429
schema
e

DEBUG:py4j.clientserver:Answer received: !yro431
DEBUG:py4j.clientserver:Answer received: !yro432
DEBUG:py4j.clientserver:Command to send: c
o431
json
e

DEBUG:py4j.clientserver:Command to send: c
o432
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Fullname","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"LastDate","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"DateOfBirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro433
DEBUG:py4j.clientserver:Answer received: !yro434
DEBUG:py4j.clientserver:Command to send: c
o433
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o434
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro436
DEBUG:py4j.clientserver:Answer received: !yro435
DEBUG:py4j.clientserver:Command to send: c
o436
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_name.avro
e

DEBUG:py4j.clientserver:Command to send: c
o435
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_renewaldate.avro
e

DEBUG:py4j.clientserver:Answer received: !yro437
DEBUG:py4j.clientserver:Command to send: c
o437
schema
e

DEBUG:py4j.clientserver:Answer received: !yro438
DEBUG:py4j.clientserver:Command to send: c
o438
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"DateOfBirth","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro439
DEBUG:py4j.clientserver:Command to send: c
o439
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro440
DEBUG:py4j.clientserver:Command to send: c
o440
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_renewaldate.avro
e

DEBUG:py4j.clientserver:Answer received: !yro441
DEBUG:py4j.clientserver:Command to send: c
o441
schema
e

DEBUG:py4j.clientserver:Answer received: !yro442
DEBUG:py4j.clientserver:Command to send: c
o442
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"full_name","type":"string","nullable":true,"metadata":{}},{"name":"renewal_date","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro443
DEBUG:py4j.clientserver:Command to send: c
o443
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro444
DEBUG:py4j.clientserver:Command to send: c
o444
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_revenue.avro
e

DEBUG:py4j.clientserver:Answer received: !xro445
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !xro446
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !xro447
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !xro448
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !xro449
DEBUG:py4j.clientserver:Answer received: !xro451
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !xro450
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !xro452
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro445
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro447
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro446
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro448
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro449
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro445
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro451
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro448
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro450
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro447
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro452
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro449
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro445
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro451
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro446
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yro453
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
o453
schema
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro451
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yro454
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro445
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro452
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
o454
json
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"full_name","type":"string","nullable":true,"metadata":{}},{"name":"renewal_date","type":"string","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro450
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro449
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !xro456
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro448
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yro455
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yro457
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro451
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
o457
schema
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro446
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
o455
format
savro
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yro458
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro445
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yro459
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
o458
json
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro447
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
o459
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_revenue.avro
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro452
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro456
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yro460
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
o460
format
savro
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro449
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yro461
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
o461
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_salary_data.avro
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro448
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro450
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro445
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro451
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro446
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro452
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro447
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yro462
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro456
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o462
schema
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yro463
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro449
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro448
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
o463
json
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro445
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro450
e

DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro451
e

DEBUG:py4j.clientserver:Answer received: !yro464
DEBUG:py4j.clientserver:Command to send: c
o464
schema
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yro466
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
o466
json
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yro465
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
o465
format
savro
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro446
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"fullname","type":"string","nullable":true,"metadata":{}},{"name":"salarydate","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yro467
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
o18
stop
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro452
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
o467
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_salary_data.avro
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro456
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro447
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro449
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro448
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro450
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro445
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro451
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro446
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro456
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro449
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro452
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro448
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro445
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro451
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro446
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro450
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro447
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o346
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o347
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o349
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro448
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o350
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro456
e

DEBUG:py4j.clientserver:Command to send: m
d
o351
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro449
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: m
d
o356
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro452
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yro468
DEBUG:py4j.clientserver:Command to send: c
o468
schema
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: m
d
o358
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yro469
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
o469
json
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro450
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: m
d
o359
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro451
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"fullname","type":"string","nullable":true,"metadata":{}},{"name":"salarydate","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro446
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: m
d
o357
e

DEBUG:py4j.clientserver:Command to send: c
o18
stop
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro447
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: m
d
o360
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro448
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro456
e

DEBUG:py4j.clientserver:Command to send: m
d
o363
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro452
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro445
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro449
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: m
d
o368
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: m
d
o371
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro451
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: m
d
o373
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o375
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro450
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro447
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: m
d
o374
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: m
d
o376
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro452
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro446
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro456
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro448
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o384
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro445
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: m
d
o385
e

DEBUG:py4j.clientserver:Command to send: c
o445
getCause
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro451
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o388
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro449
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o451
getCause
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: m
d
o387
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: m
d
o390
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: m
d
o389
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro450
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro447
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro446
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: m
d
o391
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o394
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro452
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro456
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: m
d
o393
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro448
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro449
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: m
d
o396
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro445
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
o449
getCause
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
o448
getCause
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o397
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro451
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro470
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro450
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_202309131046011804636866668315136_0004_m_000000 does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_20230913104601509506978415273574_0003_m_000000/part-00000-04939d18-b747-48a8-ac08-962d48c35c7e-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o445
toString
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
o451
toString
e

DEBUG:py4j.clientserver:Command to send: m
d
o401
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro447
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro446
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_202309131046011804636866668315136_0004_m_000000 does not exist
DEBUG:py4j.clientserver:Command to send: p
ro445
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_20230913104601509506978415273574_0003_m_000000/part-00000-04939d18-b747-48a8-ac08-962d48c35c7e-c000.avro does not exist
DEBUG:py4j.clientserver:Command to send: p
ro451
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o446
getCause
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro452
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_20230913104601509506978415273574_0003_m_000000/part-00000-04939d18-b747-48a8-ac08-962d48c35c7e-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
ERROR:root:Error processing old_salary_data.avro: An error occurred while calling o365.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_20230913104601509506978415273574_0003_m_000000/part-00000-04939d18-b747-48a8-ac08-962d48c35c7e-c000.avro does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)
	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

DEBUG:py4j.clientserver:Command to send: m
d
o402
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_202309131046011804636866668315136_0004_m_000000 does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Answer received: !yv
ERROR:root:Error processing old_salary_data.avro: An error occurred while calling o386.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_202309131046011804636866668315136_0004_m_000000 does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
o452
getCause
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: m
d
o403
e

DEBUG:py4j.clientserver:Answer received: !yp
INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro456
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o405
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: m
d
o406
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro450
e

DEBUG:py4j.clientserver:Command to send: m
d
o410
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o411
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
o450
getCause
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro449
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o412
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro447
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro448
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o413
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Answer received: !ysjava.nio.file.AccessDeniedException: C:\\Users\\lyekollu\\PycharmProject_Updated\\Accelerator\\Media\\destination_folder\\new__examdata_2023-09-13-10-45-53.avro\\_temporary\\0\\task_202309131046014262014114329394064_0007_m_000000: Permission denied\r\n	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1455)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
clearDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o449
toString
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 10) (LIN51006986.corp.capgemini.com executor driver): 3: The system cannot find the path specified.\r\n\r\n	at org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileWithMode0(Native Method)\r\n	at org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileOutputStreamWithMode(NativeIO.java:743)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:318)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter$$anon$1.getAvroFileOutputStream(AvroOutputWriter.scala:79)\r\n	at org.apache.avro.mapreduce.AvroKeyOutputFormat.getRecordWriter(AvroKeyOutputFormat.java:104)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter.<init>(AvroOutputWriter.scala:81)\r\n	at org.apache.spark.sql.avro.AvroOutputWriterFactory.newInstance(AvroOutputWriterFactory.scala:47)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: 3: The system cannot find the path specified.\r\n\r\n	at org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileWithMode0(Native Method)\r\n	at org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileOutputStreamWithMode(NativeIO.java:743)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:318)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter$$anon$1.getAvroFileOutputStream(AvroOutputWriter.scala:79)\r\n	at org.apache.avro.mapreduce.AvroKeyOutputFormat.getRecordWriter(AvroKeyOutputFormat.java:104)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter.<init>(AvroOutputWriter.scala:81)\r\n	at org.apache.spark.sql.avro.AvroOutputWriterFactory.newInstance(AvroOutputWriterFactory.scala:47)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	... 1 more\r\n
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro456
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !ysjava.nio.file.AccessDeniedException: C:\\Users\\lyekollu\\PycharmProject_Updated\\Accelerator\\Media\\destination_folder\\new__examdata_2023-09-13-10-45-53.avro\\_temporary\\0\\task_202309131046014262014114329394064_0007_m_000000: Permission denied
DEBUG:py4j.clientserver:Command to send: m
d
o414
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: p
ro449
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
clearDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o415
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !ysjava.nio.file.AccessDeniedException: C:\\Users\\lyekollu\\PycharmProject_Updated\\Accelerator\\Media\\destination_folder\\new__examdata_2023-09-13-10-45-53.avro\\_temporary\\0\\task_202309131046014262014114329394064_0007_m_000000: Permission denied\r\n	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1455)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
ERROR:root:Error processing old_salary_data.avro: An error occurred while calling o392.save.
: java.nio.file.AccessDeniedException: C:\Users\lyekollu\PycharmProject_Updated\Accelerator\Media\destination_folder\new__examdata_2023-09-13-10-45-53.avro\_temporary\0\task_202309131046014262014114329394064_0007_m_000000: Permission denied
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1455)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro446
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro452
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_202309131046011804636866668315136_0004_m_000000/part-00000-3667be98-072b-4c3a-8b59-9fcca1dd61e0-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_202309131046012253319154522600074_0005_m_000000/part-00000-db710b72-391f-499f-9818-18d1bf32b917-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o446
toString
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: m
d
o416
e

DEBUG:py4j.clientserver:Command to send: c
o447
getCause
e

DEBUG:py4j.clientserver:Command to send: c
o452
toString
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_202309131046012253319154522600074_0005_m_000000/part-00000-db710b72-391f-499f-9818-18d1bf32b917-c000.avro does not exist
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_202309131046011804636866668315136_0004_m_000000/part-00000-3667be98-072b-4c3a-8b59-9fcca1dd61e0-c000.avro does not exist
DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
clearActiveSession
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: m
d
o417
e

DEBUG:py4j.clientserver:Command to send: p
ro452
e

DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Command to send: p
ro446
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_202309131046011804636866668315136_0004_m_000000/part-00000-3667be98-072b-4c3a-8b59-9fcca1dd61e0-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
clearActiveSession
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_202309131046012253319154522600074_0005_m_000000/part-00000-db710b72-391f-499f-9818-18d1bf32b917-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: m
d
o418
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
ERROR:root:Error processing old_salary_data.avro: An error occurred while calling o398.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_202309131046011804636866668315136_0004_m_000000/part-00000-3667be98-072b-4c3a-8b59-9fcca1dd61e0-c000.avro does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)
	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yv
ERROR:root:Error processing old_salary_data.avro: An error occurred while calling o377.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_202309131046012253319154522600074_0005_m_000000/part-00000-db710b72-391f-499f-9818-18d1bf32b917-c000.avro does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)
	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

DEBUG:py4j.clientserver:Answer received: !yv
INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.api.python.PythonException
ro470
e

DEBUG:py4j.clientserver:Answer received: !ym
INFO:py4j.clientserver:Closing down clientserver connection
INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Command to send: m
d
o419
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro456
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro450
e

DEBUG:py4j.clientserver:Command to send: m
d
o420
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o456
getCause
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o421
e

DEBUG:py4j.clientserver:Answer received: !yro471
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ysjava.io.IOException: Failed to rename DeprecatedRawLocalFileStatus{path=file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_202309131046011804636866668315136_0004_m_000000/part-00000-3667be98-072b-4c3a-8b59-9fcca1dd61e0-c000.avro; isDirectory=false; length=944; replication=1; blocksize=33554432; modification_time=1694582164440; access_time=1694582164441; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/part-00000-3667be98-072b-4c3a-8b59-9fcca1dd61e0-c000.avro\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:477)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: m
d
o422
e

DEBUG:py4j.clientserver:Command to send: c
o450
toString
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o423
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.IOException: Failed to rename DeprecatedRawLocalFileStatus{path=file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_202309131046011804636866668315136_0004_m_000000/part-00000-3667be98-072b-4c3a-8b59-9fcca1dd61e0-c000.avro; isDirectory=false; length=944; replication=1; blocksize=33554432; modification_time=1694582164440; access_time=1694582164441; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/part-00000-3667be98-072b-4c3a-8b59-9fcca1dd61e0-c000.avro
DEBUG:py4j.clientserver:Command to send: p
ro450
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o448
toString
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.IOException: Failed to rename DeprecatedRawLocalFileStatus{path=file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_202309131046011804636866668315136_0004_m_000000/part-00000-3667be98-072b-4c3a-8b59-9fcca1dd61e0-c000.avro; isDirectory=false; length=944; replication=1; blocksize=33554432; modification_time=1694582164440; access_time=1694582164441; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/part-00000-3667be98-072b-4c3a-8b59-9fcca1dd61e0-c000.avro\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:477)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 10) (LIN51006986.corp.capgemini.com executor driver): 3: The system cannot find the path specified.\r\n\r\n	at org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileWithMode0(Native Method)\r\n	at org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileOutputStreamWithMode(NativeIO.java:743)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:318)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter$$anon$1.getAvroFileOutputStream(AvroOutputWriter.scala:79)\r\n	at org.apache.avro.mapreduce.AvroKeyOutputFormat.getRecordWriter(AvroKeyOutputFormat.java:104)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter.<init>(AvroOutputWriter.scala:81)\r\n	at org.apache.spark.sql.avro.AvroOutputWriterFactory.newInstance(AvroOutputWriterFactory.scala:47)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:
DEBUG:py4j.clientserver:Answer received: !yv
ERROR:root:Error processing old_salary_data.avro: An error occurred while calling o395.save.
: java.io.IOException: Failed to rename DeprecatedRawLocalFileStatus{path=file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/_temporary/0/task_202309131046011804636866668315136_0004_m_000000/part-00000-3667be98-072b-4c3a-8b59-9fcca1dd61e0-c000.avro; isDirectory=false; length=944; replication=1; blocksize=33554432; modification_time=1694582164440; access_time=1694582164441; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-53.avro/part-00000-3667be98-072b-4c3a-8b59-9fcca1dd61e0-c000.avro
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:477)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o424
e

INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o425
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o426
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o427
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro447
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: m
d
o428
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ysjava.nio.file.AccessDeniedException: C:\\Users\\lyekollu\\PycharmProject_Updated\\Accelerator\\Media\\destination_folder\\new__examdata_2023-09-13-10-45-53.avro\\_temporary\\0\\task_20230913104601509506978415273574_0003_m_000000: Permission denied\r\n	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1455)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: m
d
o430
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
o447
toString
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !ysjava.nio.file.AccessDeniedException: C:\\Users\\lyekollu\\PycharmProject_Updated\\Accelerator\\Media\\destination_folder\\new__examdata_2023-09-13-10-45-53.avro\\_temporary\\0\\task_20230913104601509506978415273574_0003_m_000000: Permission denied
DEBUG:py4j.clientserver:Command to send: m
d
o429
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: p
ro447
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro456
e

DEBUG:py4j.clientserver:Command to send: m
d
o431
e

DEBUG:py4j.clientserver:Answer received: !ysjava.nio.file.AccessDeniedException: C:\\Users\\lyekollu\\PycharmProject_Updated\\Accelerator\\Media\\destination_folder\\new__examdata_2023-09-13-10-45-53.avro\\_temporary\\0\\task_20230913104601509506978415273574_0003_m_000000: Permission denied\r\n	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1455)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro470
e

DEBUG:py4j.clientserver:Answer received: !yv
ERROR:root:Error processing old_salary_data.avro: An error occurred while calling o372.save.
: java.nio.file.AccessDeniedException: C:\Users\lyekollu\PycharmProject_Updated\Accelerator\Media\destination_folder\new__examdata_2023-09-13-10-45-53.avro\_temporary\0\task_20230913104601509506978415273574_0003_m_000000: Permission denied
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1455)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

DEBUG:py4j.clientserver:Command to send: m
d
o432
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 9) (LIN51006986.corp.capgemini.com executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro/_temporary/0/_temporary/attempt_202309131046017186632371168157769_0009_m_000000_9 does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:694)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:600)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:571)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.$anonfun$commitTask$1(SparkHadoopMapRedUtil.scala:51)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:51)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:78)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.$anonfun$commit$1(FileFormatDataWriter.scala:107)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:107)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:404)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n	... 15 more\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	... 1 more\r\nCaused by: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro/_temporary/0/_temporary/attempt_202309131046017186632371168157769_0009_m_000000_9 does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:694)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:600)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:571)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.$anonfun$commitTask$1(SparkHadoopMapRedUtil.scala:51)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:51)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:78)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.$anonfun$commit$1(FileFormatDataWriter.scala:107)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:107)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:404)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n	... 15 more\r\n
DEBUG:py4j.clientserver:Answer received: !ybfalse
INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o433
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o434
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o436
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o435
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o438
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o439
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o442
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o443
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.api.python.PythonException
ro471
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro470
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o456
toString
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 9) (LIN51006986.corp.capgemini.com executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro/_temporary/0/_temporary/attempt_202309131046017186632371168157769_0009_m_000000_9 does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:694)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:600)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:571)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.$anonfun$commitTask$1(SparkHadoopMapRedUtil.scala:51)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:51)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:78)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.$anonfun$commit$1(FileFormatDataWriter.scala:107)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:107)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:404)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n	... 15 more\r\n\nDriver stacktrace:
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro470
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro471
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro470
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro471
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro470
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro471
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro470
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro471
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro470
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro471
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro470
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro471
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro470
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro471
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro470
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro471
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro470
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o470
getCause
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro471
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro470
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ys3: The system cannot find the path specified.\r\n\r\n	at org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileWithMode0(Native Method)\r\n	at org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileOutputStreamWithMode(NativeIO.java:743)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:318)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter$$anon$1.getAvroFileOutputStream(AvroOutputWriter.scala:79)\r\n	at org.apache.avro.mapreduce.AvroKeyOutputFormat.getRecordWriter(AvroKeyOutputFormat.java:104)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter.<init>(AvroOutputWriter.scala:81)\r\n	at org.apache.spark.sql.avro.AvroOutputWriterFactory.newInstance(AvroOutputWriterFactory.scala:47)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o470
toString
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ys3: The system cannot find the path specified.\r\n
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: p
ro448
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro471
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 10) (LIN51006986.corp.capgemini.com executor driver): 3: The system cannot find the path specified.\r\n\r\n	at org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileWithMode0(Native Method)\r\n	at org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileOutputStreamWithMode(NativeIO.java:743)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:318)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter$$anon$1.getAvroFileOutputStream(AvroOutputWriter.scala:79)\r\n	at org.apache.avro.mapreduce.AvroKeyOutputFormat.getRecordWriter(AvroKeyOutputFormat.java:104)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter.<init>(AvroOutputWriter.scala:81)\r\n	at org.apache.spark.sql.avro.AvroOutputWriterFactory.newInstance(AvroOutputWriterFactory.scala:47)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: 3: The system cannot find the path specified.\r\n\r\n	at org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileWithMode0(Native Method)\r\n	at org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileOutputStreamWithMode(NativeIO.java:743)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:318)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter$$anon$1.getAvroFileOutputStream(AvroOutputWriter.scala:79)\r\n	at org.apache.avro.mapreduce.AvroKeyOutputFormat.getRecordWriter(AvroKeyOutputFormat.java:104)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter.<init>(AvroOutputWriter.scala:81)\r\n	at org.apache.spark.sql.avro.AvroOutputWriterFactory.newInstance(AvroOutputWriterFactory.scala:47)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	... 1 more\r\n
ERROR:root:Error processing old_salary_data.avro: An error occurred while calling o407.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 10) (LIN51006986.corp.capgemini.com executor driver): 3: The system cannot find the path specified.

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileWithMode0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileOutputStreamWithMode(NativeIO.java:743)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:318)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)
	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052)
	at org.apache.spark.sql.avro.AvroOutputWriter$$anon$1.getAvroFileOutputStream(AvroOutputWriter.scala:79)
	at org.apache.avro.mapreduce.AvroKeyOutputFormat.getRecordWriter(AvroKeyOutputFormat.java:104)
	at org.apache.spark.sql.avro.AvroOutputWriter.<init>(AvroOutputWriter.scala:81)
	at org.apache.spark.sql.avro.AvroOutputWriterFactory.newInstance(AvroOutputWriterFactory.scala:47)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: 3: The system cannot find the path specified.

	at org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileWithMode0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.createFileOutputStreamWithMode(NativeIO.java:743)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:318)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)
	at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052)
	at org.apache.spark.sql.avro.AvroOutputWriter$$anon$1.getAvroFileOutputStream(AvroOutputWriter.scala:79)
	at org.apache.avro.mapreduce.AvroKeyOutputFormat.getRecordWriter(AvroKeyOutputFormat.java:104)
	at org.apache.spark.sql.avro.AvroOutputWriter.<init>(AvroOutputWriter.scala:81)
	at org.apache.spark.sql.avro.AvroOutputWriterFactory.newInstance(AvroOutputWriterFactory.scala:47)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more

INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro471
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o471
getCause
e

DEBUG:py4j.clientserver:Answer received: !yro472
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro471
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro/_temporary/0/_temporary/attempt_202309131046017186632371168157769_0009_m_000000_9 does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:694)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:600)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:571)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.$anonfun$commitTask$1(SparkHadoopMapRedUtil.scala:51)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:51)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:78)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.$anonfun$commit$1(FileFormatDataWriter.scala:107)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:107)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:404)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n	... 15 more\r\n
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.api.python.PythonException
ro472
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o471
toString
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro.
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro472
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro472
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro472
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro472
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro472
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro472
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro472
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro472
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro472
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro472
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro472
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o472
getCause
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro472
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro/_temporary/0/_temporary/attempt_202309131046017186632371168157769_0009_m_000000_9 does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:694)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:600)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:571)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.$anonfun$commitTask$1(SparkHadoopMapRedUtil.scala:51)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:51)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:78)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.$anonfun$commit$1(FileFormatDataWriter.scala:107)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:107)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:404)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o472
toString
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro/_temporary/0/_temporary/attempt_202309131046017186632371168157769_0009_m_000000_9 does not exist
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
clearDefaultSession
e

DEBUG:py4j.clientserver:Command to send: p
ro456
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
clearDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 9) (LIN51006986.corp.capgemini.com executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro/_temporary/0/_temporary/attempt_202309131046017186632371168157769_0009_m_000000_9 does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:694)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:600)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:571)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.$anonfun$commitTask$1(SparkHadoopMapRedUtil.scala:51)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:51)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:78)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.$anonfun$commit$1(FileFormatDataWriter.scala:107)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:107)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:404)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n	... 15 more\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	... 1 more\r\nCaused by: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro/_temporary/0/_temporary/attempt_202309131046017186632371168157769_0009_m_000000_9 does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:694)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:600)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:571)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.$anonfun$commitTask$1(SparkHadoopMapRedUtil.scala:51)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:51)\r\n	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:78)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.$anonfun$commit$1(FileFormatDataWriter.scala:107)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:107)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:404)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\r\n	... 15 more\r\n
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

ERROR:root:Error processing old_salary_data.avro: An error occurred while calling o404.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 9) (LIN51006986.corp.capgemini.com executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro/_temporary/0/_temporary/attempt_202309131046017186632371168157769_0009_m_000000_9 does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:694)
	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:600)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:571)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.$anonfun$commitTask$1(SparkHadoopMapRedUtil.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:51)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:78)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:279)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.$anonfun$commit$1(FileFormatDataWriter.scala:107)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:107)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:404)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)
	... 15 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:788)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more
Caused by: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-10-45-54.avro/_temporary/0/_temporary/attempt_202309131046017186632371168157769_0009_m_000000_9 does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:694)
	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:600)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:571)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.$anonfun$commitTask$1(SparkHadoopMapRedUtil.scala:51)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:51)
	at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:78)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:279)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.$anonfun$commit$1(FileFormatDataWriter.scala:107)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:107)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:404)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)
	... 15 more

INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
clearActiveSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
clearActiveSession
e

DEBUG:py4j.clientserver:Answer received: !yv
INFO:py4j.clientserver:Closing down clientserver connection
INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:asyncio:Using proactor: IocpProactor
DEBUG:py4j.java_gateway:GatewayClient.address is deprecated and will be removed in version 1.0. Use GatewayParameters instead.
DEBUG:py4j.clientserver:Command to send: A
cdf5bf6b39b4b2768372b5f8983c1cbc16d085872e3185f3b51e8ddfde815a51

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.SparkConf
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.api.java.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.api.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.ml.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.mllib.api.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.resource.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.sql.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.sql.api.python.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
org.apache.spark.sql.hive.*
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: j
i
rj
scala.Tuple2
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkConf
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkConf
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.SparkConf
bTrue
e

DEBUG:py4j.clientserver:Answer received: !yro0
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yro1
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yro2
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.serializer.objectStreamReset
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.serializer.objectStreamReset
s100
e

DEBUG:py4j.clientserver:Answer received: !yro3
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.rdd.compress
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o0
set
sspark.rdd.compress
sTrue
e

DEBUG:py4j.clientserver:Answer received: !yro4
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.master
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.app.name
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.master
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
get
sspark.master
e

DEBUG:py4j.clientserver:Answer received: !yslocal[*]
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.app.name
e

DEBUG:py4j.clientserver:Answer received: !ybtrue
DEBUG:py4j.clientserver:Command to send: c
o0
get
sspark.app.name
e

DEBUG:py4j.clientserver:Answer received: !ysAvroProcessing
DEBUG:py4j.clientserver:Command to send: c
o0
contains
sspark.home
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o0
getAll
e

DEBUG:py4j.clientserver:Answer received: !yto5
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i0
e

DEBUG:py4j.clientserver:Answer received: !yro6
DEBUG:py4j.clientserver:Command to send: c
o6
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.rdd.compress
DEBUG:py4j.clientserver:Command to send: c
o6
_2
e

DEBUG:py4j.clientserver:Answer received: !ysTrue
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i1
e

DEBUG:py4j.clientserver:Answer received: !yro7
DEBUG:py4j.clientserver:Command to send: c
o7
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.app.name
DEBUG:py4j.clientserver:Command to send: c
o7
_2
e

DEBUG:py4j.clientserver:Answer received: !ysAvroProcessing
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i2
e

DEBUG:py4j.clientserver:Answer received: !yro8
DEBUG:py4j.clientserver:Command to send: c
o8
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.app.submitTime
DEBUG:py4j.clientserver:Command to send: c
o8
_2
e

DEBUG:py4j.clientserver:Answer received: !ys1694583908013
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i3
e

DEBUG:py4j.clientserver:Answer received: !yro9
DEBUG:py4j.clientserver:Command to send: c
o9
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.serializer.objectStreamReset
DEBUG:py4j.clientserver:Command to send: c
o9
_2
e

DEBUG:py4j.clientserver:Answer received: !ys100
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i4
e

DEBUG:py4j.clientserver:Answer received: !yro10
DEBUG:py4j.clientserver:Command to send: c
o10
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.submit.pyFiles
DEBUG:py4j.clientserver:Command to send: c
o10
_2
e

DEBUG:py4j.clientserver:Answer received: !ysC:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i5
e

DEBUG:py4j.clientserver:Answer received: !yro11
DEBUG:py4j.clientserver:Command to send: c
o11
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.master
DEBUG:py4j.clientserver:Command to send: c
o11
_2
e

DEBUG:py4j.clientserver:Answer received: !yslocal[*]
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i6
e

DEBUG:py4j.clientserver:Answer received: !yro12
DEBUG:py4j.clientserver:Command to send: c
o12
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.jars.packages
DEBUG:py4j.clientserver:Command to send: c
o12
_2
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark:spark-avro_2.12:3.4.1
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i7
e

DEBUG:py4j.clientserver:Answer received: !yro13
DEBUG:py4j.clientserver:Command to send: c
o13
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.submit.deployMode
DEBUG:py4j.clientserver:Command to send: c
o13
_2
e

DEBUG:py4j.clientserver:Answer received: !ysclient
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i8
e

DEBUG:py4j.clientserver:Answer received: !yro14
DEBUG:py4j.clientserver:Command to send: c
o14
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.files
DEBUG:py4j.clientserver:Command to send: c
o14
_2
e

DEBUG:py4j.clientserver:Answer received: !ysfile:///C:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,file:///C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i9
e

DEBUG:py4j.clientserver:Answer received: !yro15
DEBUG:py4j.clientserver:Command to send: c
o15
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.repl.local.jars
DEBUG:py4j.clientserver:Command to send: c
o15
_2
e

DEBUG:py4j.clientserver:Answer received: !ysfile:///C:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,file:///C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i10
e

DEBUG:py4j.clientserver:Answer received: !yro16
DEBUG:py4j.clientserver:Command to send: c
o16
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.jars
DEBUG:py4j.clientserver:Command to send: c
o16
_2
e

DEBUG:py4j.clientserver:Answer received: !ysfile:///C:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,file:///C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: a
g
o5
i11
e

DEBUG:py4j.clientserver:Answer received: !yro17
DEBUG:py4j.clientserver:Command to send: c
o17
_1
e

DEBUG:py4j.clientserver:Answer received: !ysspark.ui.showConsoleProgress
DEBUG:py4j.clientserver:Command to send: c
o17
_2
e

DEBUG:py4j.clientserver:Answer received: !ystrue
DEBUG:py4j.clientserver:Command to send: a
e
o5
e

DEBUG:py4j.clientserver:Answer received: !yi12
DEBUG:py4j.clientserver:Command to send: r
u
JavaSparkContext
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.java.JavaSparkContext
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.api.java.JavaSparkContext
ro0
e

DEBUG:py4j.clientserver:Command to send: A
cdf5bf6b39b4b2768372b5f8983c1cbc16d085872e3185f3b51e8ddfde815a51

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o1
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o2
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o3
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o4
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o5
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro18
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro19
DEBUG:py4j.clientserver:Command to send: c
o19
conf
e

DEBUG:py4j.clientserver:Answer received: !yro20
DEBUG:py4j.clientserver:Command to send: r
u
PythonAccumulatorV2
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonAccumulatorV2
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.api.python.PythonAccumulatorV2
s127.0.0.1
i57989
scdf5bf6b39b4b2768372b5f8983c1cbc16d085872e3185f3b51e8ddfde815a51
e

DEBUG:py4j.clientserver:Answer received: !yro21
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro22
DEBUG:py4j.clientserver:Command to send: c
o22
register
ro21
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
PythonUtils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.api.python.PythonUtils
isEncryptionEnabled
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.api.python.PythonUtils
isEncryptionEnabled
ro18
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
PythonUtils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.api.python.PythonUtils
getPythonAuthSocketTimeout
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.api.python.PythonUtils
getPythonAuthSocketTimeout
ro18
e

DEBUG:py4j.clientserver:Answer received: !yL15
DEBUG:py4j.clientserver:Command to send: r
u
PythonUtils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.api.python.PythonUtils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.api.python.PythonUtils
getSparkBufferSize
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.api.python.PythonUtils
getSparkBufferSize
ro18
e

DEBUG:py4j.clientserver:Answer received: !yi65536
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.SparkFiles
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-e09ef9a5-88a1-4c07-96b6-99bbb354068c\\userFiles-bcbf565e-d530-49d7-9041-cc7655f0ce06
DEBUG:py4j.clientserver:Command to send: c
o20
get
sspark.submit.pyFiles
s
e

DEBUG:py4j.clientserver:Answer received: !ysC:/Users/lyekollu/.ivy2/jars/org.apache.spark_spark-avro_2.12-3.4.1.jar,C:/Users/lyekollu/.ivy2/jars/org.tukaani_xz-1.9.jar
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.SparkFiles
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-e09ef9a5-88a1-4c07-96b6-99bbb354068c\\userFiles-bcbf565e-d530-49d7-9041-cc7655f0ce06
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.SparkFiles
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.SparkFiles
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-e09ef9a5-88a1-4c07-96b6-99bbb354068c\\userFiles-bcbf565e-d530-49d7-9041-cc7655f0ce06
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
getLocalDir
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro23
DEBUG:py4j.clientserver:Command to send: c
o23
conf
e

DEBUG:py4j.clientserver:Answer received: !yro24
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
getLocalDir
ro24
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-e09ef9a5-88a1-4c07-96b6-99bbb354068c
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
createTempDir
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
createTempDir
sC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-e09ef9a5-88a1-4c07-96b6-99bbb354068c
spyspark
e

DEBUG:py4j.clientserver:Answer received: !yro25
DEBUG:py4j.clientserver:Command to send: c
o25
getAbsolutePath
e

DEBUG:py4j.clientserver:Answer received: !ysC:\\Users\\lyekollu\\AppData\\Local\\Temp\\spark-e09ef9a5-88a1-4c07-96b6-99bbb354068c\\pyspark-afa33e17-7cab-4318-a029-8390e3d056cd
DEBUG:py4j.clientserver:Command to send: c
o20
get
sspark.python.profile
sfalse
e

DEBUG:py4j.clientserver:Answer received: !ysfalse
DEBUG:py4j.clientserver:Command to send: c
o20
get
sspark.python.profile.memory
sfalse
e

DEBUG:py4j.clientserver:Answer received: !ysfalse
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
getDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
getDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !yro26
DEBUG:py4j.clientserver:Command to send: c
o26
isDefined
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: c
o18
sc
e

DEBUG:py4j.clientserver:Answer received: !yro27
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao28
DEBUG:py4j.clientserver:Command to send: c
o28
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o28
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: i
org.apache.spark.sql.SparkSession
ro27
ro28
e

DEBUG:py4j.clientserver:Answer received: !yro29
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
setDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
setDefaultSession
ro29
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
setActiveSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
setActiveSession
ro29
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
cdf5bf6b39b4b2768372b5f8983c1cbc16d085872e3185f3b51e8ddfde815a51

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro30
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao31
DEBUG:py4j.clientserver:Command to send: c
o31
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o31
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o30
applyModifiableSettings
ro29
ro31
e

DEBUG:py4j.clientserver:Command to send: m
d
o28
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro32
DEBUG:py4j.clientserver:Command to send: c
o32
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro33
DEBUG:py4j.clientserver:Command to send: c
o33
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
cdf5bf6b39b4b2768372b5f8983c1cbc16d085872e3185f3b51e8ddfde815a51

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro34
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Command to send: c
o34
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro35
DEBUG:py4j.clientserver:Command to send: c
o35
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro36
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao37
DEBUG:py4j.clientserver:Command to send: c
o37
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o37
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o36
applyModifiableSettings
ro29
ro37
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Command to send: A
cdf5bf6b39b4b2768372b5f8983c1cbc16d085872e3185f3b51e8ddfde815a51

DEBUG:py4j.clientserver:Answer received: !yro38
DEBUG:py4j.clientserver:Command to send: c
o38
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro39
DEBUG:py4j.clientserver:Command to send: c
o39
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro40
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao41
DEBUG:py4j.clientserver:Command to send: c
o41
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o41
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o40
applyModifiableSettings
ro29
ro41
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
cdf5bf6b39b4b2768372b5f8983c1cbc16d085872e3185f3b51e8ddfde815a51

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro42
DEBUG:py4j.clientserver:Command to send: c
o42
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro43
DEBUG:py4j.clientserver:Command to send: c
o43
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro44
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao45
DEBUG:py4j.clientserver:Command to send: c
o45
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o45
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o44
applyModifiableSettings
ro29
ro45
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
cdf5bf6b39b4b2768372b5f8983c1cbc16d085872e3185f3b51e8ddfde815a51

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro46
DEBUG:py4j.clientserver:Command to send: c
o46
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !yro47
DEBUG:py4j.clientserver:Command to send: c
o47
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro48
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao49
DEBUG:py4j.clientserver:Command to send: c
o49
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o49
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o48
applyModifiableSettings
ro29
ro49
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
cdf5bf6b39b4b2768372b5f8983c1cbc16d085872e3185f3b51e8ddfde815a51

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro50
DEBUG:py4j.clientserver:Command to send: c
o50
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !yro51
DEBUG:py4j.clientserver:Command to send: c
o51
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro52
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao53
DEBUG:py4j.clientserver:Command to send: c
o53
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o53
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o52
applyModifiableSettings
ro29
ro53
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
cdf5bf6b39b4b2768372b5f8983c1cbc16d085872e3185f3b51e8ddfde815a51

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !yro54
DEBUG:py4j.clientserver:Command to send: c
o54
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro55
DEBUG:py4j.clientserver:Command to send: c
o55
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro56
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao57
DEBUG:py4j.clientserver:Command to send: c
o57
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o57
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o56
applyModifiableSettings
ro29
ro57
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
cdf5bf6b39b4b2768372b5f8983c1cbc16d085872e3185f3b51e8ddfde815a51

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro58
DEBUG:py4j.clientserver:Command to send: c
o58
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro59
DEBUG:py4j.clientserver:Command to send: c
o59
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro60
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao61
DEBUG:py4j.clientserver:Command to send: c
o61
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o61
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o60
applyModifiableSettings
ro29
ro61
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Command to send: A
cdf5bf6b39b4b2768372b5f8983c1cbc16d085872e3185f3b51e8ddfde815a51

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro62
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o62
format
savro
e

DEBUG:py4j.clientserver:Command to send: r
u
SparkSession$
rj
e

DEBUG:py4j.clientserver:Answer received: !yro63
DEBUG:py4j.clientserver:Command to send: c
o63
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession$
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

DEBUG:py4j.clientserver:Answer received: !yro64
DEBUG:py4j.clientserver:Command to send: i
java.util.HashMap
e

DEBUG:py4j.clientserver:Answer received: !yao65
DEBUG:py4j.clientserver:Command to send: c
o65
put
sspark.app.name
sAvroProcessing
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o65
put
sspark.jars.packages
sorg.apache.spark:spark-avro_2.12:3.4.1
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: c
o64
applyModifiableSettings
ro29
ro65
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
setLogLevel
sOFF
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro66
DEBUG:py4j.clientserver:Command to send: c
o66
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro67
DEBUG:py4j.clientserver:Command to send: c
o67
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_citydata.avro
e

DEBUG:py4j.clientserver:Command to send: m
d
o31
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o37
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o41
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o45
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o49
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o53
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o57
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o61
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o0
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o6
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o7
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o8
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o9
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o10
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o11
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o12
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o13
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o14
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o15
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o16
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o17
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o19
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o22
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o23
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o24
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o25
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o26
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o27
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o30
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o32
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o34
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o36
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o38
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o40
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o42
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o44
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o46
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o48
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o50
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o52
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o54
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o56
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o58
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o60
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o62
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o65
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro68
DEBUG:py4j.clientserver:Command to send: c
o68
schema
e

DEBUG:py4j.clientserver:Answer received: !yro69
DEBUG:py4j.clientserver:Answer received: !yro70
DEBUG:py4j.clientserver:Command to send: c
o69
schema
e

DEBUG:py4j.clientserver:Command to send: c
o70
schema
e

DEBUG:py4j.clientserver:Answer received: !yro72
DEBUG:py4j.clientserver:Answer received: !yro71
DEBUG:py4j.clientserver:Command to send: c
o72
schema
e

DEBUG:py4j.clientserver:Command to send: c
o71
schema
e

DEBUG:py4j.clientserver:Answer received: !yro73
DEBUG:py4j.clientserver:Answer received: !yro74
DEBUG:py4j.clientserver:Command to send: c
o73
schema
e

DEBUG:py4j.clientserver:Answer received: !yro75
DEBUG:py4j.clientserver:Command to send: c
o74
schema
e

DEBUG:py4j.clientserver:Answer received: !yro76
DEBUG:py4j.clientserver:Command to send: c
o75
schema
e

DEBUG:py4j.clientserver:Command to send: c
o76
schema
e

DEBUG:py4j.clientserver:Answer received: !yro77
DEBUG:py4j.clientserver:Command to send: c
o77
schema
e

DEBUG:py4j.clientserver:Answer received: !yro78
DEBUG:py4j.clientserver:Command to send: c
o78
json
e

DEBUG:py4j.clientserver:Answer received: !yro80
DEBUG:py4j.clientserver:Answer received: !yro83
DEBUG:py4j.clientserver:Answer received: !yro81
DEBUG:py4j.clientserver:Answer received: !yro79
DEBUG:py4j.clientserver:Answer received: !yro82
DEBUG:py4j.clientserver:Command to send: c
o80
json
e

DEBUG:py4j.clientserver:Answer received: !yro84
DEBUG:py4j.clientserver:Answer received: !yro85
DEBUG:py4j.clientserver:Command to send: c
o85
json
e

DEBUG:py4j.clientserver:Answer received: !yro86
DEBUG:py4j.clientserver:Command to send: c
o83
json
e

DEBUG:py4j.clientserver:Command to send: c
o81
json
e

DEBUG:py4j.clientserver:Command to send: c
o79
json
e

DEBUG:py4j.clientserver:Command to send: c
o82
json
e

DEBUG:py4j.clientserver:Command to send: c
o84
json
e

DEBUG:py4j.clientserver:Command to send: c
o86
json
e

DEBUG:py4j.clientserver:Answer received: !yro87
DEBUG:py4j.clientserver:Command to send: c
o87
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"date","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o77
apply
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"date","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o73
apply
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"date","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o74
apply
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"date","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o69
apply
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"date","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o71
apply
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"date","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o72
apply
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"date","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o76
apply
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"date","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"date","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o75
apply
ssurveydate
e

DEBUG:py4j.clientserver:Command to send: c
o70
apply
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"surveydate","type":"date","nullable":true,"metadata":{}},{"name":"city_name","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}},{"name":"latitude","type":"double","nullable":true,"metadata":{}},{"name":"longitude","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o68
apply
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !yro88
DEBUG:py4j.clientserver:Answer received: !yro95
DEBUG:py4j.clientserver:Answer received: !yro89
DEBUG:py4j.clientserver:Answer received: !yro91
DEBUG:py4j.clientserver:Answer received: !yro94
DEBUG:py4j.clientserver:Answer received: !yro93
DEBUG:py4j.clientserver:Answer received: !yro90
DEBUG:py4j.clientserver:Answer received: !yro96
DEBUG:py4j.clientserver:Answer received: !yro97
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro92
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro92
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro95
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro91
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro89
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro88
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro94
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro90
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro93
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro96
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro97
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro98
DEBUG:py4j.clientserver:Answer received: !yro99
DEBUG:py4j.clientserver:Answer received: !yro104
DEBUG:py4j.clientserver:Answer received: !yro100
DEBUG:py4j.clientserver:Answer received: !yro101
DEBUG:py4j.clientserver:Answer received: !yro102
DEBUG:py4j.clientserver:Answer received: !yro103
DEBUG:py4j.clientserver:Answer received: !yro105
DEBUG:py4j.clientserver:Answer received: !yro106
DEBUG:py4j.clientserver:Answer received: !yro107
DEBUG:py4j.clientserver:Command to send: c
o70
withColumn
ssurveydate
ro98
e

DEBUG:py4j.clientserver:Command to send: c
o73
withColumn
ssurveydate
ro99
e

DEBUG:py4j.clientserver:Command to send: c
o68
withColumn
ssurveydate
ro104
e

DEBUG:py4j.clientserver:Command to send: c
o71
withColumn
ssurveydate
ro100
e

DEBUG:py4j.clientserver:Command to send: c
o74
withColumn
ssurveydate
ro101
e

DEBUG:py4j.clientserver:Command to send: c
o76
withColumn
ssurveydate
ro102
e

DEBUG:py4j.clientserver:Command to send: c
o72
withColumn
ssurveydate
ro103
e

DEBUG:py4j.clientserver:Command to send: c
o69
withColumn
ssurveydate
ro105
e

DEBUG:py4j.clientserver:Command to send: c
o77
withColumn
ssurveydate
ro106
e

DEBUG:py4j.clientserver:Command to send: c
o75
withColumn
ssurveydate
ro107
e

DEBUG:py4j.clientserver:Answer received: !yro108
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro109
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
ssurveydate
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !yro110
DEBUG:py4j.clientserver:Answer received: !yro111
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro112
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro113
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro114
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !yro116
DEBUG:py4j.clientserver:Answer received: !yro115
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !yro117
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro118
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro119
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro121
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro110
ro115
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro111
ro117
e

DEBUG:py4j.clientserver:Answer received: !yro122
DEBUG:py4j.clientserver:Answer received: !yro123
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro120
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !yro124
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro125
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro126
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro127
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro128
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro123
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro122
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro129
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro130
DEBUG:py4j.clientserver:Command to send: c
o109
withColumn
ssurveydate_add_months
ro129
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro131
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
o108
withColumn
ssurveydate_add_months
ro130
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !yro132
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !yro133
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro118
ro127
e

DEBUG:py4j.clientserver:Answer received: !yro135
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro134
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro136
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
ssurveydate
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !yro137
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro138
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro119
ro133
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro139
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro124
ro132
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro125
ro135
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro140
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !yro141
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro120
ro131
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro142
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
ssurveydate
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro143
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro144
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro145
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro137
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro139
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !yro146
DEBUG:py4j.clientserver:Command to send: c
o113
withColumn
ssurveydate_add_months
ro146
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro147
DEBUG:py4j.clientserver:Command to send: c
o114
withColumn
ssurveydate_add_months
ro147
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro136
ro142
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro140
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro141
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro138
ro144
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro149
DEBUG:py4j.clientserver:Answer received: !yro148
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro143
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro150
DEBUG:py4j.clientserver:Command to send: c
o116
withColumn
ssurveydate_add_months
ro148
e

DEBUG:py4j.clientserver:Answer received: !yro151
DEBUG:py4j.clientserver:Answer received: !yro152
DEBUG:py4j.clientserver:Answer received: !yro153
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
o112
withColumn
ssurveydate_add_months
ro151
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
o121
withColumn
ssurveydate_add_months
ro150
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro149
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro154
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro145
ro153
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro152
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro155
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Command to send: c
o126
withColumn
ssurveydate_add_months
ro155
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro156
DEBUG:py4j.clientserver:Command to send: c
o154
withColumn
scurrent_timestamp
ro156
e

DEBUG:py4j.clientserver:Answer received: !yro157
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro160
DEBUG:py4j.clientserver:Answer received: !yro161
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
o128
withColumn
ssurveydate_add_months
ro160
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro162
DEBUG:py4j.clientserver:Command to send: c
o157
withColumn
scurrent_timestamp
ro162
e

DEBUG:py4j.clientserver:Answer received: !yro158
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro159
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro163
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro164
DEBUG:py4j.clientserver:Command to send: c
o158
withColumn
scurrent_timestamp
ro164
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro165
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
o165
write
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro166
DEBUG:py4j.clientserver:Command to send: c
o159
withColumn
scurrent_timestamp
ro166
e

DEBUG:py4j.clientserver:Answer received: !yro167
DEBUG:py4j.clientserver:Command to send: c
o167
write
e

DEBUG:py4j.clientserver:Answer received: !yro168
DEBUG:py4j.clientserver:Command to send: c
o168
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro171
DEBUG:py4j.clientserver:Command to send: c
o171
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro172
DEBUG:py4j.clientserver:Command to send: c
o172
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro173
DEBUG:py4j.clientserver:Command to send: c
o173
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro174
DEBUG:py4j.clientserver:Command to send: c
o174
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro
e

DEBUG:py4j.clientserver:Answer received: !yro175
DEBUG:py4j.clientserver:Command to send: c
o175
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro
e

DEBUG:py4j.clientserver:Answer received: !yro169
DEBUG:py4j.clientserver:Command to send: c
o169
write
e

DEBUG:py4j.clientserver:Answer received: !yro170
DEBUG:py4j.clientserver:Command to send: c
o170
write
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro161
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro176
DEBUG:py4j.clientserver:Command to send: c
o134
withColumn
ssurveydate_add_months
ro176
e

DEBUG:py4j.clientserver:Answer received: !yro177
DEBUG:py4j.clientserver:Command to send: c
o177
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro178
DEBUG:py4j.clientserver:Command to send: c
o178
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro179
DEBUG:py4j.clientserver:Command to send: c
o179
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro180
DEBUG:py4j.clientserver:Command to send: c
o163
withColumn
scurrent_timestamp
ro180
e

DEBUG:py4j.clientserver:Answer received: !yro181
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro182
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro183
DEBUG:py4j.clientserver:Command to send: c
o182
write
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro184
DEBUG:py4j.clientserver:Command to send: c
o181
withColumn
scurrent_timestamp
ro184
e

DEBUG:py4j.clientserver:Answer received: !yro185
DEBUG:py4j.clientserver:Command to send: c
o185
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro186
DEBUG:py4j.clientserver:Command to send: c
o186
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro187
DEBUG:py4j.clientserver:Command to send: c
o187
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro
e

DEBUG:py4j.clientserver:Answer received: !yro188
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro189
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro190
DEBUG:py4j.clientserver:Command to send: c
o188
withColumn
scurrent_timestamp
ro190
e

DEBUG:py4j.clientserver:Answer received: !yro191
DEBUG:py4j.clientserver:Command to send: c
o191
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro193
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro192
DEBUG:py4j.clientserver:Command to send: c
o192
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro194
DEBUG:py4j.clientserver:Command to send: c
o194
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-41.avro
e

DEBUG:py4j.clientserver:Answer received: !yro195
DEBUG:py4j.clientserver:Command to send: c
o195
write
e

DEBUG:py4j.clientserver:Answer received: !yro196
DEBUG:py4j.clientserver:Command to send: c
o196
write
e

DEBUG:py4j.clientserver:Answer received: !yro198
DEBUG:py4j.clientserver:Command to send: c
o198
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro197
DEBUG:py4j.clientserver:Answer received: !yro199
DEBUG:py4j.clientserver:Command to send: c
o199
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o197
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro200
DEBUG:py4j.clientserver:Command to send: c
o200
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-41.avro
e

DEBUG:py4j.clientserver:Answer received: !yro201
DEBUG:py4j.clientserver:Command to send: c
o201
format
savro
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro202
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Command to send: c
o202
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-41.avro
e

DEBUG:py4j.clientserver:Answer received: !yro203
DEBUG:py4j.clientserver:Command to send: c
o189
withColumn
scurrent_timestamp
ro203
e

DEBUG:py4j.clientserver:Answer received: !yro204
DEBUG:py4j.clientserver:Command to send: c
o204
write
e

DEBUG:py4j.clientserver:Answer received: !yro205
DEBUG:py4j.clientserver:Command to send: c
o205
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro206
DEBUG:py4j.clientserver:Command to send: c
o206
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro207
DEBUG:py4j.clientserver:Command to send: c
o207
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-41.avro
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro208
DEBUG:py4j.clientserver:Command to send: c
o193
withColumn
scurrent_timestamp
ro208
e

DEBUG:py4j.clientserver:Answer received: !yro209
DEBUG:py4j.clientserver:Command to send: c
o209
write
e

DEBUG:py4j.clientserver:Answer received: !yro210
DEBUG:py4j.clientserver:Command to send: c
o210
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro211
DEBUG:py4j.clientserver:Command to send: c
o211
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro212
DEBUG:py4j.clientserver:Command to send: c
o212
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-41.avro
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro213
DEBUG:py4j.clientserver:Command to send: c
o183
withColumn
scurrent_timestamp
ro213
e

DEBUG:py4j.clientserver:Answer received: !yro214
DEBUG:py4j.clientserver:Command to send: c
o214
write
e

DEBUG:py4j.clientserver:Answer received: !yro215
DEBUG:py4j.clientserver:Command to send: c
o215
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro216
DEBUG:py4j.clientserver:Command to send: c
o216
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro217
DEBUG:py4j.clientserver:Command to send: c
o217
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-41.avro
e

DEBUG:py4j.clientserver:Command to send: m
d
o66
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o67
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o78
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o80
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o83
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o81
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o79
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o82
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o84
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o85
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o86
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o87
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o88
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o95
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o89
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o91
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o94
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o93
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o90
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o96
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o97
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o92
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o98
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o99
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o104
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o100
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o101
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o102
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o103
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o105
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o106
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o107
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o110
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o111
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o115
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o117
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o122
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o123
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro218
DEBUG:py4j.clientserver:Answer received: !yro219
DEBUG:py4j.clientserver:Command to send: c
o218
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro220
DEBUG:py4j.clientserver:Command to send: c
o219
format
savro
e

DEBUG:py4j.clientserver:Command to send: c
o220
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro221
DEBUG:py4j.clientserver:Command to send: c
o221
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_credtcard.avro
e

DEBUG:py4j.clientserver:Answer received: !yro222
DEBUG:py4j.clientserver:Answer received: !yro223
DEBUG:py4j.clientserver:Command to send: c
o222
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_credtcard.avro
e

DEBUG:py4j.clientserver:Command to send: c
o223
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_credtcard.avro
e

DEBUG:py4j.clientserver:Answer received: !yro224
DEBUG:py4j.clientserver:Command to send: c
o224
schema
e

DEBUG:py4j.clientserver:Answer received: !yro225
DEBUG:py4j.clientserver:Command to send: c
o225
json
e

DEBUG:py4j.clientserver:Answer received: !yro226
DEBUG:py4j.clientserver:Command to send: c
o226
schema
e

DEBUG:py4j.clientserver:Answer received: !yro227
DEBUG:py4j.clientserver:Answer received: !yro228
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"date","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o227
schema
e

DEBUG:py4j.clientserver:Command to send: c
o228
json
e

DEBUG:py4j.clientserver:Command to send: c
o224
apply
sdateofexpire
e

DEBUG:py4j.clientserver:Answer received: !yro229
DEBUG:py4j.clientserver:Command to send: c
o229
json
e

DEBUG:py4j.clientserver:Answer received: !yro230
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"date","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"s_no","type":"integer","nullable":true,"metadata":{}},{"name":"first_name","type":"string","nullable":true,"metadata":{}},{"name":"last_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"dateofexpire","type":"date","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"long","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o226
apply
sdateofexpire
e

DEBUG:py4j.clientserver:Command to send: c
o227
apply
sdateofexpire
e

DEBUG:py4j.clientserver:Answer received: !yro231
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro230
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro231
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro232
DEBUG:py4j.clientserver:Command to send: c
o224
withColumn
sdateofexpire
ro232
e

DEBUG:py4j.clientserver:Answer received: !yro233
DEBUG:py4j.clientserver:Command to send: c
o226
withColumn
sdateofexpire
ro233
e

DEBUG:py4j.clientserver:Answer received: !yro234
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro235
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro236
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sdateofexpire
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sdateofexpire
e

DEBUG:py4j.clientserver:Answer received: !yro237
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro238
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro236
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yro239
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Command to send: c
o227
withColumn
sdateofexpire
ro239
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro240
DEBUG:py4j.clientserver:Answer received: !yro241
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !yro242
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro237
ro241
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro238
ro240
e

DEBUG:py4j.clientserver:Answer received: !yro243
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro244
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sdateofexpire
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro244
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro245
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro246
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro243
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Command to send: c
o235
withColumn
sdateofexpire_add_months
ro246
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !yro247
DEBUG:py4j.clientserver:Command to send: c
o234
withColumn
sdateofexpire_add_months
ro247
e

DEBUG:py4j.clientserver:Answer received: !yro248
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro249
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro250
DEBUG:py4j.clientserver:Command to send: c
o249
withColumn
scurrent_timestamp
ro250
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro251
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro245
ro251
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro252
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yro253
DEBUG:py4j.clientserver:Command to send: c
o248
withColumn
scurrent_timestamp
ro253
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro252
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro254
DEBUG:py4j.clientserver:Command to send: c
o242
withColumn
sdateofexpire_add_months
ro254
e

DEBUG:py4j.clientserver:Answer received: !yro255
DEBUG:py4j.clientserver:Command to send: c
o255
write
e

DEBUG:py4j.clientserver:Answer received: !yro256
DEBUG:py4j.clientserver:Command to send: c
o256
mode
soverwrite
e

DEBUG:py4j.clientserver:Command to send: m
d
o33
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro257
DEBUG:py4j.clientserver:Command to send: c
o257
format
savro
e

DEBUG:py4j.clientserver:Command to send: m
d
o35
e

DEBUG:py4j.clientserver:Answer received: !yro260
DEBUG:py4j.clientserver:Answer received: !xro258
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o260
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__credtcard_2023-09-13-11-15-50.avro
e

DEBUG:py4j.clientserver:Answer received: !xro261
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !xro263
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !xro264
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !xro262
DEBUG:py4j.clientserver:Answer received: !xro259
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o39
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !xro265
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro263
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o43
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro264
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro266
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o47
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o51
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o55
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro258
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro261
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro262
e

DEBUG:py4j.clientserver:Command to send: m
d
o59
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro259
e

DEBUG:py4j.clientserver:Answer received: !yro268
DEBUG:py4j.clientserver:Command to send: c
o268
write
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: m
d
o63
e

DEBUG:py4j.clientserver:Answer received: !yro269
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
o269
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro270
DEBUG:py4j.clientserver:Command to send: c
o270
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro271
DEBUG:py4j.clientserver:Command to send: c
o271
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__credtcard_2023-09-13-11-15-50.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o64
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yro267
DEBUG:py4j.clientserver:Command to send: c
o266
withColumn
scurrent_timestamp
ro267
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yro272
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro264
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro265
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o129
e

DEBUG:py4j.clientserver:Command to send: c
o272
write
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: m
d
o130
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro263
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro262
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yro273
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: m
d
o68
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
o273
mode
soverwrite
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yro274
DEBUG:py4j.clientserver:Command to send: c
o274
format
savro
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro261
e

DEBUG:py4j.clientserver:Command to send: m
d
o74
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro258
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yro275
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro259
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
o275
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__credtcard_2023-09-13-11-15-50.avro
e

DEBUG:py4j.clientserver:Command to send: m
d
o77
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o118
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro265
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro264
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro263
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: m
d
o119
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o121
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o120
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o124
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro262
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: m
d
o125
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro258
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro261
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro263
e

DEBUG:py4j.clientserver:Command to send: m
d
o126
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro259
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o127
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro264
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o131
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o132
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o133
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro262
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro265
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: m
d
o135
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro261
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o134
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro262
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro258
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro259
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: m
d
o136
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o137
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro262
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: m
d
o138
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro264
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: m
d
o139
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: m
d
o140
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro263
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro265
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro261
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro262
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro258
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: m
d
o141
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o142
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o143
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro259
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o144
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro261
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: m
d
o145
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro264
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro259
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro259
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro259
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro265
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro259
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro262
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro258
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o146
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o147
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o149
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o148
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: m
d
o150
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o151
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro263
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o152
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o153
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o154
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o155
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o157
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o160
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o161
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o158
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o159
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o163
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o168
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o171
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o172
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o173
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o176
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o177
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o178
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o181
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o183
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o185
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o186
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o188
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro264
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o189
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: m
d
o191
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro259
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro262
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro265
e

DEBUG:py4j.clientserver:Command to send: m
d
o193
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: m
d
o192
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o194
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o198
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o197
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro263
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o199
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o200
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o201
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: m
d
o205
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro264
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o206
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o210
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: m
d
o211
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro261
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: m
d
o215
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o216
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro263
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro264
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o217
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o218
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o219
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o220
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o221
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o222
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o223
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o225
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o228
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o229
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o230
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o231
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o232
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: m
d
o233
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o236
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o237
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: m
d
o238
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro258
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro263
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro259
e

DEBUG:py4j.clientserver:Command to send: m
d
o239
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o259
getCause
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro264
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o240
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro261
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o241
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: m
d
o244
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro262
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro265
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro261
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro258
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro263
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro264
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro259
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
o264
getCause
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro/_temporary/0/task_202309131115463709464618960739132_0001_m_000000 does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: c
o259
toString
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro262
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o262
getCause
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro/_temporary/0/task_202309131115463709464618960739132_0001_m_000000 does not exist
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: p
ro259
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro/_temporary/0/task_202309131115463709464618960739132_0001_m_000000 does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
ERROR:root:Error processing old_credtcard.avro: An error occurred while calling o187.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro/_temporary/0/task_202309131115463709464618960739132_0001_m_000000 does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro258
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro265
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro261
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro263
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o263
getCause
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro262
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-41.avro/_temporary/0/task_202309131115461614434562010420042_0005_m_000000 does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o262
toString
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro264
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-41.avro/_temporary/0/task_202309131115461614434562010420042_0005_m_000000 does not exist
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro265
e

DEBUG:py4j.clientserver:Command to send: p
ro262
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro/_temporary/0/task_202309131115463709464618960739132_0001_m_000000/part-00000-b174a1c0-4947-48a7-be77-d3a3fe2a356f-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro258
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-41.avro/_temporary/0/task_202309131115461614434562010420042_0005_m_000000 does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o264
toString
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

ERROR:root:Error processing old_credtcard.avro: An error occurred while calling o202.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-41.avro/_temporary/0/task_202309131115461614434562010420042_0005_m_000000 does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro/_temporary/0/task_202309131115463709464618960739132_0001_m_000000/part-00000-b174a1c0-4947-48a7-be77-d3a3fe2a356f-c000.avro does not exist
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: p
ro264
e

DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro263
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro261
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro/_temporary/0/task_202309131115463709464618960739132_0001_m_000000/part-00000-b174a1c0-4947-48a7-be77-d3a3fe2a356f-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro/_temporary/0/task_202309131115463709464618960739132_0001_m_000000/part-00000-b174a1c0-4947-48a7-be77-d3a3fe2a356f-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
ERROR:root:Error processing old_credtcard.avro: An error occurred while calling o175.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro/_temporary/0/task_202309131115463709464618960739132_0001_m_000000/part-00000-b174a1c0-4947-48a7-be77-d3a3fe2a356f-c000.avro does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)
	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o263
toString
e

INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Command to send: c
o261
getCause
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro/_temporary/0/task_202309131115463709464618960739132_0001_m_000000/part-00000-b174a1c0-4947-48a7-be77-d3a3fe2a356f-c000.avro does not exist
DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: p
ro263
e

DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro/_temporary/0/task_202309131115463709464618960739132_0001_m_000000/part-00000-b174a1c0-4947-48a7-be77-d3a3fe2a356f-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
ERROR:root:Error processing old_credtcard.avro: An error occurred while calling o174.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro/_temporary/0/task_202309131115463709464618960739132_0001_m_000000/part-00000-b174a1c0-4947-48a7-be77-d3a3fe2a356f-c000.avro does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)
	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro265
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro258
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o258
getCause
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro261
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro258
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro/_temporary/0/task_20230913111546461458200881810213_0002_m_000000/part-00000-23532d81-65f7-47a1-a4e8-e087491db989-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-41.avro/_temporary/0/task_202309131115468720731433830355122_0000_m_000000 does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o258
toString
e

DEBUG:py4j.clientserver:Command to send: c
o261
toString
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro/_temporary/0/task_20230913111546461458200881810213_0002_m_000000/part-00000-23532d81-65f7-47a1-a4e8-e087491db989-c000.avro does not exist
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-41.avro/_temporary/0/task_202309131115468720731433830355122_0000_m_000000 does not exist
DEBUG:py4j.clientserver:Command to send: p
ro258
e

DEBUG:py4j.clientserver:Command to send: p
ro261
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-41.avro/_temporary/0/task_202309131115468720731433830355122_0000_m_000000 does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro/_temporary/0/task_20230913111546461458200881810213_0002_m_000000/part-00000-23532d81-65f7-47a1-a4e8-e087491db989-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Answer received: !ym
ERROR:root:Error processing old_credtcard.avro: An error occurred while calling o207.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-41.avro/_temporary/0/task_202309131115468720731433830355122_0000_m_000000 does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

ERROR:root:Error processing old_credtcard.avro: An error occurred while calling o179.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__citydata_2023-09-13-11-15-40.avro/_temporary/0/task_20230913111546461458200881810213_0002_m_000000/part-00000-23532d81-65f7-47a1-a4e8-e087491db989-c000.avro does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)
	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro265
e

INFO:py4j.clientserver:Closing down clientserver connection
INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o265
getCause
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro265
e

DEBUG:py4j.clientserver:Answer received: !ysjava.nio.file.AccessDeniedException: C:\\Users\\lyekollu\\PycharmProject_Updated\\Accelerator\\Media\\destination_folder\\new__citydata_2023-09-13-11-15-41.avro\\_temporary\\0\\task_202309131115466803819602882406453_0006_m_000000: Permission denied\r\n	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1455)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o265
toString
e

DEBUG:py4j.clientserver:Answer received: !ysjava.nio.file.AccessDeniedException: C:\\Users\\lyekollu\\PycharmProject_Updated\\Accelerator\\Media\\destination_folder\\new__citydata_2023-09-13-11-15-41.avro\\_temporary\\0\\task_202309131115466803819602882406453_0006_m_000000: Permission denied
DEBUG:py4j.clientserver:Command to send: p
ro265
e

DEBUG:py4j.clientserver:Answer received: !ysjava.nio.file.AccessDeniedException: C:\\Users\\lyekollu\\PycharmProject_Updated\\Accelerator\\Media\\destination_folder\\new__citydata_2023-09-13-11-15-41.avro\\_temporary\\0\\task_202309131115466803819602882406453_0006_m_000000: Permission denied\r\n	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1455)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
ERROR:root:Error processing old_credtcard.avro: An error occurred while calling o212.save.
: java.nio.file.AccessDeniedException: C:\Users\lyekollu\PycharmProject_Updated\Accelerator\Media\destination_folder\new__citydata_2023-09-13-11-15-41.avro\_temporary\0\task_202309131115466803819602882406453_0006_m_000000: Permission denied
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1455)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:488)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Answer received: !xro276
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro276
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro277
DEBUG:py4j.clientserver:Command to send: c
o277
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro278
DEBUG:py4j.clientserver:Command to send: c
o278
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_data.avro
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro276
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro276
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro276
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro276
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro276
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yro279
DEBUG:py4j.clientserver:Command to send: c
o279
schema
e

DEBUG:py4j.clientserver:Answer received: !yro280
DEBUG:py4j.clientserver:Command to send: c
o280
json
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Id","type":"integer","nullable":true,"metadata":{}},{"name":"First_Name","type":"string","nullable":true,"metadata":{}},{"name":"Last_Name","type":"string","nullable":true,"metadata":{}},{"name":"Age","type":"integer","nullable":true,"metadata":{}},{"name":"City","type":"string","nullable":true,"metadata":{}},{"name":"Email","type":"string","nullable":true,"metadata":{}},{"name":"Gender","type":"string","nullable":true,"metadata":{}},{"name":"dateofbirth","type":"date","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: c
o279
apply
sdateofbirth
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yro281
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro276
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro281
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro282
DEBUG:py4j.clientserver:Command to send: c
o279
withColumn
sdateofbirth
ro282
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro276
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro276
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yro283
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sdateofbirth
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yro284
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro276
e

DEBUG:py4j.clientserver:Answer received: !xro285
DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro286
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro284
ro286
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Answer received: !yro287
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro285
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro276
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o276
getCause
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro287
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro288
DEBUG:py4j.clientserver:Command to send: c
o283
withColumn
sdateofbirth_add_months
ro288
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro285
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro276
e

DEBUG:py4j.clientserver:Answer received: !yro289
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__credtcard_2023-09-13-11-15-50.avro/_temporary/0/task_202309131115518365611184915024469_0011_m_000000/part-00000-89b0f117-637f-40a3-9111-11824c62e098-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o276
toString
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__credtcard_2023-09-13-11-15-50.avro/_temporary/0/task_202309131115518365611184915024469_0011_m_000000/part-00000-89b0f117-637f-40a3-9111-11824c62e098-c000.avro does not exist
DEBUG:py4j.clientserver:Command to send: p
ro276
e

DEBUG:py4j.clientserver:Answer received: !ysjava.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__credtcard_2023-09-13-11-15-50.avro/_temporary/0/task_202309131115518365611184915024469_0011_m_000000/part-00000-89b0f117-637f-40a3-9111-11824c62e098-c000.avro does not exist\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)\r\n	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)\r\n	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)\r\n	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Answer received: !yp
ERROR:root:Error processing old_data.avro: An error occurred while calling o260.save.
: java.io.FileNotFoundException: File file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__credtcard_2023-09-13-11-15-50.avro/_temporary/0/task_202309131115518365611184915024469_0011_m_000000/part-00000-89b0f117-637f-40a3-9111-11824c62e098-c000.avro does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:390)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341)
	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:505)
	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:700)
	at org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem.rename(ProxyLocalFileSystem.java:34)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:476)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:490)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:405)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro290
DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: c
o289
withColumn
scurrent_timestamp
ro290
e

DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro285
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !yro291
DEBUG:py4j.clientserver:Command to send: c
o291
write
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro285
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yro292
DEBUG:py4j.clientserver:Command to send: c
o292
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro293
DEBUG:py4j.clientserver:Command to send: c
o293
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro294
DEBUG:py4j.clientserver:Command to send: c
o294
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__data_2023-09-13-11-15-52.avro
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro285
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro285
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro285
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro285
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro285
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro285
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro285
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o285
getCause
e

DEBUG:py4j.clientserver:Answer received: !yro295
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro285
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 12) (LIN51006986.corp.capgemini.com executor driver): org.apache.hadoop.fs.ParentNotDirectoryException: Parent path is not a directory: file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__credtcard_2023-09-13-11-15-50.avro/_temporary/0/_temporary\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:691)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:424)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter$$anon$1.getAvroFileOutputStream(AvroOutputWriter.scala:79)\r\n	at org.apache.avro.mapreduce.AvroKeyOutputFormat.getRecordWriter(AvroKeyOutputFormat.java:104)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter.<init>(AvroOutputWriter.scala:81)\r\n	at org.apache.spark.sql.avro.AvroOutputWriterFactory.newInstance(AvroOutputWriterFactory.scala:47)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.hadoop.fs.ParentNotDirectoryException: Parent path is not a directory: file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__credtcard_2023-09-13-11-15-50.avro/_temporary/0/_temporary\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:691)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:424)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter$$anon$1.getAvroFileOutputStream(AvroOutputWriter.scala:79)\r\n	at org.apache.avro.mapreduce.AvroKeyOutputFormat.getRecordWriter(AvroKeyOutputFormat.java:104)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter.<init>(AvroOutputWriter.scala:81)\r\n	at org.apache.spark.sql.avro.AvroOutputWriterFactory.newInstance(AvroOutputWriterFactory.scala:47)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	... 1 more\r\n
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.api.python.PythonException
ro295
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o285
toString
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 12) (LIN51006986.corp.capgemini.com executor driver): org.apache.hadoop.fs.ParentNotDirectoryException: Parent path is not a directory: file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__credtcard_2023-09-13-11-15-50.avro/_temporary/0/_temporary\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:691)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:424)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter$$anon$1.getAvroFileOutputStream(AvroOutputWriter.scala:79)\r\n	at org.apache.avro.mapreduce.AvroKeyOutputFormat.getRecordWriter(AvroKeyOutputFormat.java:104)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter.<init>(AvroOutputWriter.scala:81)\r\n	at org.apache.spark.sql.avro.AvroOutputWriterFactory.newInstance(AvroOutputWriterFactory.scala:47)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro295
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro295
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro295
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro295
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro295
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro295
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro295
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro295
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro295
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro295
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: r
u
py4j
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
py4j.reflection.TypeUtil
rj
e

DEBUG:py4j.clientserver:Answer received: !ycpy4j.reflection.TypeUtil
DEBUG:py4j.clientserver:Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro295
e

DEBUG:py4j.clientserver:Answer received: !ybfalse
DEBUG:py4j.clientserver:Command to send: c
o295
getCause
e

DEBUG:py4j.clientserver:Answer received: !yn
DEBUG:py4j.clientserver:Command to send: r
u
org
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util
rj
e

DEBUG:py4j.clientserver:Answer received: !yp
DEBUG:py4j.clientserver:Command to send: r
u
org.apache.spark.util.Utils
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.util.Utils
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro295
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.hadoop.fs.ParentNotDirectoryException: Parent path is not a directory: file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__credtcard_2023-09-13-11-15-50.avro/_temporary/0/_temporary\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:691)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:424)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter$$anon$1.getAvroFileOutputStream(AvroOutputWriter.scala:79)\r\n	at org.apache.avro.mapreduce.AvroKeyOutputFormat.getRecordWriter(AvroKeyOutputFormat.java:104)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter.<init>(AvroOutputWriter.scala:81)\r\n	at org.apache.spark.sql.avro.AvroOutputWriterFactory.newInstance(AvroOutputWriterFactory.scala:47)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n
DEBUG:py4j.clientserver:Command to send: c
o295
toString
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.hadoop.fs.ParentNotDirectoryException: Parent path is not a directory: file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__credtcard_2023-09-13-11-15-50.avro/_temporary/0/_temporary
DEBUG:py4j.clientserver:Command to send: p
ro285
e

DEBUG:py4j.clientserver:Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 12) (LIN51006986.corp.capgemini.com executor driver): org.apache.hadoop.fs.ParentNotDirectoryException: Parent path is not a directory: file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__credtcard_2023-09-13-11-15-50.avro/_temporary/0/_temporary\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:691)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:424)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter$$anon$1.getAvroFileOutputStream(AvroOutputWriter.scala:79)\r\n	at org.apache.avro.mapreduce.AvroKeyOutputFormat.getRecordWriter(AvroKeyOutputFormat.java:104)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter.<init>(AvroOutputWriter.scala:81)\r\n	at org.apache.spark.sql.avro.AvroOutputWriterFactory.newInstance(AvroOutputWriterFactory.scala:47)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.hadoop.fs.ParentNotDirectoryException: Parent path is not a directory: file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__credtcard_2023-09-13-11-15-50.avro/_temporary/0/_temporary\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:691)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:424)\r\n	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521)\r\n	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\r\n	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter$$anon$1.getAvroFileOutputStream(AvroOutputWriter.scala:79)\r\n	at org.apache.avro.mapreduce.AvroKeyOutputFormat.getRecordWriter(AvroKeyOutputFormat.java:104)\r\n	at org.apache.spark.sql.avro.AvroOutputWriter.<init>(AvroOutputWriter.scala:81)\r\n	at org.apache.spark.sql.avro.AvroOutputWriterFactory.newInstance(AvroOutputWriterFactory.scala:47)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\r\n	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n	... 1 more\r\n
ERROR:root:Error processing old_data.avro: An error occurred while calling o271.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 12) (LIN51006986.corp.capgemini.com executor driver): org.apache.hadoop.fs.ParentNotDirectoryException: Parent path is not a directory: file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__credtcard_2023-09-13-11-15-50.avro/_temporary/0/_temporary
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:691)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:424)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052)
	at org.apache.spark.sql.avro.AvroOutputWriter$$anon$1.getAvroFileOutputStream(AvroOutputWriter.scala:79)
	at org.apache.avro.mapreduce.AvroKeyOutputFormat.getRecordWriter(AvroKeyOutputFormat.java:104)
	at org.apache.spark.sql.avro.AvroOutputWriter.<init>(AvroOutputWriter.scala:81)
	at org.apache.spark.sql.avro.AvroOutputWriterFactory.newInstance(AvroOutputWriterFactory.scala:47)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.hadoop.fs.ParentNotDirectoryException: Parent path is not a directory: file:/C:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__credtcard_2023-09-13-11-15-50.avro/_temporary/0/_temporary
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:691)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:424)
	at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052)
	at org.apache.spark.sql.avro.AvroOutputWriter$$anon$1.getAvroFileOutputStream(AvroOutputWriter.scala:79)
	at org.apache.avro.mapreduce.AvroKeyOutputFormat.getRecordWriter(AvroKeyOutputFormat.java:104)
	at org.apache.spark.sql.avro.AvroOutputWriter.<init>(AvroOutputWriter.scala:81)
	at org.apache.spark.sql.avro.AvroOutputWriterFactory.newInstance(AvroOutputWriterFactory.scala:47)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	... 1 more

INFO:py4j.clientserver:Closing down clientserver connection
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro296
DEBUG:py4j.clientserver:Command to send: c
o296
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro297
DEBUG:py4j.clientserver:Command to send: c
o297
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_empdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro298
DEBUG:py4j.clientserver:Command to send: c
o298
schema
e

DEBUG:py4j.clientserver:Answer received: !yro299
DEBUG:py4j.clientserver:Command to send: c
o299
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"emp_id","type":"integer","nullable":true,"metadata":{}},{"name":"emp_name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"dateofjoining","type":"date","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o298
apply
sdateofjoining
e

DEBUG:py4j.clientserver:Answer received: !yro300
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro300
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro301
DEBUG:py4j.clientserver:Command to send: c
o298
withColumn
sdateofjoining
ro301
e

DEBUG:py4j.clientserver:Answer received: !yro302
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sdateofjoining
e

DEBUG:py4j.clientserver:Answer received: !yro303
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro304
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro303
ro304
e

DEBUG:py4j.clientserver:Answer received: !yro305
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro305
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro306
DEBUG:py4j.clientserver:Command to send: c
o302
withColumn
sdateofjoining_add_months
ro306
e

DEBUG:py4j.clientserver:Answer received: !yro307
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro308
DEBUG:py4j.clientserver:Command to send: c
o307
withColumn
scurrent_timestamp
ro308
e

DEBUG:py4j.clientserver:Answer received: !yro309
DEBUG:py4j.clientserver:Command to send: c
o309
write
e

DEBUG:py4j.clientserver:Answer received: !yro310
DEBUG:py4j.clientserver:Command to send: c
o310
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro311
DEBUG:py4j.clientserver:Command to send: c
o311
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro312
DEBUG:py4j.clientserver:Command to send: c
o312
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__empdata_2023-09-13-11-15-53.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro313
DEBUG:py4j.clientserver:Command to send: c
o313
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro314
DEBUG:py4j.clientserver:Command to send: c
o314
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_examdata.avro
e

DEBUG:py4j.clientserver:Answer received: !yro315
DEBUG:py4j.clientserver:Command to send: c
o315
schema
e

DEBUG:py4j.clientserver:Answer received: !yro316
DEBUG:py4j.clientserver:Command to send: c
o316
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"start_date","type":"date","nullable":true,"metadata":{}},{"name":"end_date","type":"date","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o315
apply
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !yro317
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro317
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro318
DEBUG:py4j.clientserver:Command to send: c
o315
withColumn
sstart_date
ro318
e

DEBUG:py4j.clientserver:Answer received: !yro319
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sstart_date
e

DEBUG:py4j.clientserver:Answer received: !yro320
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro321
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro320
ro321
e

DEBUG:py4j.clientserver:Answer received: !yro322
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro322
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro323
DEBUG:py4j.clientserver:Command to send: c
o319
withColumn
sstart_date_add_months
ro323
e

DEBUG:py4j.clientserver:Answer received: !yro324
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro325
DEBUG:py4j.clientserver:Command to send: c
o324
withColumn
scurrent_timestamp
ro325
e

DEBUG:py4j.clientserver:Answer received: !yro326
DEBUG:py4j.clientserver:Command to send: c
o326
write
e

DEBUG:py4j.clientserver:Answer received: !yro327
DEBUG:py4j.clientserver:Command to send: c
o327
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro328
DEBUG:py4j.clientserver:Command to send: c
o328
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro329
DEBUG:py4j.clientserver:Command to send: c
o329
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-11-15-54.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o315
apply
send_date
e

DEBUG:py4j.clientserver:Answer received: !yro330
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro330
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro331
DEBUG:py4j.clientserver:Command to send: c
o315
withColumn
send_date
ro331
e

DEBUG:py4j.clientserver:Answer received: !yro332
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
send_date
e

DEBUG:py4j.clientserver:Answer received: !yro333
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro334
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro333
ro334
e

DEBUG:py4j.clientserver:Answer received: !yro335
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro335
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro336
DEBUG:py4j.clientserver:Command to send: c
o332
withColumn
send_date_add_months
ro336
e

DEBUG:py4j.clientserver:Answer received: !yro337
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro338
DEBUG:py4j.clientserver:Command to send: c
o337
withColumn
scurrent_timestamp
ro338
e

DEBUG:py4j.clientserver:Answer received: !yro339
DEBUG:py4j.clientserver:Command to send: c
o339
write
e

DEBUG:py4j.clientserver:Answer received: !yro340
DEBUG:py4j.clientserver:Command to send: c
o340
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro341
DEBUG:py4j.clientserver:Command to send: c
o341
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro342
DEBUG:py4j.clientserver:Command to send: c
o342
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__examdata_2023-09-13-11-15-55.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro343
DEBUG:py4j.clientserver:Command to send: c
o343
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro344
DEBUG:py4j.clientserver:Command to send: c
o344
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_first.avro
e

DEBUG:py4j.clientserver:Answer received: !yro345
DEBUG:py4j.clientserver:Command to send: c
o345
schema
e

DEBUG:py4j.clientserver:Answer received: !yro346
DEBUG:py4j.clientserver:Command to send: c
o346
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"first","type":"string","nullable":true,"metadata":{}},{"name":"last","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"date","type":"date","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"state","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o345
apply
sdate
e

DEBUG:py4j.clientserver:Answer received: !yro347
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro347
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro348
DEBUG:py4j.clientserver:Command to send: c
o345
withColumn
sdate
ro348
e

DEBUG:py4j.clientserver:Answer received: !yro349
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sdate
e

DEBUG:py4j.clientserver:Answer received: !yro350
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro351
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro350
ro351
e

DEBUG:py4j.clientserver:Answer received: !yro352
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro352
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro353
DEBUG:py4j.clientserver:Command to send: c
o349
withColumn
sdate_add_months
ro353
e

DEBUG:py4j.clientserver:Answer received: !yro354
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro355
DEBUG:py4j.clientserver:Command to send: c
o354
withColumn
scurrent_timestamp
ro355
e

DEBUG:py4j.clientserver:Answer received: !yro356
DEBUG:py4j.clientserver:Command to send: c
o356
write
e

DEBUG:py4j.clientserver:Answer received: !yro357
DEBUG:py4j.clientserver:Command to send: c
o357
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro358
DEBUG:py4j.clientserver:Command to send: c
o358
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro359
DEBUG:py4j.clientserver:Command to send: c
o359
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__first_2023-09-13-11-15-55.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro360
DEBUG:py4j.clientserver:Command to send: c
o360
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro361
DEBUG:py4j.clientserver:Command to send: c
o361
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_fullname.avro
e

DEBUG:py4j.clientserver:Answer received: !yro362
DEBUG:py4j.clientserver:Command to send: c
o362
schema
e

DEBUG:py4j.clientserver:Answer received: !yro363
DEBUG:py4j.clientserver:Command to send: c
o363
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"Fullname","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"email","type":"string","nullable":true,"metadata":{}},{"name":"LastDate","type":"date","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o362
apply
sLastDate
e

DEBUG:py4j.clientserver:Answer received: !yro364
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro364
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro365
DEBUG:py4j.clientserver:Command to send: c
o362
withColumn
sLastDate
ro365
e

DEBUG:py4j.clientserver:Answer received: !yro366
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sLastDate
e

DEBUG:py4j.clientserver:Answer received: !yro367
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro368
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro367
ro368
e

DEBUG:py4j.clientserver:Answer received: !yro369
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro369
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro370
DEBUG:py4j.clientserver:Command to send: c
o366
withColumn
sLastDate_add_months
ro370
e

DEBUG:py4j.clientserver:Answer received: !yro371
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro372
DEBUG:py4j.clientserver:Command to send: c
o371
withColumn
scurrent_timestamp
ro372
e

DEBUG:py4j.clientserver:Answer received: !yro373
DEBUG:py4j.clientserver:Command to send: c
o373
write
e

DEBUG:py4j.clientserver:Answer received: !yro374
DEBUG:py4j.clientserver:Command to send: c
o374
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro375
DEBUG:py4j.clientserver:Command to send: c
o375
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro376
DEBUG:py4j.clientserver:Command to send: c
o376
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__fullname_2023-09-13-11-15-56.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro377
DEBUG:py4j.clientserver:Command to send: c
o377
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro378
DEBUG:py4j.clientserver:Command to send: c
o378
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_name.avro
e

DEBUG:py4j.clientserver:Answer received: !yro379
DEBUG:py4j.clientserver:Command to send: c
o379
schema
e

DEBUG:py4j.clientserver:Answer received: !yro380
DEBUG:py4j.clientserver:Command to send: c
o380
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"DateOfBirth","type":"date","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o379
apply
sDateOfBirth
e

DEBUG:py4j.clientserver:Answer received: !yro381
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro381
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro382
DEBUG:py4j.clientserver:Command to send: c
o379
withColumn
sDateOfBirth
ro382
e

DEBUG:py4j.clientserver:Answer received: !yro383
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
sDateOfBirth
e

DEBUG:py4j.clientserver:Answer received: !yro384
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro385
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro384
ro385
e

DEBUG:py4j.clientserver:Answer received: !yro386
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro386
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro387
DEBUG:py4j.clientserver:Command to send: c
o383
withColumn
sDateOfBirth_add_months
ro387
e

DEBUG:py4j.clientserver:Answer received: !yro388
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro389
DEBUG:py4j.clientserver:Command to send: c
o388
withColumn
scurrent_timestamp
ro389
e

DEBUG:py4j.clientserver:Answer received: !yro390
DEBUG:py4j.clientserver:Command to send: c
o390
write
e

DEBUG:py4j.clientserver:Answer received: !yro391
DEBUG:py4j.clientserver:Command to send: c
o391
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro392
DEBUG:py4j.clientserver:Command to send: c
o392
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro393
DEBUG:py4j.clientserver:Command to send: c
o393
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__name_2023-09-13-11-15-56.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro394
DEBUG:py4j.clientserver:Command to send: c
o394
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro395
DEBUG:py4j.clientserver:Command to send: c
o395
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_renewaldate.avro
e

DEBUG:py4j.clientserver:Answer received: !yro396
DEBUG:py4j.clientserver:Command to send: c
o396
schema
e

DEBUG:py4j.clientserver:Answer received: !yro397
DEBUG:py4j.clientserver:Command to send: c
o397
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"full_name","type":"string","nullable":true,"metadata":{}},{"name":"renewal_date","type":"date","nullable":true,"metadata":{}},{"name":"gender","type":"string","nullable":true,"metadata":{}},{"name":"ccnumber","type":"double","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o396
apply
srenewal_date
e

DEBUG:py4j.clientserver:Answer received: !yro398
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro398
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro399
DEBUG:py4j.clientserver:Command to send: c
o396
withColumn
srenewal_date
ro399
e

DEBUG:py4j.clientserver:Answer received: !yro400
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
srenewal_date
e

DEBUG:py4j.clientserver:Answer received: !yro401
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro402
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro401
ro402
e

DEBUG:py4j.clientserver:Answer received: !yro403
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro403
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro404
DEBUG:py4j.clientserver:Command to send: c
o400
withColumn
srenewal_date_add_months
ro404
e

DEBUG:py4j.clientserver:Answer received: !yro405
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro406
DEBUG:py4j.clientserver:Command to send: c
o405
withColumn
scurrent_timestamp
ro406
e

DEBUG:py4j.clientserver:Answer received: !yro407
DEBUG:py4j.clientserver:Command to send: c
o407
write
e

DEBUG:py4j.clientserver:Answer received: !yro408
DEBUG:py4j.clientserver:Command to send: c
o408
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro409
DEBUG:py4j.clientserver:Command to send: c
o409
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro410
DEBUG:py4j.clientserver:Command to send: c
o410
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__renewaldate_2023-09-13-11-15-57.avro
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o29
read
e

DEBUG:py4j.clientserver:Answer received: !yro411
DEBUG:py4j.clientserver:Command to send: c
o411
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro412
DEBUG:py4j.clientserver:Command to send: c
o412
load
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/user_input/old_salary_data.avro
e

DEBUG:py4j.clientserver:Answer received: !yro413
DEBUG:py4j.clientserver:Command to send: c
o413
schema
e

DEBUG:py4j.clientserver:Answer received: !yro414
DEBUG:py4j.clientserver:Command to send: c
o414
json
e

DEBUG:py4j.clientserver:Answer received: !ys{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"fullname","type":"string","nullable":true,"metadata":{}},{"name":"salarydate","type":"date","nullable":true,"metadata":{}},{"name":"salary","type":"string","nullable":true,"metadata":{}}]}
DEBUG:py4j.clientserver:Command to send: c
o413
apply
ssalarydate
e

DEBUG:py4j.clientserver:Answer received: !yro415
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro415
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro416
DEBUG:py4j.clientserver:Command to send: c
o413
withColumn
ssalarydate
ro416
e

DEBUG:py4j.clientserver:Answer received: !yro417
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
col
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
col
ssalarydate
e

DEBUG:py4j.clientserver:Answer received: !yro418
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
lit
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
lit
i1
e

DEBUG:py4j.clientserver:Answer received: !yro419
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
add_months
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
add_months
ro418
ro419
e

DEBUG:py4j.clientserver:Answer received: !yro420
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
date_format
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
date_format
ro420
syyyy-MM-dd
e

DEBUG:py4j.clientserver:Answer received: !yro421
DEBUG:py4j.clientserver:Command to send: c
o417
withColumn
ssalarydate_add_months
ro421
e

DEBUG:py4j.clientserver:Answer received: !yro422
DEBUG:py4j.clientserver:Command to send: r
u
functions
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.functions
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

DEBUG:py4j.clientserver:Answer received: !yro423
DEBUG:py4j.clientserver:Command to send: c
o422
withColumn
scurrent_timestamp
ro423
e

DEBUG:py4j.clientserver:Answer received: !yro424
DEBUG:py4j.clientserver:Command to send: c
o424
write
e

DEBUG:py4j.clientserver:Answer received: !yro425
DEBUG:py4j.clientserver:Command to send: c
o425
mode
soverwrite
e

DEBUG:py4j.clientserver:Answer received: !yro426
DEBUG:py4j.clientserver:Command to send: c
o426
format
savro
e

DEBUG:py4j.clientserver:Answer received: !yro427
DEBUG:py4j.clientserver:Command to send: c
o427
save
sC:/Users/lyekollu/PycharmProject_Updated/Accelerator/Media/destination_folder/new__salary_data_2023-09-13-11-15-58.avro
e

DEBUG:py4j.clientserver:Command to send: m
d
o247
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o248
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o249
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o250
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o251
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o252
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o253
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o254
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o255
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o256
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o257
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o260
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o258
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o261
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o263
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o264
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o262
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o259
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o265
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o266
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o268
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o269
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o270
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o271
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o267
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o272
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o273
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o274
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o275
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o276
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o277
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o278
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o279
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o280
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o281
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o282
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o283
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o284
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o285
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o286
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o287
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o288
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o289
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o290
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o291
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o292
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o293
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o294
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o295
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o296
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o297
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o298
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o299
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o300
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o301
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o302
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o303
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o304
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o305
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o306
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o307
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o308
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o309
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o310
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o311
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o312
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o313
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o314
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o315
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o316
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o317
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o318
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o319
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o320
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o321
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o322
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o323
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o324
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o325
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o326
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o327
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o328
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o329
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o330
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o331
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o332
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o333
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o334
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o335
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o336
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o337
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o338
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o339
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o340
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o341
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o342
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o343
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o344
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o345
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o346
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o347
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o348
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o349
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o350
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o351
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o352
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o353
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o354
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o355
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o356
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o357
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o358
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o359
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o360
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o361
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o362
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o363
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o364
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o365
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o366
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o367
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o368
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o369
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o370
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o371
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o372
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o373
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o374
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o375
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o376
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o377
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o378
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o379
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o380
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o381
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o382
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o384
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o385
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o386
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o387
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o388
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o391
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o392
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o393
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o394
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o395
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o397
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: m
d
o398
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: c
o18
stop
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
clearDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
clearDefaultSession
e

DEBUG:py4j.clientserver:Answer received: !yv
DEBUG:py4j.clientserver:Command to send: r
u
SparkSession
rj
e

DEBUG:py4j.clientserver:Answer received: !ycorg.apache.spark.sql.SparkSession
DEBUG:py4j.clientserver:Command to send: r
m
org.apache.spark.sql.SparkSession
clearActiveSession
e

DEBUG:py4j.clientserver:Answer received: !ym
DEBUG:py4j.clientserver:Command to send: c
z:org.apache.spark.sql.SparkSession
clearActiveSession
e

DEBUG:py4j.clientserver:Answer received: !yv
INFO:py4j.clientserver:Closing down clientserver connection
INFO:py4j.clientserver:Closing down clientserver connection
